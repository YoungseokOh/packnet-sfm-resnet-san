#!/usr/bin/env python3
"""NCDB full image depth evaluation with visualization.

ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ depth í‰ê°€ ë° ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
Object mask ì—†ì´ ì „ì²´ í”½ì…€ì— ëŒ€í•´ GT, Pred, Error heatmapì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import argparse
import json
import math
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap, ListedColormap
import numpy as np
import torch
from PIL import Image
from tqdm import tqdm

from packnet_sfm.datasets.augmentations import to_tensor
from packnet_sfm.models.model_wrapper import ModelWrapper
from packnet_sfm.utils.config import parse_test_file
from packnet_sfm.utils.depth import compute_depth_metrics, inv2depth, load_depth, post_process_inv_depth
from packnet_sfm.utils.image import load_image


DEFAULT_ALL_SPLITS = ["combined_train.json", "combined_val.json", "combined_test.json"]
METRIC_NAMES = ["abs_rel", "sqr_rel", "rmse", "rmse_log", "a1", "a2", "a3"]


@dataclass
class SampleEntry:
    stem: str
    sequence_root: Path
    image_path: Path
    gt_path: Path
    prediction_path: Path


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Evaluate full image depth on NCDB")
    parser.add_argument("--dataset-root", type=str, required=True,
                        help="ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ")
    parser.add_argument("--split-files", type=str, nargs="*",
                        help="í‰ê°€ì— ì‚¬ìš©í•  split JSON")
    parser.add_argument("--use-all-splits", action="store_true",
                        help="combined_train/val/test ì„¸ ê°€ì§€ split ì„ ëª¨ë‘ ë¡œë“œ")
    parser.add_argument("--splits-dir", type=str, default="splits",
                        help="split-files ë””ë ‰í† ë¦¬")

    parser.add_argument("--pred-root", type=str, required=True,
                        help="ì˜ˆì¸¡ depth ì €ì¥ í´ë”")
    parser.add_argument("--gt-root", type=str, required=True,
                        help="GT depth í´ë”ëª…")
    parser.add_argument("--image-subdir", type=str, default="image_a6",
                        help="RGB ì´ë¯¸ì§€ ì„œë¸Œí´ë”")
    parser.add_argument("--checkpoint", type=str, required=True,
                        help="ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--image-shape", type=int, nargs=2, default=None,
                        help="ì¶”ë¡  ì‹œ ì´ë¯¸ì§€ í¬ê¸° (height width)")
    parser.add_argument("--flip-tta", action="store_true",
                        help="ì¢Œìš° ë°˜ì „ TTA ì‚¬ìš© ì—¬ë¶€")

    parser.add_argument("--output-file", type=str, default=None,
                        help="ê²°ê³¼ CSV ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--per-sample-json", type=str, default=None,
                        help="ìƒ˜í”Œë³„ ë©”íŠ¸ë¦­ì„ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ")

    parser.add_argument("--min-depth", type=float, default=None,
                        help="í‰ê°€ì— ì‚¬ìš©í•  ìµœì†Œ ê¹Šì´ (Noneì´ë©´ ëª¨ë¸ì—ì„œ ìë™)")
    parser.add_argument("--max-depth", type=float, default=None,
                        help="í‰ê°€ì— ì‚¬ìš©í•  ìµœëŒ€ ê¹Šì´ (Noneì´ë©´ ëª¨ë¸ì—ì„œ ìë™)")
    parser.add_argument("--crop", type=str, choices=["", "garg"], default="",
                        help="Eigen crop ì ìš© ì—¬ë¶€")
    parser.add_argument("--scale-output", type=str, default="top-center",
                        help="ìŠ¤ì¼€ì¼ ì •ë ¬ ë°©ì‹")
    parser.add_argument("--use-gt-scale", action="store_true",
                        help="GTì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ì •ë ¬")

    parser.add_argument("--device", type=str, default="cuda:0",
                        help="ì¶”ë¡  ë””ë°”ì´ìŠ¤")
    parser.add_argument("--dtype", type=str, choices=["fp16", "fp32"], default="fp32",
                        help="ì¶”ë¡  ì‹œ ë°ì´í„° íƒ€ì…")
    parser.add_argument("--debug", action="store_true",
                        help="ë””ë²„ê·¸ ì¶œë ¥ í™œì„±í™”")
    parser.add_argument("--max-samples", type=int, default=None,
                        help="í‰ê°€í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ë””ë²„ê¹…ìš©)")
    parser.add_argument("--visualize-dir", type=str, default=None,
                        help="ì‹œê°í™” ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬")

    return parser.parse_args()


def discover_split_files(args: argparse.Namespace) -> List[Path]:
    """Split JSON íŒŒì¼ ê²½ë¡œ ëª©ë¡ ë°˜í™˜."""
    dataset_root = Path(args.dataset_root)
    splits_dir = dataset_root / args.splits_dir

    if args.use_all_splits:
        return [splits_dir / s for s in DEFAULT_ALL_SPLITS]
    
    if not args.split_files:
        raise ValueError("--split-files ë˜ëŠ” --use-all-splits ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
    
    result = []
    for sf in args.split_files:
        p = Path(sf)
        if p.is_absolute():
            result.append(p)
        else:
            result.append(splits_dir / p)
    return result


def load_split_entries(args: argparse.Namespace, split_paths: List[Path]) -> List[dict]:
    """Split JSONë“¤ì„ ì½ì–´ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘í•©."""
    merged = []
    for sp in split_paths:
        if not sp.exists():
            raise FileNotFoundError(f"Split íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sp}")
        with open(sp, "r") as f:
            data = json.load(f)
            merged.extend(data)
    return merged


def get_checkpoint_id(checkpoint_path: str) -> str:
    """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì—ì„œ ID ì¶”ì¶œ (ìºì‹œ í´ë”ëª… ìƒì„±ìš©)."""
    p = Path(checkpoint_path)
    return p.stem


def normalize_entry(args: argparse.Namespace, dataset_root: Path, entry: dict, checkpoint_id: str) -> SampleEntry:
    """Split entryë¥¼ SampleEntryë¡œ ë³€í™˜."""
    stem = entry["new_filename"]
    sequence_root_raw = entry.get("dataset_root", "")
    sequence_root = Path(sequence_root_raw) if sequence_root_raw else dataset_root

    # RGB ê²½ë¡œ - dataset_root ìš°ì„ 
    image_path = sequence_root / args.image_subdir / f"{stem}.png"
    if not image_path.exists():
        image_path_raw = entry.get("image_path")
        if image_path_raw:
            image_path = dataset_root / image_path_raw

    # GT depth ê²½ë¡œ
    gt_path = sequence_root / args.gt_root / f"{stem}.png"

    # ì˜ˆì¸¡ depth ì €ì¥ ê²½ë¡œ (ì²´í¬í¬ì¸íŠ¸ë³„ í´ë”)
    pred_dir = sequence_root / args.pred_root / checkpoint_id
    pred_dir.mkdir(parents=True, exist_ok=True)
    prediction_path = pred_dir / f"{stem}.npz"

    return SampleEntry(
        stem=stem,
        sequence_root=sequence_root,
        image_path=image_path,
        gt_path=gt_path,
        prediction_path=prediction_path,
    )


def prepare_model(args: argparse.Namespace):
    """ëª¨ë¸ ë¡œë”© ë° ì„¤ì •."""
    print("### Preparing Model")
    config, state_dict = parse_test_file(args.checkpoint)
    model_wrapper = ModelWrapper(config, load_datasets=False)
    model_wrapper.load_state_dict(state_dict)

    device = torch.device(args.device)
    model_wrapper.to(device)
    model_wrapper.eval()

    dtype = torch.float16 if args.dtype == "fp16" else torch.float32
    if dtype == torch.float16:
        model_wrapper.half()

    # ëª¨ë¸ì—ì„œ depth ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
    model_min_depth = getattr(model_wrapper.depth_net, 'min_depth', 0.1)
    model_max_depth = getattr(model_wrapper.depth_net, 'max_depth', 100.0)
    
    print(f"\nğŸ“Š ëª¨ë¸ì—ì„œ ì½ì–´ì˜¨ depth ë²”ìœ„:")
    print(f"   min_depth: {model_min_depth}")
    print(f"   max_depth: {model_max_depth}")
    
    if args.min_depth is None or args.max_depth is None:
        args.min_depth = model_min_depth
        args.max_depth = model_max_depth
        print(f"   âš ï¸  í‰ê°€ ì„¤ì •ì„ ëª¨ë¸ ê°’ìœ¼ë¡œ ìë™ ì¡°ì •")

    return {
        "wrapper": model_wrapper,
        "device": device,
        "dtype": dtype,
    }


def make_eval_namespace(args: argparse.Namespace):
    """compute_depth_metricsì— ì „ë‹¬í•  ì„¤ì •."""
    return argparse.Namespace(
        min_depth=args.min_depth,
        max_depth=args.max_depth,
        crop=args.crop,
        scale_output=args.scale_output,
        use_gt_scale=args.use_gt_scale,
    )


def run_inference(model_context: dict, image_path: Path, flip_tta: bool = False) -> np.ndarray:
    """ì´ë¯¸ì§€ë¥¼ ì½ì–´ depth ì¶”ë¡  (í•™ìŠµ ì‹œ í‰ê°€ì™€ ë™ì¼í•œ ë°©ì‹)."""
    wrapper = model_context["wrapper"]
    device = model_context["device"]
    dtype = model_context["dtype"]

    image = load_image(str(image_path))
    batch = {
        "rgb": to_tensor(image).unsqueeze(0).to(device, dtype=dtype),
    }

    with torch.no_grad():
        # í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ wrapper.model() ì§ì ‘ í˜¸ì¶œ
        output = wrapper.model(batch)
    
    # ëª¨ë¸ ì¶œë ¥ì—ì„œ inv_depths ì¶”ì¶œ
    if 'inv_depths' in output:
        inv_depth = output['inv_depths'][0]  # ì²« ë²ˆì§¸ ìŠ¤ì¼€ì¼
    else:
        raise KeyError(f"Cannot find inv_depths in output keys: {output.keys()}")
    
    if flip_tta:
        batch_flip = {"rgb": torch.flip(batch["rgb"], dims=[-1])}
        with torch.no_grad():
            output_flip = wrapper.model(batch_flip)
        
        inv_depth_flip = torch.flip(output_flip['inv_depths'][0], dims=[-1])
        inv_depth = post_process_inv_depth(inv_depth, inv_depth_flip, method='mean')
    
    depth = inv2depth(inv_depth)[0, 0].cpu().numpy()
    return depth


def load_prediction(pred_path: Path) -> Optional[np.ndarray]:
    """ìºì‹œëœ ì˜ˆì¸¡ depth ë¡œë“œ."""
    if not pred_path.exists():
        return None
    data = np.load(str(pred_path))
    return data["depth"]


def save_prediction(pred_path: Path, depth: np.ndarray) -> None:
    """ì˜ˆì¸¡ depthë¥¼ NPZë¡œ ì €ì¥."""
    pred_path.parent.mkdir(parents=True, exist_ok=True)
    np.savez_compressed(str(pred_path), depth=depth)


def ensure_dir(path: Path) -> None:
    """ë””ë ‰í† ë¦¬ ìƒì„±."""
    path.mkdir(parents=True, exist_ok=True)


def visualize_full_image(
    rgb_path: Path,
    gt_depth: np.ndarray,
    pred_depth: np.ndarray,
    stem: str,
    save_path: Path,
    min_depth: float = 0.05,
    max_depth: float = 100.0,
) -> None:
    """ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ 4-panel ì‹œê°í™” (RGB, GT, Pred, Error heatmap)."""
    
    # RGB ë¡œë“œ
    rgb = np.array(Image.open(rgb_path))
    
    # Valid mask
    valid_mask = (gt_depth > min_depth) & (gt_depth < max_depth)
    
    # Gradient colormap with gray for invalid areas (ê°ì²´ ë§ˆìŠ¤í¬ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ)
    from matplotlib.colors import ListedColormap
    colors_list = ['#cccccc']  # Invalid ì˜ì—­ìš© íšŒìƒ‰
    gradient_colors = ['#00ff00', '#ffff00', '#ff8000', '#ff0000']  # green->yellow->orange->red
    n_gradient = 256
    gradient_cmap = LinearSegmentedColormap.from_list('gradient', gradient_colors, N=n_gradient)
    colors_list.extend([gradient_cmap(i) for i in range(n_gradient)])
    
    combined_cmap = ListedColormap(colors_list)
    
    # Error ê³„ì‚° (valid í”½ì…€ë§Œ)
    error_map = np.full_like(gt_depth, -1.0)  # -1ë¡œ ì´ˆê¸°í™” (invalid)
    if valid_mask.any():
        gt_valid = gt_depth[valid_mask]
        pred_valid = pred_depth[valid_mask]
        abs_rel_valid = np.abs(gt_valid - pred_valid) / (gt_valid + 1e-7)
        error_map[valid_mask] = np.clip(abs_rel_valid, 0, 0.5)  # 0.5 ì´ìƒì€ 0.5ë¡œ í´ë¦¬í•‘
    
    # Display map: -1 -> 0 (íšŒìƒ‰), 0~0.5 -> 1~256 (ê·¸ë¼ë””ì–¸íŠ¸)
    display_map = error_map.copy()
    display_map[error_map >= 0] = (error_map[error_map >= 0] / 0.5) * (n_gradient - 1) + 1
    display_map[error_map < 0] = 0
    
    # Error ë¶„ë¥˜ (í˜„ì¬ ê¸°ì¤€)
    error_bins = [
        ("[Excellent]", 0.00, 0.05),
        ("[Good]",      0.05, 0.10),
        ("[Fair]",      0.10, 0.20),
        ("[Poor]",      0.20, 0.30),
        ("[Bad]",       0.30, float('inf'))
    ]
    
    bin_counts = []
    valid_errors = error_map[error_map >= 0]  # -1 ì œì™¸
    total_valid = len(valid_errors)
    
    for label, low, high in error_bins:
        count = np.sum((valid_errors >= low) & (valid_errors < high))
        bin_counts.append((label, count, count / total_valid * 100 if total_valid > 0 else 0))
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    if total_valid > 0:
        abs_rel_mean = valid_errors.mean()
        a1 = np.mean(np.maximum(gt_valid / pred_valid, pred_valid / gt_valid) < 1.25)
    else:
        abs_rel_mean = np.nan
        a1 = np.nan
    
    # 4-panel plot
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Panel 1: RGB
    axes[0, 0].imshow(rgb)
    axes[0, 0].set_title(f'RGB Image\n{stem}', fontsize=12, fontweight='bold')
    axes[0, 0].axis('off')
    
    # Panel 2: GT Depth (ê°ì²´ ë§ˆìŠ¤í¬ ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ)
    gt_display = gt_depth.copy()
    gt_display[~valid_mask] = np.nan
    # 99 percentileì„ ì‚¬ìš©í•˜ì—¬ outlier ì œê±°í•œ ë²”ìœ„ ì„¤ì •
    gt_valid_values = gt_depth[valid_mask]
    if len(gt_valid_values) > 0:
        gt_vmax = np.percentile(gt_valid_values, 99)
    else:
        gt_vmax = max_depth
    
    im1 = axes[0, 1].imshow(gt_display, cmap='viridis', vmin=0, vmax=gt_vmax)
    axes[0, 1].set_title(f'GT Depth (masked)\nRange: [0, {gt_vmax:.1f}]m (99%ile)', 
                         fontsize=12, fontweight='bold')
    axes[0, 1].axis('off')
    plt.colorbar(im1, ax=axes[0, 1], label='Depth (m)', fraction=0.046, pad=0.04)
    
    # Panel 3: Pred Depth (GTì™€ ë™ì¼í•œ ë²”ìœ„ ì‚¬ìš©)
    pred_display = pred_depth.copy()
    pred_display[~valid_mask] = np.nan
    im2 = axes[1, 0].imshow(pred_display, cmap='viridis', vmin=0, vmax=gt_vmax)
    axes[1, 0].set_title(f'Predicted Depth\nRange: [0, {gt_vmax:.1f}]m', 
                         fontsize=12, fontweight='bold')
    axes[1, 0].axis('off')
    plt.colorbar(im2, ax=axes[1, 0], label='Depth (m)', fraction=0.046, pad=0.04)
    
    # Panel 4: Error Heatmap
    im3 = axes[1, 1].imshow(display_map, cmap=combined_cmap, vmin=0, vmax=n_gradient)
    axes[1, 1].set_title(f'Error Heatmap (abs_rel)\nMean: {abs_rel_mean:.4f}, a1: {a1:.4f}', 
                         fontsize=12, fontweight='bold')
    axes[1, 1].axis('off')
    
    # ColorbarëŠ” 0~0.5 ë²”ìœ„ë§Œ í‘œì‹œ
    cbar = plt.colorbar(im3, ax=axes[1, 1], fraction=0.046, pad=0.04)
    cbar.set_label('Absolute Relative Error', rotation=270, labelpad=20)
    cbar.set_ticks([1, n_gradient//4, n_gradient//2, 3*n_gradient//4, n_gradient])
    cbar.set_ticklabels(['0.0', '0.125', '0.25', '0.375', '0.5+'])
    
    # Error distribution text box
    stats_text = f"Error Distribution ({total_valid:,} pixels):\n"
    stats_text += "-" * 40 + "\n"
    for label, count, pct in bin_counts:
        bar = "â–“" * int(pct / 10) + "â–‘" * (10 - int(pct / 10))
        stats_text += f"{label:12s}: {count:7,} ({pct:5.1f}%) {bar}\n"
    
    axes[1, 1].text(1.25, 0.5, stats_text, transform=axes[1, 1].transAxes,
                   fontsize=10, verticalalignment='center',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                   family='monospace')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close(fig)


def main() -> None:
    args = parse_args()
    dataset_root = Path(args.dataset_root)
    split_paths = discover_split_files(args)
    raw_entries = load_split_entries(args, split_paths)
    
    checkpoint_id = get_checkpoint_id(args.checkpoint)
    print(f"\nğŸ“ ì²´í¬í¬ì¸íŠ¸ ID: {checkpoint_id}")
    print(f"   ì˜ˆì¸¡ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {args.pred_root}/{checkpoint_id}/\n")
    
    samples = [normalize_entry(args, dataset_root, entry, checkpoint_id) for entry in raw_entries]

    if args.debug:
        print(f"ì´ {len(samples)}ê°œ ìƒ˜í”Œ ë¡œë“œ")

    model_context = prepare_model(args)
    eval_namespace = make_eval_namespace(args)

    # ê²°ê³¼ ì €ì¥
    all_metrics = []
    sample_results = []
    
    visualize_dir = Path(args.visualize_dir) if args.visualize_dir else None
    if visualize_dir:
        ensure_dir(visualize_dir)
        print(f"\nì‹œê°í™” ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬: {visualize_dir}")

    processed_samples = 0
    
    for sample_idx, sample in enumerate(tqdm(samples, desc="Evaluating")):
        # max_samples ì²´í¬
        if args.max_samples is not None and processed_samples >= args.max_samples:
            if args.debug:
                print(f"\nìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ë„ë‹¬: {args.max_samples}, í‰ê°€ ì¢…ë£Œ")
            break
        
        gt_data = load_depth(str(sample.gt_path)) if sample.gt_path.exists() else None
        if gt_data is None:
            if args.debug:
                print(f"GT ëˆ„ë½ìœ¼ë¡œ ìŠ¤í‚µ: {sample.gt_path}")
            continue

        prediction = load_prediction(sample.prediction_path)
        if prediction is None:
            prediction = run_inference(model_context, sample.image_path, args.flip_tta)
            save_prediction(sample.prediction_path, prediction)
        elif args.debug:
            print(f"ìºì‹œëœ ì˜ˆì¸¡ ì‚¬ìš©: {sample.prediction_path}")

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        gt_tensor = torch.from_numpy(gt_data).unsqueeze(0).unsqueeze(0)
        pred_tensor = torch.from_numpy(prediction).unsqueeze(0).unsqueeze(0)
        
        metrics = compute_depth_metrics(
            config=eval_namespace,
            gt=gt_tensor,
            pred=pred_tensor,
            use_gt_scale=eval_namespace.use_gt_scale,
        )
        
        all_metrics.append(metrics.cpu().numpy())
        
        # ìƒ˜í”Œë³„ ê²°ê³¼ ì €ì¥
        sample_results.append({
            'stem': sample.stem,
            'metrics': metrics.cpu().numpy().tolist(),
        })
        
        # ì‹œê°í™”
        if visualize_dir:
            viz_path = visualize_dir / f"{sample_idx:04d}_{sample.stem}.png"
            visualize_full_image(
                rgb_path=sample.image_path,
                gt_depth=gt_data,
                pred_depth=prediction,
                stem=sample.stem,
                save_path=viz_path,
                min_depth=eval_namespace.min_depth,
                max_depth=eval_namespace.max_depth,
            )
            if args.debug:
                print(f"ì‹œê°í™” ì €ì¥: {viz_path}")
        
        processed_samples += 1

    # ì „ì²´ í‰ê·  ê³„ì‚°
    if all_metrics:
        mean_metrics = np.stack(all_metrics).mean(axis=0)
        
        print("\n" + "="*80)
        print("í‰ê°€ ìš”ì•½ (ì „ì²´ ì´ë¯¸ì§€ ê¸°ì¤€)")
        print("="*80)
        print(f"Samples: {len(all_metrics)}")
        print("-" * 80)
        for i, name in enumerate(METRIC_NAMES):
            print(f"{name:12s}: {mean_metrics[i]:.4f}")
        print("="*80)
        
        # JSON ì €ì¥
        if args.per_sample_json:
            output = {
                'metric_names': METRIC_NAMES,
                'samples': sample_results,
                'mean': mean_metrics.tolist(),
            }
            with open(args.per_sample_json, 'w') as f:
                json.dump(output, f, indent=2)
            print(f"\nâœ… ìƒ˜í”Œë³„ ê²°ê³¼ ì €ì¥: {args.per_sample_json}")
        
        # CSV ì €ì¥
        if args.output_file:
            import csv
            with open(args.output_file, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['stem'] + METRIC_NAMES)
                for result in sample_results:
                    writer.writerow([result['stem']] + result['metrics'])
            print(f"âœ… CSV ì €ì¥: {args.output_file}")
    else:
        print("\nâš ï¸  í‰ê°€ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
