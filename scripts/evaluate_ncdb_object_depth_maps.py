#!/usr/bin/env python3
"""NCDB object-masked depth evaluation.

segmentation_results/class_masks ì—ì„œ Mask2Former ì¸ìŠ¤í„´ìŠ¤ ë§ˆìŠ¤í¬ë¥¼ ì½ì–´ í•´ë‹¹ ê°ì²´
ì˜ì—­ì— í•œì •í•´ ê¹Šì´ ì˜ˆì¸¡ í’ˆì§ˆì„ ì •ëŸ‰ í‰ê°€í•©ë‹ˆë‹¤. ì˜ˆì¸¡ ê¹Šì´ëŠ” ì§€ì •í•œ ì²´í¬í¬ì¸íŠ¸ë¡œ
image_a6 ì´ë¯¸ì§€ë¥¼ ì¦‰ì‹œ ì¶”ë¡ í•˜ê±°ë‚˜, ê¸°ì¡´ ìºì‹œë¥¼ ì¬í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import argparse
import json
import math
import shutil
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import matplotlib.pyplot as plt
import numpy as np
import torch
from PIL import Image
from tqdm import tqdm

from packnet_sfm.datasets.augmentations import to_tensor
from packnet_sfm.models.model_wrapper import ModelWrapper
from packnet_sfm.utils.config import parse_test_file
from packnet_sfm.utils.depth import compute_depth_metrics, inv2depth, load_depth, post_process_inv_depth
from packnet_sfm.utils.image import load_image


DEFAULT_ALL_SPLITS = ["combined_train.json", "combined_val.json", "combined_test.json"]
METRIC_NAMES = ["abs_rel", "sqr_rel", "rmse", "rmse_log", "a1", "a2", "a3"]


@dataclass
class SampleEntry:
    stem: str
    sequence_root: Path
    image_path: Path
    gt_path: Path
    segmentation_root: Path
    prediction_path: Path


@dataclass
class InstanceResult:
    stem: str
    class_name: str
    mask_path: Path
    valid_pixels: int
    metrics: List[float]
    gt_mean_depth: float = 0.0
    gt_median_depth: float = 0.0


@dataclass
class SampleData:
    """í”½ì…€ ë ˆë²¨ ê±°ë¦¬ë³„ í‰ê°€ë¥¼ ìœ„í•œ ì›ë³¸ ë°ì´í„°"""
    stem: str
    gt_depth: np.ndarray
    pred_depth: np.ndarray
    mask: np.ndarray
    class_name: str


class MetricsAccumulator:
    """ë‹¨ìˆœ í‰ê·  ë©”íŠ¸ë¦­ ëˆ„ì‚°ê¸°."""

    def __init__(self, metric_names: Sequence[str]):
        self.metric_names = list(metric_names)
        self._rows: List[np.ndarray] = []

    def add(self, tensor: torch.Tensor) -> None:
        self._rows.append(tensor.detach().cpu().numpy())

    def count(self) -> int:
        return len(self._rows)

    def mean(self) -> List[float]:
        if not self._rows:
            return [math.nan for _ in self.metric_names]
        stacked = np.stack(self._rows, axis=0)
        return stacked.mean(axis=0).tolist()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Evaluate depth on NCDB object masks")
    parser.add_argument("--dataset-root", type=str, required=True,
                        help="ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ (splits ë””ë ‰í† ë¦¬ë¥¼ í¬í•¨í•˜ëŠ” ìµœìƒìœ„ í´ë”)")
    parser.add_argument("--split-files", type=str, nargs="*",
                        help="í‰ê°€ì— ì‚¬ìš©í•  split JSON (dataset-root ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ í˜¹ì€ ì ˆëŒ€ê²½ë¡œ)")
    parser.add_argument("--use-all-splits", action="store_true",
                        help="combined_train/val/test ì„¸ ê°€ì§€ split ì„ ëª¨ë‘ ë¡œë“œ")
    parser.add_argument("--splits-dir", type=str, default="splits",
                        help="split-files ê°€ ìƒëŒ€ê²½ë¡œì¼ ë•Œ ê¸°ì¤€ì´ ë  ë””ë ‰í† ë¦¬")

    parser.add_argument("--segmentation-root", type=str, required=True,
                        help="segmentation ê²°ê³¼ê°€ ìœ„ì¹˜í•œ í´ë”ëª… ë˜ëŠ” ì ˆëŒ€ê²½ë¡œ (ì˜ˆ: segmentation_results)")
    parser.add_argument("--class-mask-subdir", type=str, default="class_masks",
                        help="segmentation-root í•˜ìœ„ì—ì„œ í´ë˜ìŠ¤ë³„ ë§ˆìŠ¤í¬ê°€ ìœ„ì¹˜í•œ ì„œë¸Œí´ë”ëª…")
    parser.add_argument("--pred-root", type=str, required=True,
                        help="ì˜ˆì¸¡ ê¹Šì´ë§µì„ ì €ì¥/ë¶ˆëŸ¬ì˜¬ í´ë”ëª… ë˜ëŠ” ì ˆëŒ€ê²½ë¡œ")
    parser.add_argument("--gt-root", type=str, required=True,
                        help="GT ê¹Šì´ë§µì´ ìœ„ì¹˜í•œ í´ë”ëª… ë˜ëŠ” ì ˆëŒ€ê²½ë¡œ (ì˜ˆ: newest_depth_maps)")
    parser.add_argument("--image-subdir", type=str, default="image_a6",
                        help="RGB ì…ë ¥ ì´ë¯¸ì§€ê°€ ìœ„ì¹˜í•œ ì„œë¸Œí´ë”ëª…")

    parser.add_argument("--checkpoint", type=str, required=True,
                        help="PackNet-SfM ì²´í¬í¬ì¸íŠ¸(.ckpt). ì§€ì • ëª¨ë¸ë¡œ on-the-fly ì¶”ë¡ ")
    parser.add_argument("--image-shape", type=int, nargs=2, default=None,
                        help="ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  (H W). ë¯¸ì§€ì • ì‹œ ì²´í¬í¬ì¸íŠ¸ config")
    parser.add_argument("--flip-tta", action="store_true",
                        help="ì¢Œìš° flip test-time augmentation ì ìš©")

    parser.add_argument("--classes", type=str, nargs="*", default=None,
                        help="í‰ê°€í•  í´ë˜ìŠ¤ ì´ë¦„ ëª©ë¡. ë¯¸ì§€ì • ì‹œ ì²« ìƒ˜í”Œì—ì„œ ìë™ ì¶”ë¡ ")
    parser.add_argument("--output-file", type=str, default="metrics_object_masks.txt",
                        help="ìµœì¢… ìš”ì•½ ë©”íŠ¸ë¦­ì„ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--per-instance-json", type=str, default=None,
                        help="ê° ì¸ìŠ¤í„´ìŠ¤ë³„ ì„¸ë¶€ ë©”íŠ¸ë¦­ì„ JSON ìœ¼ë¡œ ì €ì¥")

    parser.add_argument("--min-depth", type=float, default=0.3, help="í‰ê°€ ìµœì†Œ ê¹Šì´")
    parser.add_argument("--max-depth", type=float, default=100.0, help="í‰ê°€ ìµœëŒ€ ê¹Šì´")
    parser.add_argument("--crop", type=str, default="", choices=["", "garg"], help="ì ìš©í•  crop")
    parser.add_argument("--scale-output", type=str, default="top-center",
                        help="ì˜ˆì¸¡ ê¹Šì´ë¥¼ GT í•´ìƒë„ë¡œ ë§ì¶œ ë•Œ ì‚¬ìš©í•  ëª¨ë“œ")
    parser.add_argument("--use-gt-scale", action="store_true",
                        help="GT median scaling ì ìš© ì—¬ë¶€")

    parser.add_argument("--device", type=str, default=None,
                        help="torch device (ì˜ˆ: cuda:0). ìƒëµ ì‹œ GPU ìš°ì„ ")
    parser.add_argument("--dtype", type=str, choices=["fp16", "fp32"], default=None,
                        help="ëª¨ë¸ ì¶”ë¡  dtype override")
    parser.add_argument("--debug", action="store_true", help="ì¶”ê°€ ë¡œê·¸ ì¶œë ¥")
    parser.add_argument("--max-samples", type=int, default=None,
                        help="ë””ë²„ê¹…ìš©: ì²˜ë¦¬í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ")
    parser.add_argument("--visualize-dir", type=str, default=None,
                        help="ì‹œê°í™” ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬")
    parser.add_argument("--distance-metric", type=str, default="median", choices=["mean", "median"],
                        help="ê±°ë¦¬ ë²”ìœ„ ë¶„ë¥˜ ì‹œ ì‚¬ìš©í•  GT í†µê³„ (ê¸°ë³¸ê°’: median)")
    
    # Output structure arguments
    parser.add_argument("--output-root", type=str, default="outputs",
                        help="ëª¨ë“  ê²°ê³¼ë¥¼ ì €ì¥í•  ìµœìƒìœ„ ë””ë ‰í† ë¦¬")
    parser.add_argument("--save-rgb", action="store_true", default=False,
                        help="RGB ì´ë¯¸ì§€ ë³µì‚¬ ì—¬ë¶€")
    parser.add_argument("--save-gt", action="store_true", default=False,
                        help="GT depth ë³µì‚¬ ì—¬ë¶€")
    parser.add_argument("--save-pred", action="store_true", default=False,
                        help="ì˜ˆì¸¡ depth ë³µì‚¬ ì—¬ë¶€")

    args = parser.parse_args()

    if not args.split_files and not args.use_all_splits:
        parser.error("--split-files ë˜ëŠ” --use-all-splits ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

    return args


def resolve_path(base: Path, maybe_path: str) -> Path:
    path = Path(maybe_path)
    return path if path.is_absolute() else base / path


def discover_split_files(args: argparse.Namespace) -> List[Path]:
    dataset_root = Path(args.dataset_root)
    split_paths: List[Path] = []

    if args.use_all_splits:
        splits_dir = resolve_path(dataset_root, args.splits_dir)
        for name in DEFAULT_ALL_SPLITS:
            candidate = splits_dir / name
            if not candidate.exists():
                raise FileNotFoundError(f"split íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {candidate}")
            split_paths.append(candidate)

    if args.split_files:
        for item in args.split_files:
            candidate = Path(item)
            if not candidate.is_absolute():
                candidate = resolve_path(dataset_root, args.splits_dir) / item
            if not candidate.exists():
                raise FileNotFoundError(f"split íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {candidate}")
            split_paths.append(candidate)

    unique_paths: List[Path] = []
    seen = set()
    for path in split_paths:
        if path in seen:
            continue
        unique_paths.append(path)
        seen.add(path)
    return unique_paths


def load_split_entries(args: argparse.Namespace, split_paths: Iterable[Path]) -> List[dict]:
    entries: List[dict] = []
    for split_path in split_paths:
        with open(split_path, "r") as f:
            data = json.load(f)
        if not isinstance(data, list):
            raise ValueError(f"split í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤: {split_path}")
        entries.extend(data)
    if not entries:
        raise RuntimeError("split ì— ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
    return entries


def get_checkpoint_id(checkpoint_path: str) -> str:
    """ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œì—ì„œ ê³ ìœ  ID ì¶”ì¶œ"""
    ckpt_path = Path(checkpoint_path)
    
    # íŒŒì¼ëª…ì—ì„œ .ckpt ì œê±°
    basename = ckpt_path.stem
    
    # ê²½ë¡œì— íŠ¹ì • íŒ¨í„´ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    # ì˜ˆ: checkpoints/resnetsan01_640x384_newest_test_fixed_method_0.3_100_silog_1.0/... 
    #     -> resnetsan01_640x384_newest_test_fixed_method_0.3_100_silog_1.0
    if ckpt_path.parent.name and ckpt_path.parent.name.startswith('resnetsan'):
        return ckpt_path.parent.name
    
    # ë‹¨ìˆœ íŒŒì¼ëª… ì‚¬ìš© (ì˜ˆ: ResNet-SAN_0.5to100.ckpt -> ResNet-SAN_0.5to100)
    return basename


def normalize_entry(args: argparse.Namespace, dataset_root: Path, entry: dict, checkpoint_id: str) -> SampleEntry:
    if "new_filename" not in entry:
        raise ValueError(f"split í•­ëª©ì— new_filename ì´ ì—†ìŠµë‹ˆë‹¤: {entry}")

    stem = entry["new_filename"]

    sequence_root_raw = entry.get("dataset_root")
    if sequence_root_raw:
        sequence_root = Path(sequence_root_raw)
        if not sequence_root.is_absolute():
            sequence_root = dataset_root / sequence_root
    else:
        sequence_root = dataset_root

    if not sequence_root.exists():
        raise FileNotFoundError(f"sequence_root ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sequence_root}")

    # âœ… RGB ì´ë¯¸ì§€ëŠ” í•­ìƒ dataset_root/sequence_rootì™€ ê°™ì€ ìœ„ì¹˜ì—ì„œ ê°€ì ¸ì˜´
    # (GT/Maskì™€ ì •ë ¬ì„ ìœ„í•´ image_pathëŠ” ë¬´ì‹œ)
    image_path = resolve_path(sequence_root, args.image_subdir) / f"{stem}.png"
    
    if not image_path.exists():
        # fallback: splitì— ëª…ì‹œëœ image_path ì‚¬ìš©
        image_path_raw = entry.get("image_path")
        if image_path_raw:
            fallback_path = Path(image_path_raw) if Path(image_path_raw).is_absolute() else sequence_root / image_path_raw
            if fallback_path.exists():
                print(f"âš ï¸  WARNING: RGB ì´ë¯¸ì§€ë¥¼ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ì—ì„œ ê°€ì ¸ì˜´: {fallback_path}")
                image_path = fallback_path
            else:
                raise FileNotFoundError(f"RGB ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")

    segmentation_root = resolve_path(sequence_root, args.segmentation_root)
    class_mask_root = segmentation_root / args.class_mask_subdir if args.class_mask_subdir else segmentation_root

    gt_candidates = [resolve_path(sequence_root, args.gt_root) / f"{stem}.png",
                     resolve_path(sequence_root, args.gt_root) / f"{stem}.npz"]
    gt_path = next((p for p in gt_candidates if p.exists()), gt_candidates[0])

    # âœ… ì²´í¬í¬ì¸íŠ¸ë³„ í´ë” ìƒì„±: pred_root/checkpoint_id/
    pred_root_base = resolve_path(sequence_root, args.pred_root)
    pred_root = pred_root_base / checkpoint_id
    pred_path = pred_root / f"{stem}.npz"

    return SampleEntry(
        stem=stem,
        sequence_root=sequence_root,
        image_path=image_path,
        gt_path=gt_path,
        segmentation_root=class_mask_root,
        prediction_path=pred_path,
    )


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def create_output_structure(checkpoint_id: str, output_root: str) -> Dict[str, Path]:
    """ì²´í¬í¬ì¸íŠ¸ë³„ ì¶œë ¥ í´ë” êµ¬ì¡° ìƒì„±"""
    base = Path(output_root) / f"{checkpoint_id}_results"
    
    structure = {
        'rgb': base / 'rgb',
        'gt': base / 'gt',
        'pred': base / 'pred',
        'viz': base / 'viz',
        'metrics': base / 'metrics'
    }
    
    for path in structure.values():
        ensure_dir(path)
    
    return structure


def copy_file_to_output(src: Path, dst_dir: Path, new_name: Optional[str] = None) -> None:
    """íŒŒì¼ì„ ì¶œë ¥ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬"""
    if not src.exists():
        return
    
    dst_name = new_name if new_name else src.name
    dst_path = dst_dir / dst_name
    
    shutil.copy2(src, dst_path)


def save_depth_as_png(depth: np.ndarray, path: Path, scale: float = 256.0) -> None:
    """Depthë¥¼ 16-bit PNGë¡œ ì €ì¥ (meter to mm ë³€í™˜)"""
    # depth: (H, W) in meters
    depth_mm = (depth * scale).astype(np.uint16)
    Image.fromarray(depth_mm).save(path)


def load_mask(mask_path: Path, target_shape: Tuple[int, int]) -> np.ndarray:
    mask = Image.open(mask_path).convert("L")
    if mask.size != (target_shape[1], target_shape[0]):
        mask = mask.resize((target_shape[1], target_shape[0]), Image.NEAREST)
    mask_arr = (np.array(mask) > 0).astype(np.float32)
    return mask_arr


def collect_masks_for_stem(segmentation_root: Path, class_names: Sequence[str], stem: str) -> Dict[str, List[Path]]:
    result: Dict[str, List[Path]] = {}
    if not segmentation_root.exists():
        return result
    for class_name in class_names:
        class_dir = segmentation_root / class_name
        if not class_dir.exists():
            continue
        pattern = f"{stem}*.png"
        files = sorted(class_dir.glob(pattern))
        if files:
            result[class_name] = files
    return result


@dataclass
class ModelContext:
    wrapper: ModelWrapper
    device: torch.device
    dtype: torch.dtype
    image_shape: Tuple[int, int]


def prepare_model(args: argparse.Namespace) -> ModelContext:
    config, state_dict = parse_test_file(args.checkpoint)

    wrapper = ModelWrapper(config, load_datasets=False)
    wrapper.load_state_dict(state_dict, strict=False)

    if args.dtype == "fp16":
        dtype = torch.float16
    elif args.dtype == "fp32":
        dtype = torch.float32
    else:
        dtype = torch.float16 if getattr(config.arch, "dtype", None) == torch.float16 else torch.float32

    if args.device:
        device = torch.device(args.device)
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    wrapper = wrapper.to(device=device, dtype=dtype)
    wrapper.eval()

    if args.image_shape is not None:
        image_shape = (int(args.image_shape[0]), int(args.image_shape[1]))
    else:
        aug_shape = getattr(getattr(config, "datasets", None), "augmentation", None)
        if aug_shape is not None:
            image_shape = tuple(map(int, getattr(aug_shape, "image_shape")))
        else:
            raise RuntimeError("image_shape ë¥¼ config ë˜ëŠ” ì¸ìë¡œë¶€í„° ê²°ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ëª¨ë¸ì—ì„œ min_depth, max_depth ì½ì–´ì˜¤ê¸°
    depth_net = wrapper.depth_net if hasattr(wrapper, 'depth_net') else None
    if depth_net is not None and hasattr(depth_net, 'min_depth') and hasattr(depth_net, 'max_depth'):
        model_min_depth = float(depth_net.min_depth)
        model_max_depth = float(depth_net.max_depth)
        print(f"\nğŸ“Š ëª¨ë¸ì—ì„œ ì½ì–´ì˜¨ depth ë²”ìœ„:")
        print(f"   min_depth: {model_min_depth}")
        print(f"   max_depth: {model_max_depth}")
        
        # argsì— ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ëª¨ë¸ ê°’ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ
        if args.min_depth == 0.3 and args.max_depth == 100.0:  # ê¸°ë³¸ê°’ì¸ ê²½ìš°
            print(f"   âš ï¸  í‰ê°€ ì„¤ì •ì„ ëª¨ë¸ ê°’ìœ¼ë¡œ ìë™ ì¡°ì •")
            args.min_depth = model_min_depth
            args.max_depth = model_max_depth
        elif abs(args.min_depth - model_min_depth) > 0.01 or abs(args.max_depth - model_max_depth) > 0.1:
            print(f"   âš ï¸  WARNING: í‰ê°€ ì„¤ì •ê³¼ ëª¨ë¸ í•™ìŠµ ì„¤ì •ì´ ë‹¤ë¦…ë‹ˆë‹¤!")
            print(f"   í‰ê°€: min={args.min_depth}, max={args.max_depth}")
            print(f"   ëª¨ë¸: min={model_min_depth}, max={model_max_depth}")

    return ModelContext(wrapper=wrapper, device=device, dtype=dtype, image_shape=image_shape)


def run_inference(context: ModelContext, image_path: Path, flip_tta: bool) -> np.ndarray:
    img = load_image(str(image_path)).convert("RGB")
    if img.size != (context.image_shape[1], context.image_shape[0]):
        img = img.resize((context.image_shape[1], context.image_shape[0]), Image.LANCZOS)
    img_tensor = to_tensor(img).unsqueeze(0).to(device=context.device, dtype=context.dtype)

    with torch.no_grad():
        inv_depth = context.wrapper.depth(img_tensor)["inv_depths"][0]
        if flip_tta:
            flipped = torch.flip(img_tensor, dims=[3])
            inv_depth_f = context.wrapper.depth(flipped)["inv_depths"][0]
            inv_depth = post_process_inv_depth(inv_depth, inv_depth_f, method="mean")
        depth = inv2depth(inv_depth).squeeze().detach().cpu().float().numpy()

    return depth


def load_prediction(prediction_path: Path) -> Optional[np.ndarray]:
    if not prediction_path.exists():
        return None
    try:
        data = np.load(prediction_path)
        if isinstance(data, np.lib.npyio.NpzFile):
            if "depth" in data:
                return data["depth"]
            return data[list(data.files)[0]]
        return data
    except Exception:
        return None


def save_prediction(prediction_path: Path, depth: np.ndarray) -> None:
    ensure_dir(prediction_path.parent)
    np.savez_compressed(prediction_path, depth=depth.astype(np.float32))


def make_eval_namespace(args: argparse.Namespace) -> argparse.Namespace:
    return argparse.Namespace(
        min_depth=args.min_depth,
        max_depth=args.max_depth,
        crop=args.crop,
        scale_output=args.scale_output,
    )


def print_summary_table(class_metrics: Dict[str, Tuple[List[float], int]], 
                        overall: Tuple[List[float], int],
                        full_image_metrics: Optional[Tuple[List[float], int]] = None) -> None:
    header = ["Class", "Count"] + METRIC_NAMES
    rows = []
    for class_name, (metrics, count) in sorted(class_metrics.items()):
        rows.append([class_name, str(count)] + [f"{m:.4f}" if not math.isnan(m) else "nan" for m in metrics])
    rows.append(["car+road", str(overall[1])] + [f"{m:.4f}" if not math.isnan(m) else "nan" for m in overall[0]])
    
    # ì „ì²´ í”½ì…€ ë©”íŠ¸ë¦­ ì¶”ê°€ (full image)
    if full_image_metrics is not None:
        rows.append(["ALL", str(full_image_metrics[1])] + [f"{m:.4f}" if not math.isnan(m) else "nan" for m in full_image_metrics[0]])

    col_widths = [max(len(row[i]) for row in rows + [header]) for i in range(len(header))]

    def print_row(row: Sequence[str]) -> None:
        print("  ".join(word.ljust(col_widths[i]) for i, word in enumerate(row)))

    print("\ní‰ê°€ ìš”ì•½ (ê°ì²´ ë§ˆìŠ¤í¬ ë° ì „ì²´ ê¸°ì¤€)")
    print_row(header)
    print("-" * (sum(col_widths) + 2 * (len(col_widths) - 1)))
    for row in rows:
        print_row(row)


def visualize_sample(
    image_path: Path,
    gt_depth: np.ndarray,
    pred_depth: np.ndarray,
    mask: np.ndarray,
    save_path: Path,
    class_name: str,
    stem: str,
    metrics: Optional[List[float]] = None,
) -> None:
    """ì´ë¯¸ì§€, GT, Pred, ë§ˆìŠ¤í¬ë¥¼ Gradient ì—ëŸ¬ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™” (4-panel)"""
    from matplotlib.colors import LinearSegmentedColormap
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # [1] RGB + ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´
    img = Image.open(image_path).convert("RGB")
    img_array = np.array(img)
    
    # ë§ˆìŠ¤í¬ ì˜ì—­ì„ ë°˜íˆ¬ëª… ì´ˆë¡ìƒ‰ìœ¼ë¡œ ì˜¤ë²„ë ˆì´
    overlay = img_array.copy()
    mask_bool = mask > 0
    overlay[mask_bool, 1] = np.clip(overlay[mask_bool, 1] + 100, 0, 255)  # Green channel boost
    
    # ì•ŒíŒŒ ë¸”ë Œë”© (70% ì›ë³¸, 30% ì˜¤ë²„ë ˆì´)
    img_with_mask = (0.7 * img_array + 0.3 * overlay).astype(np.uint8)
    
    axes[0, 0].imshow(img_with_mask)
    axes[0, 0].set_title(f"RGB + Mask Overlay\n{stem}", fontsize=12, fontweight='bold')
    axes[0, 0].axis('off')
    
    # [2] GT Depth (ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ)
    gt_masked = gt_depth.copy()
    gt_masked[mask == 0] = np.nan
    gt_max = np.nanpercentile(gt_masked, 99) if np.any(~np.isnan(gt_masked)) else 1.0
    im1 = axes[0, 1].imshow(gt_masked, cmap='viridis', vmin=0, vmax=gt_max)
    axes[0, 1].set_title(f"GT Depth (masked)\n{class_name}", fontsize=12, fontweight='bold')
    axes[0, 1].axis('off')
    plt.colorbar(im1, ax=axes[0, 1], label='Depth (m)', fraction=0.046, pad=0.04)
    
    # [3] Pred Depth (ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ)
    pred_masked = pred_depth.copy()
    pred_masked[mask == 0] = np.nan
    im2 = axes[1, 0].imshow(pred_masked, cmap='viridis', vmin=0, vmax=gt_max)
    axes[1, 0].set_title(f"Predicted Depth", fontsize=12, fontweight='bold')
    axes[1, 0].axis('off')
    plt.colorbar(im2, ax=axes[1, 0], label='Depth (m)', fraction=0.046, pad=0.04)
    
    # [4] Error Heatmap (Gradient Green->Yellow->Orange->Red)
    valid_mask = (mask > 0) & (gt_depth > 0)
    
    # ì—ëŸ¬ ë§µ ìƒì„±: ë§ˆìŠ¤í¬ ë°–ì€ íšŒìƒ‰(NaN ëŒ€ì‹  -1), ë§ˆìŠ¤í¬ ì•ˆì€ abs_rel ê°’
    abs_rel_map = np.full_like(gt_depth, -1.0, dtype=np.float32)  # ê¸°ë³¸ê°’ -1 (íšŒìƒ‰ìœ¼ë¡œ í‘œì‹œ)
    
    if np.any(valid_mask):
        # ìœ íš¨í•œ í”½ì…€ë§Œ abs_rel ê³„ì‚°
        abs_rel_values = np.abs(pred_depth[valid_mask] - gt_depth[valid_mask]) / gt_depth[valid_mask]
        abs_rel_map[valid_mask] = np.clip(abs_rel_values, 0, 0.5)  # 0.5 ì´ìƒì€ 0.5ë¡œ í´ë¦¬í•‘
    
    # Gradient colormap with gray for masked-out areas
    # -1: íšŒìƒ‰(ë§ˆìŠ¤í¬ ë°–), 0: ì´ˆë¡(ì™„ë²½), 0.5: ë¹¨ê°•(ë‚˜ì¨)
    from matplotlib.colors import ListedColormap
    colors_list = ['#cccccc']  # -1 ê°’ìš© íšŒìƒ‰
    gradient_colors = ['#00ff00', '#ffff00', '#ff8000', '#ff0000']  # green->yellow->orange->red
    n_gradient = 256
    gradient_cmap = LinearSegmentedColormap.from_list('gradient', gradient_colors, N=n_gradient)
    colors_list.extend([gradient_cmap(i) for i in range(n_gradient)])
    
    # -1~0.5 ë²”ìœ„ë¥¼ 0~257 ì¸ë±ìŠ¤ë¡œ ë§¤í•‘
    # -1 -> 0 (íšŒìƒ‰), 0~0.5 -> 1~256 (ê·¸ë¼ë””ì–¸íŠ¸)
    display_map = abs_rel_map.copy()
    display_map[abs_rel_map >= 0] = (abs_rel_map[abs_rel_map >= 0] / 0.5) * (n_gradient - 1) + 1
    display_map[abs_rel_map < 0] = 0
    
    combined_cmap = ListedColormap(colors_list)
    im3 = axes[1, 1].imshow(display_map, cmap=combined_cmap, vmin=0, vmax=n_gradient)
    axes[1, 1].set_title(f"Error Heatmap (abs_rel)\nGreen=Good, Red=Bad, Gray=No mask", fontsize=12, fontweight='bold')
    axes[1, 1].axis('off')
    
    # ColorbarëŠ” 0~0.5 ë²”ìœ„ë§Œ í‘œì‹œ
    cbar = plt.colorbar(im3, ax=axes[1, 1], label='Absolute Relative Error', fraction=0.046, pad=0.04, 
                        ticks=[1, n_gradient//4, n_gradient//2, 3*n_gradient//4, n_gradient])
    cbar.ax.set_yticklabels(['0.0', '0.125', '0.25', '0.375', '0.5+'])
    cbar.ax.set_ylabel('abs_rel (0.0=Perfect, 0.5+=Bad)', rotation=270, labelpad=20)
    
    # í†µê³„ í…ìŠ¤íŠ¸
    valid_errors = abs_rel_map[abs_rel_map >= 0]  # -1ì´ ì•„ë‹Œ ì‹¤ì œ ì—ëŸ¬ ê°’ë§Œ
    if len(valid_errors) > 0:
        stats_text = f"""Statistics (masked area):
  Mean abs_rel: {valid_errors.mean():.4f}  |  Median abs_rel: {np.median(valid_errors):.4f}  |  Total pixels: {len(valid_errors):,}

Pixel Distribution:
  [Excellent] (0.00-0.05): {(valid_errors < 0.05).sum():6,d} ({100*(valid_errors < 0.05).mean():5.1f}%)
  [Good]      (0.05-0.10): {((valid_errors >= 0.05) & (valid_errors < 0.10)).sum():6,d} ({100*((valid_errors >= 0.05) & (valid_errors < 0.10)).mean():5.1f}%)
  [Fair]      (0.10-0.20): {((valid_errors >= 0.10) & (valid_errors < 0.20)).sum():6,d} ({100*((valid_errors >= 0.10) & (valid_errors < 0.20)).mean():5.1f}%)
  [Poor]      (0.20-0.30): {((valid_errors >= 0.20) & (valid_errors < 0.30)).sum():6,d} ({100*((valid_errors >= 0.20) & (valid_errors < 0.30)).mean():5.1f}%)
  [Bad]       (0.30+):     {(valid_errors >= 0.30).sum():6,d} ({100*(valid_errors >= 0.30).mean():5.1f}%)"""
        
        if metrics:
            stats_text += f"\n\nDepth Metrics: abs_rel={metrics[0]:.4f}, rmse={metrics[2]:.4f}, a1={metrics[4]:.4f}"
        
        fig.text(0.5, 0.02, stats_text, ha='center', fontsize=10, family='monospace',
                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
    
    plt.tight_layout(rect=[0, 0.15, 1, 1])
    ensure_dir(save_path.parent)
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close(fig)


def analyze_by_distance_ranges(
    instance_results: List[InstanceResult],
    dist_ranges: List[Tuple[str, float, float]],
    use_median: bool = True
) -> None:
    """ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê±°ë¦¬ ë²”ìœ„ë³„ë¡œ ë¶„ë¥˜í•˜ê³  ë©”íŠ¸ë¦­ì„ ì§‘ê³„ (êµ¬ ë°©ì‹ - ì°¸ê³ ìš©)"""
    
    # ê±°ë¦¬ ë²”ìœ„ë³„ ê·¸ë£¹í™”
    distance_groups: Dict[str, List[InstanceResult]] = {name: [] for name, _, _ in dist_ranges}
    
    for instance in instance_results:
        # "_ALL" í†µí•© ë§ˆìŠ¤í¬ëŠ” ì œì™¸ (ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ë§Œ ë¶„ì„)
        if instance.class_name.endswith("_ALL"):
            continue
            
        representative_distance = instance.gt_median_depth if use_median else instance.gt_mean_depth
        
        # ê±°ë¦¬ ë²”ìœ„ ê²°ì •
        for range_name, min_d, max_d in dist_ranges:
            if min_d <= representative_distance < max_d:
                distance_groups[range_name].append(instance)
                break
    
    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    header = ["Range", "Count"] + METRIC_NAMES
    rows = []
    
    for range_name, min_d, max_d in dist_ranges:
        instances = distance_groups[range_name]
        count = len(instances)
        
        if count == 0:
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° NaNìœ¼ë¡œ í‘œì‹œ
            rows.append([range_name, "0"] + ["nan"] * len(METRIC_NAMES))
        else:
            # ë©”íŠ¸ë¦­ í‰ê·  ê³„ì‚°
            metrics_array = np.array([inst.metrics for inst in instances])
            avg_metrics = metrics_array.mean(axis=0)
            rows.append([range_name, str(count)] + [f"{m:.4f}" for m in avg_metrics])
    
    # ì¶œë ¥
    col_widths = [max(len(row[i]) for row in rows + [header]) for i in range(len(header))]
    
    def print_row(row: Sequence[str]) -> None:
        print("  ".join(word.ljust(col_widths[i]) for i, word in enumerate(row)))
    
    distance_type = "Median" if use_median else "Mean"
    print(f"\nê±°ë¦¬ë³„ í‰ê°€ ê²°ê³¼ (Instance {distance_type} ê¸°ì¤€ - êµ¬ ë°©ì‹)")
    print_row(header)
    print("-" * (sum(col_widths) + 2 * (len(col_widths) - 1)))
    for row in rows:
        print_row(row)


def analyze_by_distance_ranges_pixel_level(
    samples_data: List[SampleData],
    dist_ranges: List[Tuple[str, float, float]],
    eval_namespace: argparse.Namespace,
    class_filter: Optional[str] = None
) -> List[Tuple[str, int, List[float]]]:
    """í”½ì…€ ë ˆë²¨ë¡œ ê±°ë¦¬ ë²”ìœ„ë³„ í‰ê°€ (ì‹ ê·œ ì •í™•í•œ ë°©ì‹)
    
    Args:
        samples_data: ìƒ˜í”Œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        dist_ranges: ê±°ë¦¬ ë²”ìœ„ ë¦¬ìŠ¤íŠ¸
        eval_namespace: í‰ê°€ ì„¤ì •
        class_filter: íŠ¹ì • í´ë˜ìŠ¤ë§Œ í•„í„°ë§ (Noneì´ë©´ ì „ì²´)
        
    Returns:
        List of (range_name, pixel_count, metrics_list)
    """
    
    # 1. ê±°ë¦¬ë³„ë¡œ í”½ì…€ ìˆ˜ì§‘
    range_pixels: Dict[str, Dict[str, List[float]]] = {}
    
    for sample in samples_data:
        # í´ë˜ìŠ¤ í•„í„° ì ìš©
        if class_filter is not None and sample.class_name != class_filter:
            continue
            
        valid_mask = (sample.mask > 0) & (sample.gt_depth > 0)
        gt_valid = sample.gt_depth[valid_mask]
        pred_valid = sample.pred_depth[valid_mask]
        
        for range_name, min_d, max_d in dist_ranges:
            # ì´ ë²”ìœ„ì— ì†í•˜ëŠ” í”½ì…€ í•„í„°ë§
            in_range = (gt_valid >= min_d) & (gt_valid < max_d)
            
            if not np.any(in_range):
                continue
            
            if range_name not in range_pixels:
                range_pixels[range_name] = {'gt': [], 'pred': []}
            
            range_pixels[range_name]['gt'].extend(gt_valid[in_range].tolist())
            range_pixels[range_name]['pred'].extend(pred_valid[in_range].tolist())
    
    # 2. ê° ë²”ìœ„ë³„ ë©”íŠ¸ë¦­ ê³„ì‚° (ì§ì ‘ ê³„ì‚° - min/max depth í•„í„°ë§ ì—†ì´)
    header = ["Range", "Pixels"] + METRIC_NAMES
    rows = []
    results = []  # (range_name, pixel_count, metrics_list) ì €ì¥
    
    for range_name, min_d, max_d in dist_ranges:
        if range_name not in range_pixels or len(range_pixels[range_name]['gt']) == 0:
            rows.append([range_name, "0"] + ["nan"] * len(METRIC_NAMES))
            results.append((range_name, 0, [float('nan')] * len(METRIC_NAMES)))
            continue
        
        # GT/Pred ë°°ì—´ (ì´ë¯¸ í•„í„°ë§ëœ ë°ì´í„°)
        gt_array = np.array(range_pixels[range_name]['gt'])
        pred_array = np.array(range_pixels[range_name]['pred'])
        
        # âœ… ë©”íŠ¸ë¦­ ì§ì ‘ ê³„ì‚° (compute_depth_metricsì˜ min/max í•„í„°ë§ ìš°íšŒ)
        gt_tensor = torch.from_numpy(gt_array).float()
        pred_tensor = torch.from_numpy(pred_array).float()
        
        # abs_rel, sqr_rel, rmse, rmse_log, a1, a2, a3
        thresh = torch.max((gt_tensor / pred_tensor), (pred_tensor / gt_tensor))
        a1 = (thresh < 1.25).float().mean()
        a2 = (thresh < 1.25 ** 2).float().mean()
        a3 = (thresh < 1.25 ** 3).float().mean()
        
        diff = gt_tensor - pred_tensor
        abs_rel = torch.mean(torch.abs(diff) / gt_tensor)
        sq_rel = torch.mean(diff ** 2 / gt_tensor)
        rmse = torch.sqrt(torch.mean(diff ** 2))
        rmse_log = torch.sqrt(torch.mean((torch.log(gt_tensor) - torch.log(pred_tensor)) ** 2))
        
        metrics_list = [abs_rel.item(), sq_rel.item(), rmse.item(), rmse_log.item(),
                       a1.item(), a2.item(), a3.item()]
        
        rows.append([range_name, str(len(gt_array))] + 
                   [f"{m:.4f}" for m in metrics_list])
        results.append((range_name, len(gt_array), metrics_list))
    
    # í…Œì´ë¸” ì¶œë ¥
    col_widths = [max(len(row[i]) for row in rows + [header]) for i in range(len(header))]
    
    def print_row(row: Sequence[str]) -> None:
        print("  ".join(word.ljust(col_widths[i]) for i, word in enumerate(row)))
    
    # í´ë˜ìŠ¤ë³„ ì œëª© ì¶œë ¥
    if class_filter:
        print(f"\nê±°ë¦¬ë³„ í‰ê°€ ê²°ê³¼ [{class_filter.upper()}] (í”½ì…€ ë ˆë²¨)")
    else:
        print(f"\nê±°ë¦¬ë³„ í‰ê°€ ê²°ê³¼ [ALL] (í”½ì…€ ë ˆë²¨)")
    print_row(header)
    print("-" * (sum(col_widths) + 2 * (len(col_widths) - 1)))
    for row in rows:
        print_row(row)
    
    return results


def print_distance_error_distribution(
    samples_data: List[SampleData],
    dist_ranges: List[Tuple[str, float, float]]
) -> None:
    """ê±°ë¦¬ë³„ ì—ëŸ¬ ë“±ê¸‰ ë¶„í¬ ì¶œë ¥"""
    
    error_bins = [
        ("[Excellent]", 0.00, 0.05),
        ("[Good]",      0.05, 0.10),
        ("[Fair]",      0.10, 0.20),
        ("[Poor]",      0.20, 0.30),
        ("[Bad]",       0.30, float('inf'))
    ]
    
    print("\nê±°ë¦¬ë³„ ì—ëŸ¬ ë¶„í¬")
    print("=" * 70)
    
    for range_name, min_d, max_d in dist_ranges:
        # í”½ì…€ ìˆ˜ì§‘ ë° abs_rel ê³„ì‚°
        all_abs_rel = []
        
        for sample in samples_data:
            valid_mask = (sample.mask > 0) & (sample.gt_depth > 0)
            gt_valid = sample.gt_depth[valid_mask]
            pred_valid = sample.pred_depth[valid_mask]
            
            # ì´ ë²”ìœ„ì— ì†í•˜ëŠ” í”½ì…€ í•„í„°ë§
            in_range = (gt_valid >= min_d) & (gt_valid < max_d)
            
            if not np.any(in_range):
                continue
            
            # abs_rel ê³„ì‚°
            abs_rel = np.abs(pred_valid[in_range] - gt_valid[in_range]) / gt_valid[in_range]
            all_abs_rel.extend(abs_rel.tolist())
        
        if len(all_abs_rel) == 0:
            print(f"\n{range_name}: No pixels")
            continue
        
        abs_rel_array = np.array(all_abs_rel)
        total = len(abs_rel_array)
        
        print(f"\n{range_name} ({total:,} pixels)")
        print("-" * 70)
        
        # ì—ëŸ¬ ë“±ê¸‰ë³„ ì¹´ìš´íŠ¸
        for bin_name, bin_min, bin_max in error_bins:
            count = ((abs_rel_array >= bin_min) & (abs_rel_array < bin_max)).sum()
            pct = 100 * count / total
            bar = "â–“" * int(pct / 10) + "â–‘" * (10 - int(pct / 10))
            print(f"  {bin_name:15s}: {count:6,d} ({pct:5.1f}%)  {bar}")
    
    print("=" * 70)


def main() -> None:
    args = parse_args()
    dataset_root = Path(args.dataset_root)
    split_paths = discover_split_files(args)
    raw_entries = load_split_entries(args, split_paths)
    
    # âœ… ì²´í¬í¬ì¸íŠ¸ ID ì¶”ì¶œ
    checkpoint_id = get_checkpoint_id(args.checkpoint)
    print(f"\nğŸ“ ì²´í¬í¬ì¸íŠ¸ ID: {checkpoint_id}")
    print(f"   ì˜ˆì¸¡ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {args.pred_root}/{checkpoint_id}/\n")
    
    samples = [normalize_entry(args, dataset_root, entry, checkpoint_id) for entry in raw_entries]

    if args.debug:
        print(f"ì´ {len(samples)}ê°œ ìƒ˜í”Œ ë¡œë“œ")

    model_context = prepare_model(args)
    eval_namespace = make_eval_namespace(args)

    detected_classes: Optional[List[str]] = args.classes if args.classes else None
    instance_records: List[InstanceResult] = []
    all_samples_data: List[SampleData] = []  # âœ… í”½ì…€ ë ˆë²¨ ë¶„ì„ìš© ë°ì´í„°
    class_accumulators: Dict[str, MetricsAccumulator] = defaultdict(lambda: MetricsAccumulator(METRIC_NAMES))
    overall_accumulator = MetricsAccumulator(METRIC_NAMES)

    reference_shape: Optional[Tuple[int, int]] = None
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
    output_dirs = create_output_structure(checkpoint_id, args.output_root)
    print(f"\nğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {Path(args.output_root) / f'{checkpoint_id}_results'}")
    
    # ë””ë²„ê¹…ìš© ì¹´ìš´í„°
    processed_samples = 0
    visualize_dir = output_dirs['viz']  # í•­ìƒ viz ë””ë ‰í† ë¦¬ì— ì €ì¥
    print(f"   ì‹œê°í™” ì €ì¥ ìœ„ì¹˜: {visualize_dir}")

    for sample_idx, sample in enumerate(tqdm(samples, desc="Evaluating")):
        # max_samples ì²´í¬
        if args.max_samples is not None and processed_samples >= args.max_samples:
            if args.debug:
                print(f"\nìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ë„ë‹¬: {args.max_samples}, í‰ê°€ ì¢…ë£Œ")
            break
            
        gt_data = load_depth(str(sample.gt_path)) if sample.gt_path.exists() else None
        if gt_data is None:
            if args.debug:
                print(f"GT ëˆ„ë½ìœ¼ë¡œ ìŠ¤í‚µ: {sample.gt_path}")
            continue

        if reference_shape is None:
            reference_shape = gt_data.shape
            if args.debug:
                print(f"GT ê¸°ì¤€ í•´ìƒë„: {reference_shape[::-1]} (w x h)")
        elif gt_data.shape != reference_shape:
            raise ValueError(f"GT í•´ìƒë„ê°€ ì¼ê´€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {sample.gt_path} -> {gt_data.shape}")

        prediction = load_prediction(sample.prediction_path)
        if prediction is None:
            prediction = run_inference(model_context, sample.image_path, args.flip_tta)
            save_prediction(sample.prediction_path, prediction)
        elif args.debug:
            print(f"ìºì‹œëœ ì˜ˆì¸¡ ì‚¬ìš©: {sample.prediction_path}")
        
        # íŒŒì¼ ë³µì‚¬ (ì„ íƒì )
        if args.save_rgb:
            copy_file_to_output(sample.image_path, output_dirs['rgb'], f"{sample.stem}.png")
        
        if args.save_gt:
            copy_file_to_output(sample.gt_path, output_dirs['gt'], f"{sample.stem}.png")
        
        if args.save_pred:
            # ì˜ˆì¸¡ depthë¥¼ 16-bit PNGë¡œ ì €ì¥
            save_depth_as_png(prediction, output_dirs['pred'] / f"{sample.stem}.png")

        if prediction.shape != gt_data.shape and args.debug:
            print(f"ì˜ˆì¸¡/GT í•´ìƒë„ ë¶ˆì¼ì¹˜: pred {prediction.shape}, gt {gt_data.shape}")

        if detected_classes is None:
            class_root = sample.segmentation_root
            if not class_root.exists():
                raise FileNotFoundError(f"class_masks ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {class_root}")
            detected_classes = sorted([d.name for d in class_root.iterdir() if d.is_dir()])
            if args.debug:
                print(f"ìë™ íƒì§€ëœ í´ë˜ìŠ¤: {detected_classes}")

        mask_groups = collect_masks_for_stem(sample.segmentation_root, detected_classes or [], sample.stem)
        if not mask_groups:
            if args.debug:
                print(f"ë§ˆìŠ¤í¬ ë¯¸ì¡´ì¬ë¡œ ìƒ˜í”Œ ìŠ¤í‚µ: {sample.stem}")
            continue

        pred_tensor = torch.tensor(prediction, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
        sample_has_valid_mask = False
        
        # ì´ë¯¸ì§€ ì „ì²´ ê°ì²´ë¥¼ í•©ì¹œ ë§ˆìŠ¤í¬ (í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¦¬)
        combined_masks_by_class: Dict[str, np.ndarray] = {}

        for class_name, mask_paths in mask_groups.items():
            # í´ë˜ìŠ¤ë³„ í†µí•© ë§ˆìŠ¤í¬ ì´ˆê¸°í™”
            if class_name not in combined_masks_by_class:
                combined_masks_by_class[class_name] = np.zeros_like(gt_data, dtype=np.float32)
            
            for mask_idx, mask_path in enumerate(mask_paths):
                mask = load_mask(mask_path, gt_data.shape)
                valid_mask = (mask > 0) & (gt_data > 0)
                if not np.any(valid_mask):
                    if args.debug:
                        print(f"ìœ íš¨ í”½ì…€ì´ ì—†ì–´ ìŠ¤í‚µ: {mask_path}")
                    continue

                sample_has_valid_mask = True
                
                # í´ë˜ìŠ¤ë³„ í†µí•© ë§ˆìŠ¤í¬ì— ì¶”ê°€ (OR ì—°ì‚°)
                combined_masks_by_class[class_name] = np.maximum(combined_masks_by_class[class_name], mask)
                
                # ë””ë²„ê·¸ ì¶œë ¥
                if args.debug:
                    print(f"\n{'='*80}")
                    print(f"[Sample {processed_samples}] {sample.stem} - {class_name} - mask #{mask_idx}")
                    print(f"{'='*80}")
                    print(f"Image path: {sample.image_path}")
                    print(f"GT path: {sample.gt_path}")
                    print(f"Mask path: {mask_path}")
                    print(f"\nGT depth (ì „ì²´ ì´ë¯¸ì§€):")
                    gt_valid_all = gt_data > 0
                    if gt_valid_all.any():
                        print(f"  Range: [{gt_data[gt_valid_all].min():.4f}, {gt_data[gt_valid_all].max():.4f}]")
                        print(f"  Mean: {gt_data[gt_valid_all].mean():.4f}")
                        print(f"  Median: {np.median(gt_data[gt_valid_all]):.4f}")
                        print(f"  Valid pixels: {gt_valid_all.sum()} / {gt_data.size}")
                    
                    print(f"\nPred depth (ì „ì²´ ì´ë¯¸ì§€):")
                    print(f"  Range: [{prediction.min():.4f}, {prediction.max():.4f}]")
                    print(f"  Mean: {prediction.mean():.4f}")
                    print(f"  Median: {np.median(prediction):.4f}")
                    
                    print(f"\nMask info:")
                    print(f"  Unique values: {np.unique(mask)}")
                    print(f"  Masked pixels: {(mask > 0).sum()}")
                    
                    print(f"\nGT depth (ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ):")
                    gt_in_mask = gt_data[valid_mask]
                    print(f"  Range: [{gt_in_mask.min():.4f}, {gt_in_mask.max():.4f}]")
                    print(f"  Mean: {gt_in_mask.mean():.4f}")
                    print(f"  Median: {np.median(gt_in_mask):.4f}")
                    print(f"  Valid pixels: {len(gt_in_mask)}")
                    
                    print(f"\nPred depth (ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ):")
                    pred_in_mask = prediction[valid_mask]
                    print(f"  Range: [{pred_in_mask.min():.4f}, {pred_in_mask.max():.4f}]")
                    print(f"  Mean: {pred_in_mask.mean():.4f}")
                    print(f"  Median: {np.median(pred_in_mask):.4f}")

                # âœ… GTì™€ Pred ëª¨ë‘ ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
                # compute_depth_metricsëŠ” 4D tensor (B, C, H, W)ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ
                # ë§ˆìŠ¤í¬ëœ ì˜ì—­ì„ ì›ë³¸ í¬ê¸°ë¡œ ì¬êµ¬ì„± (ë§ˆìŠ¤í¬ ë°–ì€ 0)
                gt_masked_full = gt_data * mask
                pred_masked_full = prediction * mask
                
                gt_masked_tensor = torch.tensor(gt_masked_full, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                pred_masked_tensor = torch.tensor(pred_masked_full, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                
                metrics = compute_depth_metrics(eval_namespace, gt_masked_tensor, pred_masked_tensor, use_gt_scale=args.use_gt_scale)

                # GT ê±°ë¦¬ í†µê³„ ê³„ì‚°
                gt_valid_values = gt_data[valid_mask]
                gt_mean = float(gt_valid_values.mean())
                gt_median = float(np.median(gt_valid_values))

                if args.debug:
                    print(f"\nGT distance statistics:")
                    print(f"  Mean: {gt_mean:.4f}m")
                    print(f"  Median: {gt_median:.4f}m")
                    print(f"\nComputed metrics (ì¸ìŠ¤í„´ìŠ¤ë³„):")
                    for i, name in enumerate(METRIC_NAMES):
                        print(f"  {name}: {metrics[i].item():.6f}")

                class_accumulators[class_name].add(metrics)
                overall_accumulator.add(metrics)

                instance_records.append(InstanceResult(
                    stem=sample.stem,
                    class_name=class_name,
                    mask_path=mask_path,
                    valid_pixels=int(valid_mask.sum()),
                    metrics=metrics.detach().cpu().numpy().tolist(),
                    gt_mean_depth=gt_mean,
                    gt_median_depth=gt_median,
                ))
                
                # ì‹œê°í™” (ì¸ìŠ¤í„´ìŠ¤ë³„)
                if visualize_dir is not None and args.debug:  # ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ ì¸ìŠ¤í„´ìŠ¤ë³„ ì €ì¥
                    vis_filename = f"{sample_idx:04d}_{sample.stem}_{class_name}_inst{mask_idx}.png"
                    vis_path = visualize_dir / vis_filename
                    visualize_sample(
                        sample.image_path,
                        gt_data,
                        prediction,
                        mask,
                        vis_path,
                        f"{class_name} (inst {mask_idx})",
                        sample.stem,
                        metrics.detach().cpu().numpy().tolist(),
                    )
                    if args.debug:
                        print(f"\nì‹œê°í™” ì €ì¥ (ì¸ìŠ¤í„´ìŠ¤): {vis_path}")
        
        # í´ë˜ìŠ¤ë³„ í†µí•© ë§ˆìŠ¤í¬ë¡œ í‰ê°€ (ì´ë¯¸ì§€ë‹¹ ëª¨ë“  ê°ì²´)
        for class_name, combined_mask in combined_masks_by_class.items():
            if not np.any(combined_mask > 0):
                continue
            
            valid_combined = (combined_mask > 0) & (gt_data > 0)
            if not np.any(valid_combined):
                continue
            
            # âœ… GTì™€ Pred ëª¨ë‘ ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
            # compute_depth_metricsëŠ” 4D tensorë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ ì›ë³¸ í¬ê¸° ìœ ì§€ (ë§ˆìŠ¤í¬ ë°–ì€ 0)
            gt_combined_full = gt_data * combined_mask
            pred_combined_full = prediction * combined_mask
            
            gt_combined_tensor = torch.tensor(gt_combined_full, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
            pred_combined_tensor = torch.tensor(pred_combined_full, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
            
            combined_metrics = compute_depth_metrics(eval_namespace, gt_combined_tensor, pred_combined_tensor, use_gt_scale=args.use_gt_scale)
            
            if args.debug:
                print(f"\n{'='*80}")
                print(f"[Sample {processed_samples}] {sample.stem} - {class_name} - ALL INSTANCES COMBINED")
                print(f"{'='*80}")
                print(f"Combined mask pixels: {(combined_mask > 0).sum()}")
                print(f"Valid pixels: {valid_combined.sum()}")
                print(f"\nComputed metrics (í†µí•©):")
                for i, name in enumerate(METRIC_NAMES):
                    print(f"  {name}: {combined_metrics[i].item():.6f}")
            
            # í†µí•© ë§ˆìŠ¤í¬ GT ê±°ë¦¬ í†µê³„
            gt_combined_valid = gt_data[valid_combined]
            gt_combined_mean = float(gt_combined_valid.mean())
            gt_combined_median = float(np.median(gt_combined_valid))
            
            # í†µí•© ë§ˆìŠ¤í¬ ê²°ê³¼ë¥¼ ë³„ë„ë¡œ ì €ì¥
            instance_records.append(InstanceResult(
                stem=sample.stem,
                class_name=f"{class_name}_ALL",  # í†µí•©ì„ì„ í‘œì‹œ
                mask_path=Path(f"combined_{class_name}"),
                valid_pixels=int(valid_combined.sum()),
                metrics=combined_metrics.detach().cpu().numpy().tolist(),
                gt_mean_depth=gt_combined_mean,
                gt_median_depth=gt_combined_median,
            ))
            
            # ì‹œê°í™” (í†µí•© ë§ˆìŠ¤í¬)
            if visualize_dir is not None:
                vis_filename = f"{sample_idx:04d}_{sample.stem}_{class_name}_ALL.png"
                vis_path = visualize_dir / vis_filename
                visualize_sample(
                    sample.image_path,
                    gt_data,
                    prediction,
                    combined_mask,
                    vis_path,
                    f"{class_name} (ALL)",
                    sample.stem,
                    combined_metrics.detach().cpu().numpy().tolist(),
                )
                if args.debug:
                    print(f"\nì‹œê°í™” ì €ì¥ (í†µí•©): {vis_path}")
            
            # âœ… í”½ì…€ ë ˆë²¨ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì €ì¥
            all_samples_data.append(SampleData(
                stem=sample.stem,
                gt_depth=gt_data.copy(),
                pred_depth=prediction.copy(),
                mask=combined_mask.copy(),
                class_name=class_name,
            ))
        
        # âœ… ì „ì²´ ì´ë¯¸ì§€ í”½ì…€ ìˆ˜ì§‘ (ë§ˆìŠ¤í¬ ì—†ì´ GT > 0ì¸ ëª¨ë“  í”½ì…€)
        if sample_has_valid_mask:
            # ì „ì²´ ì´ë¯¸ì§€ë¥¼ "ALL" í´ë˜ìŠ¤ë¡œ ì €ì¥
            full_image_mask = (gt_data > 0).astype(np.float32)
            all_samples_data.append(SampleData(
                stem=sample.stem,
                gt_depth=gt_data.copy(),
                pred_depth=prediction.copy(),
                mask=full_image_mask,
                class_name="ALL",
            ))
        
        if sample_has_valid_mask:
            processed_samples += 1

    if not instance_records:
        raise RuntimeError("í‰ê°€ ê°€ëŠ¥í•œ ê°ì²´ ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    class_metrics: Dict[str, Tuple[List[float], int]] = {}
    for class_name, acc in class_accumulators.items():
        class_metrics[class_name] = (acc.mean(), acc.count())

    overall_metrics = (overall_accumulator.mean(), overall_accumulator.count())
    
    # âœ… ì „ì²´ ì´ë¯¸ì§€ í”½ì…€ ë©”íŠ¸ë¦­ ê³„ì‚° (ë§ˆìŠ¤í¬ ë¬´ì‹œ, GT > 0ì¸ ëª¨ë“  í”½ì…€)
    full_image_accumulator = MetricsAccumulator(METRIC_NAMES)
    for sample in samples:
        gt_data = load_depth(str(sample.gt_path)) if sample.gt_path.exists() else None
        if gt_data is None:
            continue
        
        prediction = load_prediction(sample.prediction_path)
        if prediction is None:
            continue
        
        # ì „ì²´ ì´ë¯¸ì§€ì—ì„œ GT > 0ì¸ ëª¨ë“  í”½ì…€ë¡œ í‰ê°€
        gt_tensor = torch.tensor(gt_data, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        pred_tensor = torch.tensor(prediction, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
        full_metrics = compute_depth_metrics(eval_namespace, gt_tensor, pred_tensor, use_gt_scale=args.use_gt_scale)
        full_image_accumulator.add(full_metrics)
    
    full_image_metrics = (full_image_accumulator.mean(), full_image_accumulator.count()) if full_image_accumulator.count() > 0 else None

    print_summary_table(class_metrics, overall_metrics, full_image_metrics)
    
    # ê±°ë¦¬ ë²”ìœ„ ì •ì˜
    dist_ranges = [
        ("D < 1m", 0.0, 1.0),
        ("1m < D < 2m", 1.0, 2.0),
        ("2m < D < 3m", 2.0, 3.0),
        ("D > 3m", 3.0, args.max_depth)
    ]
    
    # âœ… í´ë˜ìŠ¤ë³„ ê±°ë¦¬ë³„ í‰ê°€
    distance_results = {}  # {class_name: [(range_name, pixels, metrics), ...]}
    
    if all_samples_data:
        # ê° í´ë˜ìŠ¤ë³„ë¡œ ê±°ë¦¬ í‰ê°€ ìˆ˜í–‰ (ALL í¬í•¨)
        detected_classes = sorted(set(sample.class_name for sample in all_samples_data))
        for class_name in detected_classes:
            results = analyze_by_distance_ranges_pixel_level(all_samples_data, dist_ranges, eval_namespace, class_filter=class_name)
            distance_results[class_name] = results
        
        # car+road í•©ì¹œ ê²ƒ (carì™€ road ìƒ˜í”Œë§Œ ì‚¬ìš©)
        car_road_samples = [s for s in all_samples_data if s.class_name in ['car', 'road']]
        if car_road_samples:
            car_road_results = analyze_by_distance_ranges_pixel_level(car_road_samples, dist_ranges, eval_namespace, class_filter=None)
            distance_results['car+road'] = car_road_results
        
        print_distance_error_distribution(all_samples_data, dist_ranges)

    # ë©”íŠ¸ë¦­ ì €ì¥ - output_dirs['metrics']ì— ìë™ ì €ì¥
    summary_path = output_dirs['metrics'] / "summary.csv"
    with open(summary_path, "w") as f:
        f.write("Class,Count," + ",".join(METRIC_NAMES) + "\n")
        for class_name, (metrics, count) in sorted(class_metrics.items()):
            metric_str = ",".join(f"{m:.6f}" if not math.isnan(m) else "nan" for m in metrics)
            f.write(f"{class_name},{count},{metric_str}\n")
        metric_str = ",".join(f"{m:.6f}" if not math.isnan(m) else "nan" for m in overall_metrics[0])
        f.write(f"car+road,{overall_metrics[1]},{metric_str}\n")
        if full_image_metrics is not None:
            metric_str = ",".join(f"{m:.6f}" if not math.isnan(m) else "nan" for m in full_image_metrics[0])
            f.write(f"ALL,{full_image_metrics[1]},{metric_str}\n")
    print(f"\nâœ… ìš”ì•½ ë©”íŠ¸ë¦­ ì €ì¥: {summary_path}")
    
    # âœ… ê±°ë¦¬ë³„ ë©”íŠ¸ë¦­ ì €ì¥
    if distance_results:
        distance_path = output_dirs['metrics'] / "summary_by_distance.csv"
        with open(distance_path, "w") as f:
            f.write("Class,Range,Pixels," + ",".join(METRIC_NAMES) + "\n")
            for class_name in sorted(distance_results.keys()):
                for range_name, pixel_count, metrics in distance_results[class_name]:
                    metric_str = ",".join(f"{m:.6f}" if not math.isnan(m) else "nan" for m in metrics)
                    f.write(f"{class_name},{range_name},{pixel_count},{metric_str}\n")
        print(f"âœ… ê±°ë¦¬ë³„ ë©”íŠ¸ë¦­ ì €ì¥: {distance_path}")

    # Per-instance JSON ì €ì¥
    json_path = output_dirs['metrics'] / "per_instance.json"
    with open(json_path, "w") as f:
        json.dump({
            "metric_names": METRIC_NAMES,
            "instances": [
                {
                    "stem": item.stem,
                    "class": item.class_name,
                    "mask_path": str(item.mask_path),
                    "valid_pixels": item.valid_pixels,
                    "gt_mean_depth": item.gt_mean_depth,
                    "gt_median_depth": item.gt_median_depth,
                    "metrics": item.metrics,
                }
                for item in instance_records
            ],
        }, f, indent=2)
    print(f"âœ… ì¸ìŠ¤í„´ìŠ¤ë³„ ë©”íŠ¸ë¦­ ì €ì¥: {json_path}")
    
    # ì‹¤í–‰ ì •ë³´ README ìƒì„±
    readme_path = output_dirs['metrics'].parent / "README.txt"
    with open(readme_path, "w") as f:
        f.write(f"Evaluation Results for {checkpoint_id}\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"Checkpoint: {args.checkpoint}\n")
        f.write(f"Dataset: {args.dataset_root}\n")
        f.write(f"Min/Max Depth: {args.min_depth} / {args.max_depth}\n")
        f.write(f"Image Shape: {args.image_shape}\n")
        f.write(f"Flip TTA: {args.flip_tta}\n")
        f.write(f"GT Scale: {args.use_gt_scale}\n")
        f.write(f"Processed Samples: {processed_samples}\n")
        f.write(f"Total Instances: {len(instance_records)}\n")
        f.write(f"\nOutput Structure:\n")
        f.write(f"  - rgb/: RGB images\n")
        f.write(f"  - gt/: Ground truth depth maps\n")
        f.write(f"  - pred/: Predicted depth maps (16-bit PNG)\n")
        f.write(f"  - viz/: Visualization results (4-panel)\n")
        f.write(f"  - metrics/: Evaluation metrics (CSV, JSON)\n")
    print(f"âœ… README ìƒì„±: {readme_path}")

    if args.output_file:
        out_path = Path(args.output_file)
        ensure_dir(out_path.parent if out_path.parent != Path("") else Path("."))
        with open(out_path, "w") as f:
            f.write("Class,Count," + ",".join(METRIC_NAMES) + "\n")
            for class_name, (metrics, count) in sorted(class_metrics.items()):
                metric_str = ",".join(f"{m:.6f}" if not math.isnan(m) else "nan" for m in metrics)
                f.write(f"{class_name},{count},{metric_str}\n")
            metric_str = ",".join(f"{m:.6f}" if not math.isnan(m) else "nan" for m in overall_metrics[0])
            f.write(f"ALL,{overall_metrics[1]},{metric_str}\n")

    if args.per_instance_json:
        json_path = Path(args.per_instance_json)
        ensure_dir(json_path.parent if json_path.parent != Path("") else Path("."))
        with open(json_path, "w") as f:
            json.dump({
                "metric_names": METRIC_NAMES,
                "instances": [
                    {
                        "stem": item.stem,
                        "class": item.class_name,
                        "mask_path": str(item.mask_path),
                        "valid_pixels": item.valid_pixels,
                        "gt_mean_depth": item.gt_mean_depth,
                        "gt_median_depth": item.gt_median_depth,
                        "metrics": item.metrics,
                    }
                    for item in instance_records
                ],
            }, f, indent=2)


if __name__ == "__main__":
    main()

