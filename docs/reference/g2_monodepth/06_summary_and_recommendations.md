# G2-MonoDepth ë¶„ì„ ìš”ì•½ ë° ì ìš© ì œì•ˆ

## 1. G2-MonoDepth í•µì‹¬ ìš”ì•½

### 1.1 í”„ë¡œì íŠ¸ ê°œìš”

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ëª©ì ** | RGB + ë‹¤ì–‘í•œ sparse depth ì†ŒìŠ¤ë¥¼ í†µí•©í•œ depth inference |
| **í•µì‹¬ í˜ì‹ ** | 0~100% sparsityì—ì„œ í•™ìŠµí•˜ì—¬ ì–´ë–¤ ì„¼ì„œì—ë„ ì¼ë°˜í™” |
| **ë„¤íŠ¸ì›Œí¬** | 7-Layer UNet with ReZero |
| **Loss** | Absolute + Relative + Gradient Loss |

### 1.2 í•µì‹¬ ê¸°ìˆ  ìš”ì†Œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    G2-MonoDepth í•µì‹¬ ìš”ì†Œ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ğŸ”· ReZero BottleNeck                                           â”‚
â”‚     - í•™ìŠµ ê°€ëŠ¥í•œ residual scaling (alpha = 0 ì´ˆê¸°í™”)            â”‚
â”‚     - ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì˜ ì•ˆì •ì  í•™ìŠµ                                 â”‚
â”‚                                                                 â”‚
â”‚  ğŸ”· 3-Term Loss                                                 â”‚
â”‚     - Absolute: ì ˆëŒ€ depth ì •í™•ë„                                â”‚
â”‚     - Relative: scale-invariant ë¶„í¬ í•™ìŠµ                        â”‚
â”‚     - Gradient: edge/êµ¬ì¡° ë³´ì¡´                                   â”‚
â”‚                                                                 â”‚
â”‚  ğŸ”· Robust Standardization                                      â”‚
â”‚     - MAD (Mean Absolute Deviation) ì‚¬ìš©                        â”‚
â”‚     - Outlierì— ê°•ê±´í•œ ì •ê·œí™”                                    â”‚
â”‚                                                                 â”‚
â”‚  ğŸ”· Multi-Sparsity Training                                     â”‚
â”‚     - 0% (RGB-only) ~ 100% (Dense) ì „ ë²”ìœ„ í•™ìŠµ                  â”‚
â”‚     - ë‹¤ì–‘í•œ ì„¼ì„œ artifact ì‹œë®¬ë ˆì´ì…˜                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ìš°ë¦¬ í”„ë¡œì íŠ¸ì™€ì˜ ë¹„êµ

### 2.1 ì•„í‚¤í…ì²˜ ë¹„êµ

| í•­ëª© | G2-MonoDepth | PackNet-SfM (ìš°ë¦¬) |
|------|--------------|-------------------|
| Encoder | Custom UNet (7-layer) | ResNet18 (pretrained) |
| Decoder | Symmetric UNet | Custom Decoder |
| Skip Connection | Addition | Addition |
| Normalization | LayerNorm | BatchNorm |
| Activation | GELU | ELU |
| Residual ê¸°ë²• | ReZero | Standard |
| ì…ë ¥ | 5ch (RGB + sparse + mask) | 3ch (RGB) |
| ì¶œë ¥ | Direct depth | Sigmoid Ã— max_depth |

### 2.2 Loss ë¹„êµ

| í•­ëª© | G2-MonoDepth | PackNet-SfM (ìš°ë¦¬) |
|------|--------------|-------------------|
| Absolute Loss | L1 | Silog Loss |
| Relative Loss | Standardized L1 | SSI Loss |
| Gradient Loss | Multi-scale Sobel | âŒ ì—†ìŒ |
| ì´ Loss | A + R + 0.5G | 0.5Ã—SSI + 0.5Ã—Silog |

### 2.3 ë°ì´í„° ì²˜ë¦¬ ë¹„êµ

| í•­ëª© | G2-MonoDepth | PackNet-SfM (ìš°ë¦¬) |
|------|--------------|-------------------|
| Augmentation | Heavy (sparsity, artifacts) | Basic (flip, color) |
| Normalization | Robust (MAD) | Standard |
| Mask ì²˜ë¦¬ | hole_point ì±„ë„ | GT ìœ íš¨ì„± mask |

---

## 3. ì ìš© ê°€ëŠ¥í•œ ìš”ì†Œ

### 3.1 ğŸŸ¢ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (High Priority)

#### (1) Gradient Loss ì¶”ê°€

**íš¨ê³¼**: Edge ë³´ì¡´, ì „ì²´ ë§µ ì¼ê´€ì„± í–¥ìƒ

```python
# ì œì•ˆ êµ¬í˜„
class MultiScaleGradientLoss(nn.Module):
    def __init__(self, scales=[1, 2, 4, 8]):
        super().__init__()
        self.scales = scales
        self.sobel_x = torch.tensor([
            [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]
        ]).float().view(1, 1, 3, 3)
        self.sobel_y = torch.tensor([
            [-1, -2, -1], [0, 0, 0], [1, 2, 1]
        ]).float().view(1, 1, 3, 3)
    
    def forward(self, pred, gt, mask):
        total_loss = 0
        for scale in self.scales:
            # Downsample
            pred_s = F.avg_pool2d(pred, scale) if scale > 1 else pred
            gt_s = F.avg_pool2d(gt, scale) if scale > 1 else gt
            mask_s = (F.avg_pool2d(mask.float(), scale) > 0.5).float() if scale > 1 else mask
            
            # Gradient
            grad_pred = self.compute_gradient(pred_s)
            grad_gt = self.compute_gradient(gt_s)
            
            # Loss
            total_loss += self.masked_l1(grad_pred, grad_gt, mask_s)
        
        return total_loss / len(self.scales)
```

**Config ë³€ê²½**:
```yaml
loss:
  ssi_weight: 0.4
  silog_weight: 0.4
  gradient_weight: 0.2  # ìƒˆë¡œ ì¶”ê°€
```

#### (2) Loss Logging ì„¸ë¶„í™”

**íš¨ê³¼**: í•™ìŠµ ë¶„ì„ ìš©ì´, íŠœë‹ ê°€ì´ë“œ

```python
# supervised_loss.py ìˆ˜ì •
def compute_loss(pred, gt, mask):
    ssi_loss = compute_ssi_loss(pred, gt, mask)
    silog_loss = compute_silog_loss(pred, gt, mask)
    gradient_loss = compute_gradient_loss(pred, gt, mask)
    
    total = ssi_weight * ssi_loss + silog_weight * silog_loss + grad_weight * gradient_loss
    
    return total, {
        'ssi_loss': ssi_loss.item(),
        'silog_loss': silog_loss.item(),
        'gradient_loss': gradient_loss.item(),
        'total_loss': total.item()
    }
```

### 3.2 ğŸŸ¡ ê²€í†  í›„ ì ìš© (Medium Priority)

#### (1) ReZero ê¸°ë²•

**íš¨ê³¼**: í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ, ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ ê°€ëŠ¥

```python
class ReZeroResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv_block = ...  # ê¸°ì¡´ conv block
        self.alpha = nn.Parameter(torch.zeros(1))  # ReZero
    
    def forward(self, x):
        return x + self.alpha * self.conv_block(x)
```

**ì ìš© ìœ„ì¹˜**: ResNetSANì˜ SpatialAttention ëª¨ë“ˆ

#### (2) GELU Activation

**íš¨ê³¼**: ë” ë¶€ë“œëŸ¬ìš´ activation, í•™ìŠµ ì•ˆì •ì„±

```python
# í˜„ì¬: ELU
self.act = nn.ELU(inplace=True)

# ë³€ê²½: GELU
self.act = nn.GELU()
```

**ì£¼ì˜**: ê¸°ì¡´ pretrained weightsì™€ì˜ í˜¸í™˜ì„± í™•ì¸ í•„ìš”

### 3.3 ğŸ”µ ì°¸ê³ ë§Œ (Low Priority)

#### (1) LayerNorm ì „í™˜

- BatchNorm â†’ LayerNormì€ í° ë³€ê²½
- ResNet pretrained weights í˜¸í™˜ì„± ë¬¸ì œ
- ìƒˆë¡œ í•™ìŠµí•  ê²½ìš°ì—ë§Œ ê³ ë ¤

#### (2) Multi-Sparsity Training

- ìš°ë¦¬ëŠ” RGB-onlyì´ë¯€ë¡œ ì§ì ‘ ì ìš© ë¶ˆê°€
- í•˜ì§€ë§Œ data augmentation ê°•í™”ëŠ” ì°¸ê³  ê°€ëŠ¥

#### (3) Robust Standardization (MAD)

- í˜„ì¬ SSI Lossê°€ ìœ ì‚¬í•œ ì—­í• 
- í•„ìš” ì‹œ SSI Loss ë‚´ë¶€ì— ì ìš© ê°€ëŠ¥

---

## 4. êµ¬í˜„ ìš°ì„ ìˆœìœ„

### 4.1 Phase 1: Gradient Loss ì¶”ê°€ (ê¶Œì¥)

```
ëª©í‘œ: ì „ì²´ ë§µ ì¼ê´€ì„± í–¥ìƒ

êµ¬í˜„ ì‚¬í•­:
1. MultiScaleGradientLoss í´ë˜ìŠ¤ êµ¬í˜„
2. supervised_loss.pyì— í†µí•©
3. YAML configì— gradient_weight ì¶”ê°€
4. í•™ìŠµ ë° í‰ê°€

ì˜ˆìƒ íš¨ê³¼:
- Edge ì„ ëª…ë„ í–¥ìƒ
- Depth ê²½ê³„ ë³´ì¡´
- ì „ì²´ êµ¬ì¡° ì¼ê´€ì„± ê°œì„ 
```

### 4.2 Phase 2: Loss Logging ì„¸ë¶„í™”

```
ëª©í‘œ: í•™ìŠµ ë¶„ì„ ë° íŠœë‹ ìš©ì´ì„±

êµ¬í˜„ ì‚¬í•­:
1. ê° loss term ë³„ë„ logging
2. TensorBoard/WandBì— ì‹œê°í™”
3. Loss term ë³„ ì¶”ì´ ë¶„ì„

ì˜ˆìƒ íš¨ê³¼:
- ì–´ë–¤ lossê°€ í•™ìŠµì— ê¸°ì—¬í•˜ëŠ”ì§€ íŒŒì•…
- ìµœì ì˜ weight ì¡°í•© íƒìƒ‰ ìš©ì´
```

### 4.3 Phase 3: ReZero (ì„ íƒì )

```
ëª©í‘œ: í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

êµ¬í˜„ ì‚¬í•­:
1. Attention ëª¨ë“ˆì— ReZero ì ìš©
2. ê¸°ì¡´ weights í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
3. í•™ìŠµ ì†ë„ ë° ìˆ˜ë ´ ë¹„êµ

ì˜ˆìƒ íš¨ê³¼:
- ë” ì•ˆì •ì ì¸ í•™ìŠµ
- ì ì¬ì ìœ¼ë¡œ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ ê°€ëŠ¥
```

---

## 5. ì‹¤í—˜ ê³„íš ì œì•ˆ

### 5.1 Baseline í™•ë¦½

```
ì‹¤í—˜ëª…: Baseline (í˜„ì¬ êµ¬í˜„)
Config: ssi_weight=0.5, silog_weight=0.5
ë©”íŠ¸ë¦­: RMSE, MAE, Î´1, Î´2, Î´3
```

### 5.2 Gradient Loss ì‹¤í—˜

```
ì‹¤í—˜ëª…: +Gradient Loss
Config ë³€í˜•:
  A) ssi=0.4, silog=0.4, grad=0.2
  B) ssi=0.35, silog=0.35, grad=0.3
  C) ssi=0.5, silog=0.5, grad=0.1 (ë³´ìˆ˜ì )

í‰ê°€:
  - Edge ì˜ì—­ì—ì„œì˜ ì„±ëŠ¥ ë³€í™”
  - ì „ì²´ ë©”íŠ¸ë¦­ ë³€í™”
  - í•™ìŠµ ì†ë„ ë³€í™”
```

### 5.3 Ablation Study

```
ì‹¤í—˜ëª…: Loss Term Ablation
Variants:
  1) SSI only
  2) Silog only
  3) SSI + Silog
  4) SSI + Silog + Gradient

ë¶„ì„:
  - ê° loss termì˜ ê¸°ì—¬ë„
  - ìµœì  ì¡°í•© íƒìƒ‰
```

---

## 6. ê²°ë¡ 

### 6.1 í•µì‹¬ takeaway

1. **Gradient LossëŠ” ì¦‰ì‹œ ì ìš© ê°€ì¹˜ê°€ ìˆìŒ**
   - ì „ì²´ ë§µ ì¼ê´€ì„± ê°œì„ ì— ì§ì ‘ì ìœ¼ë¡œ ê¸°ì—¬
   - êµ¬í˜„ ë‚œì´ë„ ë‚®ìŒ, ë¦¬ìŠ¤í¬ ë‚®ìŒ

2. **Loss ì„¸ë¶„í™” loggingì€ í•„ìˆ˜**
   - í•™ìŠµ ë¶„ì„ ë° ë””ë²„ê¹…ì— í•„ìˆ˜
   - í–¥í›„ íŠœë‹ì— í•„ìš”

3. **ReZeroëŠ” ì„ íƒì **
   - í•™ìŠµ ì•ˆì •ì„± ì´ìŠˆê°€ ìˆì„ ë•Œ ê³ ë ¤
   - ê¸°ì¡´ weights í˜¸í™˜ì„± ì£¼ì˜

### 6.2 ë‹¤ìŒ ë‹¨ê³„

```
1. Gradient Loss êµ¬í˜„ ë° í†µí•©
2. ì‹¤í—˜ ì§„í–‰ (Baseline vs +Gradient)
3. ê²°ê³¼ ë¶„ì„ ë° weight íŠœë‹
4. í•„ìš” ì‹œ ì¶”ê°€ ê¸°ë²• ì ìš©
```

---

## ë¬¸ì„œ ëª©ë¡

| íŒŒì¼ | ë‚´ìš© |
|------|------|
| [01_overview.md](01_overview.md) | í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš” |
| [02_network_architecture.md](02_network_architecture.md) | ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„¸ |
| [03_loss_functions.md](03_loss_functions.md) | Loss í•¨ìˆ˜ ìƒì„¸ |
| [04_data_processing.md](04_data_processing.md) | ë°ì´í„° ì²˜ë¦¬ ìƒì„¸ |
| [05_training_process.md](05_training_process.md) | í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ìƒì„¸ |
| [06_summary_and_recommendations.md](06_summary_and_recommendations.md) | ìš”ì•½ ë° ì ìš© ì œì•ˆ (í˜„ì¬ ë¬¸ì„œ) |
