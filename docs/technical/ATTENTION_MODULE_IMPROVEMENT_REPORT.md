# Attention Module ê¸°ë°˜ ëª¨ë¸ ê°œì„  ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2024-12-05  
**ëŒ€ìƒ ëª¨ë¸**: ResNetSAN01 (ResNet-18 Encoder + Dual-Head Decoder)  
**ëª©í‘œ**: AImotive NPU ì œì•½ì‚¬í•­ ë‚´ì—ì„œ ì ìš© ê°€ëŠ¥í•œ Attention ê¸°ë²• ë¶„ì„

---

## 1. í˜„ì¬ ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„

### 1.1 ì „ì²´ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ResNetSAN01                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚   Input     â”‚ RGB Image (B, 3, 384, 640)                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚          â†“                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              ResNet-18 Encoder (Pretrained)              â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚   â”‚  â”‚ feat0  â”‚ â”‚ feat1  â”‚ â”‚ feat2  â”‚ â”‚ feat3  â”‚ â”‚ feat4  â”‚ â”‚   â”‚
â”‚   â”‚  â”‚ 64ch   â”‚ â”‚ 64ch   â”‚ â”‚ 128ch  â”‚ â”‚ 256ch  â”‚ â”‚ 512ch  â”‚ â”‚   â”‚
â”‚   â”‚  â”‚ 1/2    â”‚ â”‚ 1/4    â”‚ â”‚ 1/8    â”‚ â”‚ 1/16   â”‚ â”‚ 1/32   â”‚ â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚          â”‚          â”‚          â”‚          â”‚         â”‚
â”‚           â”‚          â”‚          â”‚          â”‚          â†“         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚       â”‚          â”‚          â”‚          â”‚   DualHead      â”‚  â”‚
â”‚   â”‚       â”‚ Skip     â”‚ Skip     â”‚ Skip     â”‚   Decoder       â”‚  â”‚
â”‚   â”‚       â”‚ Connect  â”‚ Connect  â”‚ Connect  â”‚                 â”‚  â”‚
â”‚   â”‚       â†“          â†“          â†“          â†“                 â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚   â”‚  â”‚ 16ch   â”‚ â”‚ 32ch   â”‚ â”‚ 64ch   â”‚ â”‚ 128ch  â”‚â†â”€â”€â”        â”‚  â”‚
â”‚   â”‚  â”‚ 1/1    â”‚ â”‚ 1/2    â”‚ â”‚ 1/4    â”‚ â”‚ 1/8    â”‚   â”‚256ch   â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚1/16    â”‚  â”‚
â”‚   â”‚       â”‚                                         â”‚        â”‚  â”‚
â”‚   â”‚       â†“                                         â”‚        â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚
â”‚   â”‚  â”‚ Dual Output Heads                                    â”‚  â”‚
â”‚   â”‚  â”‚  â”œâ”€â”€ Integer Head:    [B, 1, H, W] sigmoid [0,1]     â”‚  â”‚
â”‚   â”‚  â”‚  â””â”€â”€ Fractional Head: [B, 1, H, W] sigmoid [0,1]     â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Final Depth = Integer_sigmoid Ã— max_depth + Fractional_sigmoid Ã— 1.0
```

### 1.2 í˜„ì¬ ë¬¸ì œì  ë¶„ì„

| êµ¬ì„± ìš”ì†Œ | í˜„ì¬ ìƒíƒœ | ë¬¸ì œì  |
|----------|----------|--------|
| **Encoder** | ResNet-18 (Local convolutions only) | ì „ì—­ì  ì»¨í…ìŠ¤íŠ¸ ë¶€ì¬ |
| **Skip Connection** | ë‹¨ìˆœ Concatenation | ì •ë³´ ì„ íƒ ëŠ¥ë ¥ ì—†ìŒ |
| **Decoder** | ConvBlock + Upsample | ë¨¼ ê±°ë¦¬ ì˜ì¡´ì„± ëª¨ë¸ë§ ë¶ˆê°€ |
| **Feature Fusion** | ë™ì¼ ë¹„ì¤‘ ê²°í•© | Adaptive weighting ì—†ìŒ |

### 1.3 Depth Estimationì—ì„œ Attentionì´ í•„ìš”í•œ ì´ìœ 

1. **ì¥ê±°ë¦¬ ì˜ì¡´ì„± (Long-range Dependencies)**
   - ë„ë¡œ ì¥ë©´ì—ì„œ ì†Œì‹¤ì (vanishing point)ê³¼ ê·¼ê²½ì˜ ê´€ê³„ íŒŒì•… í•„ìš”
   - CNNì˜ ì œí•œëœ receptive fieldë¡œëŠ” ë¶ˆì¶©ë¶„

2. **ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì»¨í…ìŠ¤íŠ¸ (Multi-scale Context)**
   - ê°€ê¹Œìš´ ë¬¼ì²´(ìƒì„¸ texture) vs ë¨¼ ë¬¼ì²´(ì „ì—­ êµ¬ì¡°)
   - ìŠ¤ì¼€ì¼ë³„ë¡œ ë‹¤ë¥¸ íŠ¹ì§•ì´ ì¤‘ìš”

3. **ê²½ê³„ ì„ ëª…ë„ (Edge Sharpness)**
   - ê¹Šì´ ë¶ˆì—°ì†ì ì—ì„œ ì •í™•í•œ ì˜ˆì¸¡ í•„ìš”
   - Attentionìœ¼ë¡œ ê²½ê³„ ì˜ì—­ì— ì§‘ì¤‘ ê°€ëŠ¥

---

## 2. ì ìš© ê°€ëŠ¥í•œ Attention ê¸°ë²• ë¶„ì„

### 2.1 Self-Attention (Full)

```python
# Standard Self-Attention
# Complexity: O(HÃ—W Ã— HÃ—W Ã— C) = O(NÂ² Ã— C)

Q = Conv1x1(x)  # [B, C, H, W] â†’ [B, C', H, W]
K = Conv1x1(x)  # [B, C, H, W] â†’ [B, C', H, W]
V = Conv1x1(x)  # [B, C, H, W] â†’ [B, C', H, W]

# Reshape: [B, C', HW]
attention = softmax(Q^T Ã— K / sqrt(d))  # [B, HW, HW]  â† ë¬¸ì œ!
output = attention Ã— V
```

**âš ï¸ AImotive NPU ì œì•½ì‚¬í•­ ìœ„ë°˜:**
- `Reshape` ë¯¸ì§€ì› â†’ [B, C, H, W] â†’ [B, C, HW] ë¶ˆê°€
- `MatMul` with spatial dimensions ë¯¸ì§€ì›
- ë©”ëª¨ë¦¬: 640Ã—384 = 245,760 â†’ Attention map 60GB í•„ìš”

**âŒ ì ìš© ë¶ˆê°€**

---

### 2.2 Squeeze-and-Excitation (SE) Block - âœ… ê¶Œì¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SE Block (NPU Compatible)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Input: [B, C, H, W]                                       â”‚
â”‚           â”‚                                                  â”‚
â”‚           â†“                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚   â”‚ GlobalAvgPool     â”‚  [B, C, H, W] â†’ [B, C, 1, 1]        â”‚
â”‚   â”‚ (ReduceMean)      â”‚  âœ… NPU ì§€ì›                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚             â†“                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚   â”‚ Conv1x1 (Reduce)  â”‚  [B, C, 1, 1] â†’ [B, C/r, 1, 1]     â”‚
â”‚   â”‚ + ReLU            â”‚  âœ… NPU ì§€ì›                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚             â†“                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚   â”‚ Conv1x1 (Expand)  â”‚  [B, C/r, 1, 1] â†’ [B, C, 1, 1]     â”‚
â”‚   â”‚ + Sigmoid         â”‚  âœ… NPU ì§€ì›                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚             â†“                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚   â”‚ Mul (Channel-wise)â”‚  Input Ã— Scale = Output            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  âœ… NPU ì§€ì›                        â”‚
â”‚             â†“                                                â”‚
â”‚   Output: [B, C, H, W]                                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**êµ¬í˜„ ì½”ë“œ:**
```python
class SEBlock(nn.Module):
    """Squeeze-and-Excitation Block (AImotive NPU Compatible)"""
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)  # GlobalAvgPool âœ…
        self.fc1 = nn.Conv2d(channels, channels // reduction, 1)  # âœ…
        self.fc2 = nn.Conv2d(channels // reduction, channels, 1)  # âœ…
        self.relu = nn.ReLU(inplace=True)  # âœ…
        self.sigmoid = nn.Sigmoid()  # âœ…
    
    def forward(self, x):
        scale = self.pool(x)
        scale = self.relu(self.fc1(scale))
        scale = self.sigmoid(self.fc2(scale))
        return x * scale  # Element-wise Mul âœ…
```

**NPU í˜¸í™˜ì„± ê²€ì¦:**
| ì—°ì‚° | ONNX Operation | AImotive ì§€ì› |
|-----|---------------|--------------|
| GlobalAvgPool | GlobalAveragePool | âœ… ì§€ì› |
| Conv 1Ã—1 | Conv (kernel=1) | âœ… ì§€ì› |
| ReLU | Relu | âœ… ì§€ì› |
| Sigmoid | Sigmoid | âœ… ì§€ì› |
| Channel-wise Mul | Mul | âœ… ì§€ì› |

**íš¨ê³¼:**
- **ì±„ë„ ê°„ ê´€ê³„** í•™ìŠµ (ì–´ë–¤ featureê°€ ì¤‘ìš”í•œì§€)
- ì¶”ê°€ íŒŒë¼ë¯¸í„°: ~0.1% (negligible)
- FLOPs ì¦ê°€: ~1%

---

### 2.3 CBAM (Convolutional Block Attention Module) - âœ… ë¶€ë¶„ ì ìš© ê¶Œì¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CBAM Module                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Input: [B, C, H, W]                                          â”‚
â”‚           â”‚                                                     â”‚
â”‚           â†“                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚         Channel Attention (SE-like)        â”‚                â”‚
â”‚   â”‚                                            â”‚                â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                â”‚
â”‚   â”‚  â”‚ AvgPool(HW) â”‚    â”‚ MaxPool(HW) â”‚       â”‚                â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚                â”‚
â”‚   â”‚         â†“                   â†“              â”‚                â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚                â”‚
â”‚   â”‚  â”‚     Shared MLP (FCâ†’ReLUâ†’FC)     â”‚      â”‚                â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                â”‚
â”‚   â”‚         â”‚                  â”‚              â”‚                â”‚
â”‚   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                â”‚
â”‚   â”‚                 â†“                          â”‚                â”‚
â”‚   â”‚            Add + Sigmoid                   â”‚                â”‚
â”‚   â”‚                 â†“                          â”‚                â”‚
â”‚   â”‚         Channel Attention Map              â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                     â†“                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚         Spatial Attention                  â”‚                â”‚
â”‚   â”‚                                            â”‚                â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                â”‚
â”‚   â”‚  â”‚ AvgPool(C)  â”‚    â”‚ MaxPool(C)  â”‚       â”‚                â”‚
â”‚   â”‚  â”‚ [B,1,H,W]   â”‚    â”‚ [B,1,H,W]   â”‚       â”‚                â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚                â”‚
â”‚   â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                â”‚
â”‚   â”‚                â†“ Concat                    â”‚                â”‚
â”‚   â”‚         [B, 2, H, W]                       â”‚                â”‚
â”‚   â”‚                â†“                           â”‚                â”‚
â”‚   â”‚         Conv 7Ã—7 â†’ Sigmoid                 â”‚                â”‚
â”‚   â”‚                â†“                           â”‚                â”‚
â”‚   â”‚         Spatial Attention Map              â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                     â†“                                           â”‚
â”‚   Output: [B, C, H, W]                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**NPU í˜¸í™˜ì„±:**

| ë¶€ë¶„ | ì—°ì‚° | NPU ì§€ì› | ë¹„ê³  |
|-----|------|---------|------|
| Channel Attention | GlobalAvgPool, GlobalMaxPool, Conv1Ã—1, Sigmoid | âœ… | ì™„ì „ í˜¸í™˜ |
| Spatial Attention | ReduceMax/Mean on Channels | âš ï¸ | Axis=1ë§Œ ì§€ì›, ì±„ë„ 256 ì œí•œ |
| Spatial Attention | Conv 7Ã—7 | âœ… | kernel â‰¤ 17 |

**ê¶Œì¥ êµ¬í˜„:**
```python
class NPUCompatibleCBAM(nn.Module):
    """CBAM with NPU-friendly Spatial Attention"""
    def __init__(self, channels, reduction=16):
        super().__init__()
        # Channel Attention (SE-like) âœ…
        self.ca_avg = nn.AdaptiveAvgPool2d(1)
        self.ca_max = nn.AdaptiveMaxPool2d(1)
        self.ca_fc1 = nn.Conv2d(channels, channels // reduction, 1)
        self.ca_fc2 = nn.Conv2d(channels // reduction, channels, 1)
        
        # Spatial Attention âœ… (ì±„ë„ ìˆ˜ ì œí•œ ì¤€ìˆ˜)
        # ì£¼ì˜: ì…ë ¥ ì±„ë„ì´ 256 ì´ˆê³¼ ì‹œ ReduceMax ë¯¸ì§€ì›
        self.sa_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)
        self.sigmoid = nn.Sigmoid()
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        # Channel Attention
        avg_out = self.ca_fc2(self.relu(self.ca_fc1(self.ca_avg(x))))
        max_out = self.ca_fc2(self.relu(self.ca_fc1(self.ca_max(x))))
        ca = self.sigmoid(avg_out + max_out)
        x = x * ca
        
        # Spatial Attention
        avg_out = torch.mean(x, dim=1, keepdim=True)  # ReduceMean âœ…
        max_out, _ = torch.max(x, dim=1, keepdim=True)  # ReduceMax âš ï¸
        sa = self.sigmoid(self.sa_conv(torch.cat([avg_out, max_out], dim=1)))
        return x * sa
```

**âš ï¸ ì£¼ì˜ì‚¬í•­:**
- Spatial Attentionì˜ `ReduceMax`ëŠ” ì±„ë„ â‰¤ 256ì—ì„œë§Œ NPU ê°€ì†
- 512 ì±„ë„ (feat4)ì—ì„œëŠ” **Channel Attentionë§Œ ì‚¬ìš©** ê¶Œì¥

---

### 2.4 Efficient Attention (Linear Attention) - âš ï¸ ì œí•œì  ì ìš©

```
Standard Attention: O(NÂ²)
   Attention = softmax(Q Ã— K^T) Ã— V

Linear Attention: O(N)
   Attention = Ï•(Q) Ã— (Ï•(K)^T Ã— V)
   
   where Ï• is a kernel function (e.g., elu(x) + 1)
```

**ë¬¸ì œì :**
- `MatMul`ì´ spatial dimensionì—ì„œ í•„ìš” â†’ AImotive ë¯¸ì§€ì›
- Reshape í•„ìš” â†’ ë¯¸ì§€ì›

**âŒ ì§ì ‘ ì ìš© ë¶ˆê°€**

---

### 2.5 Axial Attention (1D Factorized) - âš ï¸ ë¶€ë¶„ ì ìš© ê°€ëŠ¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Axial Attention                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   ëŒ€ì‹  1D Convolutionìœ¼ë¡œ ê·¼ì‚¬:                              â”‚
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚   â”‚ Input [B,C,H,W]â”‚                                        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚           â”‚                                                  â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚           â†“                                 â†“                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ Conv (1, k_h) â”‚ Height-wise     â”‚ Conv (k_w, 1) â”‚ Width â”‚
â”‚   â”‚ í° ì»¤ë„ ì‚¬ìš©   â”‚                 â”‚ í° ì»¤ë„ ì‚¬ìš©   â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚           â”‚                                 â”‚                â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                         â†“                                    â”‚
â”‚                    Add or Concat                            â”‚
â”‚                         â†“                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚        Output [B, C, H, W]              â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**NPU í˜¸í™˜ êµ¬í˜„:**
```python
class AxialConvBlock(nn.Module):
    """Axial Attention approximation using 1D Convolutions"""
    def __init__(self, channels, kernel_size=17):
        super().__init__()
        # Height-wise: (1, k) kernel
        self.conv_h = nn.Conv2d(
            channels, channels, 
            kernel_size=(kernel_size, 1),  # âœ… max 17
            padding=(kernel_size // 2, 0),
            groups=channels  # Depthwise for efficiency
        )
        # Width-wise: (k, 1) kernel  
        self.conv_w = nn.Conv2d(
            channels, channels,
            kernel_size=(1, kernel_size),  # âœ… max 17
            padding=(0, kernel_size // 2),
            groups=channels
        )
        self.norm = nn.BatchNorm2d(channels)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        h_attn = self.conv_h(x)
        w_attn = self.conv_w(x)
        return self.relu(self.norm(h_attn + w_attn + x))
```

**ì¥ì :**
- í° receptive field (17Ã—17 â†’ 33Ã—33 íš¨ê³¼)
- NPU ì™„ì „ í˜¸í™˜
- íŒŒë¼ë¯¸í„° ì¦ê°€ ìµœì†Œ

---

### 2.6 ECA (Efficient Channel Attention) - âœ… ê°•ë ¥ ê¶Œì¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ECA (Efficient Channel Attention)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   SE Blockì˜ ê°œì„  ë²„ì „ - FC ëŒ€ì‹  1D Conv ì‚¬ìš©                â”‚
â”‚                                                              â”‚
â”‚   Input: [B, C, H, W]                                       â”‚
â”‚           â”‚                                                  â”‚
â”‚           â†“                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚   â”‚ GlobalAvgPool     â”‚  [B, C, 1, 1]                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚             â†“                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚   â”‚ Squeeze [B, C]    â”‚  Unsqueeze í›„                       â”‚
â”‚   â”‚ â†’ Conv1D (k=3~5)  â”‚  ì¸ì ‘ ì±„ë„ ê´€ê³„ í•™ìŠµ                â”‚
â”‚   â”‚ â†’ Sigmoid         â”‚                                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚             â†“                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚   â”‚ Expand + Mul      â”‚  ì›ë³¸ê³¼ ê³±                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚             â†“                                                â”‚
â”‚   Output: [B, C, H, W]                                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**âš ï¸ NPU ì œì•½:**
- Conv1DëŠ” ì§ì ‘ ì§€ì›ë˜ì§€ ì•ŠìŒ
- **ëŒ€ì•ˆ**: Conv2D (kernel=1Ã—k)ë¡œ êµ¬í˜„

```python
class ECABlock(nn.Module):
    """ECA using Conv2D (NPU Compatible)"""
    def __init__(self, channels, k_size=5):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool2d(1)
        # Conv2d with kernel (1, k) simulates 1D conv
        self.conv = nn.Conv2d(
            1, 1, 
            kernel_size=(1, k_size), 
            padding=(0, k_size // 2),
            bias=False
        )  # âœ… NPU ì§€ì›
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        B, C, H, W = x.shape
        # [B, C, 1, 1] â†’ [B, 1, 1, C]
        y = self.pool(x).view(B, 1, 1, C)
        # Conv2d acts as 1D conv on channel dimension
        y = self.conv(y)
        y = self.sigmoid(y).view(B, C, 1, 1)
        return x * y.expand_as(x)
```

**ì£¼ì˜:** `view/reshape` ì—°ì‚°ì´ í•„ìš” â†’ NPUì—ì„œ ë¯¸ì§€ì›ë  ìˆ˜ ìˆìŒ
â†’ **SE Blockì´ ë” ì•ˆì „í•œ ì„ íƒ**

---

## 3. ì ìš© ìœ„ì¹˜ë³„ ê¶Œì¥ ì‚¬í•­

### 3.1 ëª¨ë¸ êµ¬ì¡°ìƒ Attention ì ìš© ìœ„ì¹˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ê¶Œì¥ Attention ì ìš© ìœ„ì¹˜                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   RGB Input                                                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â†“                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                    ResNet-18 Encoder                     â”‚  â”‚
â”‚   â”‚                                                          â”‚  â”‚
â”‚   â”‚   Layer1 (64ch)  â†’ [1ï¸âƒ£ SE Block ì„ íƒì ]                  â”‚  â”‚
â”‚   â”‚        â”‚                                                 â”‚  â”‚
â”‚   â”‚   Layer2 (128ch) â†’ [1ï¸âƒ£ SE Block ì„ íƒì ]                  â”‚  â”‚
â”‚   â”‚        â”‚                                                 â”‚  â”‚
â”‚   â”‚   Layer3 (256ch) â†’ [2ï¸âƒ£ CBAM (Channel + Spatial)]        â”‚  â”‚
â”‚   â”‚        â”‚              â† ê°€ì¥ íš¨ê³¼ì ì¸ ìœ„ì¹˜                â”‚  â”‚
â”‚   â”‚   Layer4 (512ch) â†’ [2ï¸âƒ£ SE Block Only]                   â”‚  â”‚
â”‚   â”‚                       â† Spatial ì œì™¸ (ì±„ë„ > 256)        â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                  â”‚
â”‚              Skip Connectionsâ”‚                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚              â”‚               â”‚               â”‚                  â”‚
â”‚              â†“               â†“               â†“                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                    Dual-Head Decoder                     â”‚  â”‚
â”‚   â”‚                                                          â”‚  â”‚
â”‚   â”‚   UpConv4 (256ch) â”€â”¬â”€ [3ï¸âƒ£ SE Block]                      â”‚  â”‚
â”‚   â”‚        â”‚           â”‚                                     â”‚  â”‚
â”‚   â”‚   UpConv3 (128ch) â”€â”´â”€ [3ï¸âƒ£ SE Block]                      â”‚  â”‚
â”‚   â”‚        â”‚              â† Skip fusion ì§í›„                  â”‚  â”‚
â”‚   â”‚   UpConv2 (64ch)  â†’ [3ï¸âƒ£ Axial Conv ì„ íƒì ]               â”‚  â”‚
â”‚   â”‚        â”‚              â† ê³ í•´ìƒë„ì—ì„œ í° receptive field   â”‚  â”‚
â”‚   â”‚   UpConv1 (32ch)  â†’ [4ï¸âƒ£ Skip - ìµœì¢… ì¶œë ¥ ì§ì „]           â”‚  â”‚
â”‚   â”‚        â”‚                                                 â”‚  â”‚
â”‚   â”‚   UpConv0 (16ch)  â†’ Integer + Fractional Heads          â”‚  â”‚
â”‚   â”‚                                                          â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ìš°ì„ ìˆœìœ„ë³„ ì ìš© ì „ëµ

| ìš°ì„ ìˆœìœ„ | ìœ„ì¹˜ | ì ìš© ëª¨ë“ˆ | ì˜ˆìƒ íš¨ê³¼ | ì—°ì‚° ì¦ê°€ |
|---------|------|----------|----------|----------|
| **ğŸ¥‡ 1ìˆœìœ„** | Encoder Layer3 (256ch) | CBAM | ì¤‘ê°„ ë ˆë²¨ feature ê°•í™” | ~3% |
| **ğŸ¥‡ 1ìˆœìœ„** | Decoder Skip Fusion | SE Block | ì¤‘ìš” ì±„ë„ ì„ íƒ | ~1% |
| **ğŸ¥ˆ 2ìˆœìœ„** | Encoder Layer4 (512ch) | SE Block | ê³ ìˆ˜ì¤€ semantic ê°•í™” | ~0.5% |
| **ğŸ¥ˆ 2ìˆœìœ„** | Decoder UpConv2 | Axial Conv | ê²½ê³„ ì„ ëª…ë„ í–¥ìƒ | ~2% |
| **ğŸ¥‰ 3ìˆœìœ„** | Encoder Layer1-2 | SE Block | ì €ìˆ˜ì¤€ feature ê°•í™” | ~0.5% |

---

## 4. êµ¬í˜„ ê¶Œì¥ ì½”ë“œ

### 4.1 NPU-Compatible SE Block

```python
class NPUSEBlock(nn.Module):
    """
    Squeeze-and-Excitation Block optimized for AImotive NPU
    
    All operations are NPU-compatible:
    - GlobalAveragePool âœ…
    - Conv2d 1Ã—1 âœ…
    - ReLU âœ…
    - Sigmoid âœ…
    - Element-wise Mul âœ…
    """
    def __init__(self, channels: int, reduction: int = 16):
        super().__init__()
        reduced_channels = max(channels // reduction, 8)  # ìµœì†Œ 8ì±„ë„
        
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(channels, reduced_channels, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(reduced_channels, channels, 1, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        scale = self.pool(x)  # [B, C, 1, 1]
        scale = self.fc(scale)  # [B, C, 1, 1]
        return x * scale  # Broadcast mul
```

### 4.2 NPU-Compatible CBAM (Channel Attention Only for 512ch)

```python
class NPUCBAMBlock(nn.Module):
    """
    CBAM with NPU constraints consideration
    - Spatial Attention disabled for channels > 256
    """
    def __init__(self, channels: int, reduction: int = 16, 
                 use_spatial: bool = True):
        super().__init__()
        self.channels = channels
        self.use_spatial = use_spatial and (channels <= 256)
        
        # Channel Attention (always enabled)
        reduced_channels = max(channels // reduction, 8)
        self.ca_pool_avg = nn.AdaptiveAvgPool2d(1)
        self.ca_pool_max = nn.AdaptiveMaxPool2d(1)
        self.ca_mlp = nn.Sequential(
            nn.Conv2d(channels, reduced_channels, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(reduced_channels, channels, 1, bias=False)
        )
        
        # Spatial Attention (conditional)
        if self.use_spatial:
            self.sa_conv = nn.Sequential(
                nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),
                nn.Sigmoid()
            )
        
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Channel Attention
        ca_avg = self.ca_mlp(self.ca_pool_avg(x))
        ca_max = self.ca_mlp(self.ca_pool_max(x))
        ca = self.sigmoid(ca_avg + ca_max)
        x = x * ca
        
        # Spatial Attention (if channels <= 256)
        if self.use_spatial:
            sa_avg = torch.mean(x, dim=1, keepdim=True)
            sa_max, _ = torch.max(x, dim=1, keepdim=True)
            sa = self.sa_conv(torch.cat([sa_avg, sa_max], dim=1))
            x = x * sa
        
        return x
```

### 4.3 Skip Connectionì— Attention ì ìš©

```python
class AttentiveSkipFusion(nn.Module):
    """
    Attention-weighted skip connection fusion
    í•™ìŠµ ê°€ëŠ¥í•œ attentionìœ¼ë¡œ encoder/decoder íŠ¹ì§• ì„ íƒì  ê²°í•©
    """
    def __init__(self, enc_channels: int, dec_channels: int):
        super().__init__()
        total_channels = enc_channels + dec_channels
        
        # SE-style attention for fused features
        self.attention = NPUSEBlock(total_channels, reduction=8)
        
        # 1x1 conv to match channels
        self.conv = nn.Conv2d(total_channels, dec_channels, 1, bias=False)
        self.bn = nn.BatchNorm2d(dec_channels)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, enc_feat: torch.Tensor, dec_feat: torch.Tensor) -> torch.Tensor:
        # Concatenate
        fused = torch.cat([enc_feat, dec_feat], dim=1)
        
        # Apply channel attention
        fused = self.attention(fused)
        
        # Reduce channels
        out = self.relu(self.bn(self.conv(fused)))
        return out
```

---

## 5. ìˆ˜ì •ëœ Decoder êµ¬ì¡° ì œì•ˆ

### 5.1 Attention-Enhanced Dual-Head Decoder

```python
class AttentionDualHeadDecoder(nn.Module):
    """
    Dual-Head Decoder with SE Attention at skip connections
    """
    def __init__(self, num_ch_enc, scales=range(4), max_depth=15.0):
        super().__init__()
        self.num_ch_enc = num_ch_enc
        self.scales = scales
        self.max_depth = max_depth
        self.num_ch_dec = np.array([16, 32, 64, 128, 256])
        
        self.convs = OrderedDict()
        
        for i in range(4, -1, -1):
            # UpConv 0
            num_ch_in = num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]
            num_ch_out = self.num_ch_dec[i]
            self.convs[("upconv", i, 0)] = ConvBlock(num_ch_in, num_ch_out)
            
            # UpConv 1 with SE attention after skip connection
            num_ch_in = num_ch_out
            if i > 0:
                num_ch_in += num_ch_enc[i - 1]
            
            self.convs[("upconv", i, 1)] = ConvBlock(num_ch_in, num_ch_out)
            
            # ğŸ†• SE Attention after fusion (i > 0ì—ì„œë§Œ)
            if i > 0:
                self.convs[("se_block", i)] = NPUSEBlock(num_ch_out)
        
        # Dual Heads (unchanged)
        for s in scales:
            self.convs[("integer_conv", s)] = Conv3x3(self.num_ch_dec[s], 1)
            self.convs[("fractional_conv", s)] = Conv3x3(self.num_ch_dec[s], 1)
        
        self.decoder = nn.ModuleList(list(self.convs.values()))
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, input_features):
        outputs = {}
        x = input_features[-1]
        
        for i in range(4, -1, -1):
            x = self.convs[("upconv", i, 0)](x)
            x = [upsample(x)]
            
            if i > 0:
                x += [input_features[i - 1]]
            
            x = torch.cat(x, 1)
            x = self.convs[("upconv", i, 1)](x)
            
            # ğŸ†• Apply SE attention after skip fusion
            if i > 0:
                x = self.convs[("se_block", i)](x)
            
            if i in self.scales:
                outputs[("integer", i)] = self.sigmoid(
                    self.convs[("integer_conv", i)](x))
                outputs[("fractional", i)] = self.sigmoid(
                    self.convs[("fractional_conv", i)](x))
        
        return outputs
```

---

## 6. ì‹¤í—˜ ê³„íš

### 6.1 ë‹¨ê³„ë³„ ì ìš© ë° í‰ê°€

| ë‹¨ê³„ | ì ìš© ë‚´ìš© | í‰ê°€ ì§€í‘œ | ì˜ˆìƒ ê²°ê³¼ |
|-----|----------|----------|----------|
| **Baseline** | í˜„ì¬ ëª¨ë¸ | Abs Rel, RMSE, Î´<1.25 | ê¸°ì¤€ |
| **Stage 1** | Decoder SE Block | ìœ„ ì§€í‘œ + Latency | Î” < 5ms, Î´â†‘1-2% |
| **Stage 2** | Encoder L3 CBAM | ìœ„ ì§€í‘œ + Latency | ê²½ê³„ ì •í™•ë„ ê°œì„  |
| **Stage 3** | Axial Conv (ì„ íƒ) | ìœ„ ì§€í‘œ + ë©”ëª¨ë¦¬ | ì›ê±°ë¦¬ ì •í™•ë„ ê°œì„  |

### 6.2 NPU ë°°í¬ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ONNX ë³€í™˜ ì„±ê³µ ì—¬ë¶€
- [ ] Reshape ì—°ì‚° ì—†ìŒ í™•ì¸
- [ ] ëª¨ë“  Conv kernel â‰¤ 17
- [ ] ì¶œë ¥ ì±„ë„ 8ì˜ ë°°ìˆ˜ í™•ì¸
- [ ] AImotive Compiler í†µê³¼

---

## 7. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

### 7.1 ì¦‰ì‹œ ì ìš© ê¶Œì¥ (Low Risk, High Return)

1. **SE Block on Decoder Skip Connections**
   - êµ¬í˜„ ë‚œì´ë„: â­ (ì‰¬ì›€)
   - NPU í˜¸í™˜ì„±: âœ… ì™„ì „ í˜¸í™˜
   - ì˜ˆìƒ ê°œì„ : 1-3% accuracy, <5% latency ì¦ê°€

2. **CBAM on Encoder Layer3 (256ch)**
   - êµ¬í˜„ ë‚œì´ë„: â­â­ (ë³´í†µ)
   - NPU í˜¸í™˜ì„±: âœ… ì™„ì „ í˜¸í™˜
   - ì˜ˆìƒ ê°œì„ : 2-4% accuracy (íŠ¹íˆ ì¤‘ê±°ë¦¬)

### 7.2 ì¶”ê°€ ê²€í†  í•„ìš” (Medium Risk)

3. **Axial Conv for Large Receptive Field**
   - í° ì»¤ë„ (17Ã—1, 1Ã—17)ë¡œ ì›ê±°ë¦¬ ì˜ì¡´ì„± ê°œì„ 
   - ì¶”ê°€ ì‹¤í—˜ í•„ìš”

### 7.3 ì ìš© ë¶ˆê°€ (NPU ì œì•½)

- âŒ Full Self-Attention (Reshape, MatMul ë¯¸ì§€ì›)
- âŒ Transformer Block (ìœ„ì™€ ë™ì¼)
- âŒ Cross-Attention (ìœ„ì™€ ë™ì¼)
- âŒ Deformable Convolution (ë¯¸ì§€ì›)

---

## 8. ì°¸ê³  ë¬¸í—Œ

1. Hu, J., et al. "Squeeze-and-Excitation Networks" (CVPR 2018)
2. Woo, S., et al. "CBAM: Convolutional Block Attention Module" (ECCV 2018)
3. Wang, Q., et al. "ECA-Net: Efficient Channel Attention" (CVPR 2020)
4. Ho, J., et al. "Axial Attention in Multidimensional Transformers" (2019)
5. Ranftl, R., et al. "Vision Transformers for Dense Prediction" (ICCV 2021)

---

**ì‘ì„±ì**: AI Analysis System  
**ê²€í†  í•„ìš”**: ì‹¤ì œ NPU í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜ì˜  
**ë‹¤ìŒ ë‹¨ê³„**: SE Block êµ¬í˜„ ë° í•™ìŠµ ì‹¤í—˜
