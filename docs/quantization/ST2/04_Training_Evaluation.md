# 4. í•™ìŠµ ë° í‰ê°€

## í•™ìŠµ ì‹¤í–‰

### 4.1. í•™ìŠµ ëª…ë ¹ì–´

```bash
cd /workspace/packnet-sfm

# Dual-Head ëª¨ë¸ í•™ìŠµ
python scripts/train.py \
    configs/train_resnet_san_ncdb_dual_head_640x384.yaml

# í•™ìŠµ ì§„í–‰ í™•ì¸
tail -f checkpoints/resnetsan01_dual_head_640x384/training.log
```

### 4.2. í•™ìŠµ íŒŒë¼ë¯¸í„°

**ê¶Œì¥ ì„¤ì •**:

```yaml
# configs/train_resnet_san_ncdb_dual_head_640x384.yaml
datasets:
    train:
        batch_size: 4  # ê¸°ì¡´ê³¼ ë™ì¼
    validation:
        batch_size: 4

optimizer:
    name: 'Adam'
    learning_rate: 2.0e-4  # ê¸°ì¡´ê³¼ ë™ì¼
    weight_decay: 0.0

scheduler:
    name: 'StepLR'
    step_size: 15
    gamma: 0.1

checkpoint:
    save_top_k: 3
    monitor: 'abs_rel'
    mode: 'min'

trainer:
    max_epochs: 30
    gradient_clip_val: 1.0
    check_val_every_n_epoch: 1
```

---

## í•™ìŠµ ëª¨ë‹ˆí„°ë§

### 4.3. ì£¼ìš” ë©”íŠ¸ë¦­

| Epoch | Integer Loss | Fractional Loss | Consistency Loss | Val abs_rel |
|-------|--------------|-----------------|------------------|-------------|
| 1 | 0.050 | 0.080 | 0.120 | ~0.150 |
| 5 | 0.010 | 0.040 | 0.060 | ~0.120 |
| 10 | 0.005 | 0.020 | 0.030 | ~0.090 |
| 15 | 0.003 | 0.015 | 0.020 | ~0.070 |
| 20 | 0.002 | 0.010 | 0.015 | ~0.060 |
| 25 | 0.001 | 0.007 | 0.012 | ~0.057 |
| **30** | **0.001** | **0.005** | **0.010** | **~0.055** |

**ê¸°ëŒ€ ì‚¬í•­**:
- **Integer Loss**: ë¹ ë¥´ê²Œ ìˆ˜ë ´ (Epoch 5ì— 0.01 ì´í•˜)
- **Fractional Loss**: ì²œì²œíˆ ê°ì†Œ (í•µì‹¬ ì •ë°€ë„)
- **Consistency Loss**: ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œ
- **Val abs_rel**: 30 epochì— 0.055 ë‹¬ì„± ëª©í‘œ

### 4.3.1. í•™ìŠµ ì´ìƒ íƒì§€ ê¸°ì¤€ (Health Check)

**ğŸŸ¢ Epoch 5 ì²´í¬í¬ì¸íŠ¸**:

| ë©”íŠ¸ë¦­ | ì •ìƒ (âœ…) | ê²½ê³  (âš ï¸) | ë¹„ì •ìƒ (âŒ) |
|--------|----------|----------|-----------|
| Integer Loss | < 0.012 | 0.012~0.020 | > 0.020 |
| Fractional Loss | < 0.045 | 0.045~0.060 | > 0.060 |
| Consistency Loss | < 0.065 | 0.065~0.080 | > 0.080 |
| Val abs_rel | < 0.125 | 0.125~0.140 | > 0.140 |

**ì¡°ì¹˜ ì‚¬í•­**:
- âœ… **ì •ìƒ**: ê³„ì† í•™ìŠµ
- âš ï¸ **ê²½ê³ **: ë¡œê·¸ í™•ì¸, ë‹¤ìŒ ì²´í¬í¬ì¸íŠ¸ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°
- âŒ **ë¹„ì •ìƒ**: í•™ìŠµ ì¤‘ë‹¨, Troubleshooting ì„¹ì…˜ ì°¸ì¡°

**ğŸŸ¡ Epoch 10 ì²´í¬í¬ì¸íŠ¸**:

| ë©”íŠ¸ë¦­ | ì •ìƒ (âœ…) | ê²½ê³  (âš ï¸) | ë¹„ì •ìƒ (âŒ) |
|--------|----------|----------|-----------|
| Integer Loss | < 0.007 | 0.007~0.015 | > 0.015 |
| Fractional Loss | < 0.025 | 0.025~0.035 | > 0.035 |
| Consistency Loss | < 0.035 | 0.035~0.045 | > 0.045 |
| Val abs_rel | < 0.095 | 0.095~0.110 | > 0.110 |

**ğŸ”µ Epoch 20 ì²´í¬í¬ì¸íŠ¸** (ìµœì¢… ìˆ˜ë ´ í™•ì¸):

| ë©”íŠ¸ë¦­ | ì •ìƒ (âœ…) | ê²½ê³  (âš ï¸) | ë¹„ì •ìƒ (âŒ) |
|--------|----------|----------|-----------|
| Integer Loss | < 0.003 | 0.003~0.005 | > 0.005 |
| Fractional Loss | < 0.012 | 0.012~0.018 | > 0.018 |
| Consistency Loss | < 0.018 | 0.018~0.025 | > 0.025 |
| Val abs_rel | < 0.065 | 0.065~0.075 | > 0.075 |

**ë¹„ì •ìƒ ìƒí™© ëŒ€ì‘**:

1. **Integer Lossê°€ ë†’ìŒ** (> ì„ê³„ê°’):
   - ì›ì¸: Learning rateê°€ ë„ˆë¬´ ë‚®ìŒ, ë˜ëŠ” max_depth ì„¤ì • ì˜¤ë¥˜
   - ì¡°ì¹˜: LR ì¦ê°€ (2e-4 â†’ 3e-4), max_depth í™•ì¸

2. **Fractional Lossê°€ ë†’ìŒ** (> ì„ê³„ê°’):
   - ì›ì¸: `fractional_weight`ê°€ ë‚®ìŒ (ê¸°ë³¸ê°’ 10.0 ë¯¸ë§Œ)
   - ì¡°ì¹˜: `fractional_weight`ë¥¼ 15.0~20.0ìœ¼ë¡œ ì¦ê°€

3. **Val abs_relì´ ì •ì²´** (20 epoch ì´í›„ë„ > 0.075):
   - ì›ì¸: ê³¼ì í•© ë˜ëŠ” ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ
   - ì¡°ì¹˜: Early stopping, ë°ì´í„°ì…‹ ê²€ì¦

### 4.4. TensorBoard ëª¨ë‹ˆí„°ë§

```bash
# TensorBoard ì‹¤í–‰
tensorboard --logdir checkpoints/resnetsan01_dual_head_640x384

# ì£¼ìš” í™•ì¸ ì‚¬í•­:
# 1. Loss curves: Integer/Fractional/Consistency ëª¨ë‘ ê°ì†Œ ì¶”ì„¸
# 2. Validation metrics: abs_rel, rmse, Î´<1.25
# 3. Learning rate schedule: Step decay í™•ì¸
# 4. Gradient norms: í­ë°œí•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
```

### 4.5. í•™ìŠµ ì¤‘ ì²´í¬í¬ì¸íŠ¸

```bash
# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™•ì¸
ls -lh checkpoints/resnetsan01_dual_head_640x384/*.ckpt

# ì¤‘ê°„ í‰ê°€ (Epoch 15)
python scripts/eval.py \
    --checkpoint checkpoints/resnetsan01_dual_head_640x384/epoch_15.ckpt \
    --config configs/train_resnet_san_ncdb_dual_head_640x384.yaml

# ìµœì¢… í‰ê°€ (Epoch 30)
python scripts/eval.py \
    --checkpoint checkpoints/resnetsan01_dual_head_640x384/epoch_30.ckpt \
    --config configs/train_resnet_san_ncdb_dual_head_640x384.yaml
```

---

## í‰ê°€ í”„ë¡œì„¸ìŠ¤

### 4.6. FP32 í‰ê°€ (PyTorch)

**âš ï¸ ì¤‘ìš”: ê³µì‹ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©**

Dual-Head ëª¨ë¸ì˜ FP32 ì„±ëŠ¥ì„ í‰ê°€í•  ë•ŒëŠ” **`scripts/eval_official.py`ë¥¼ ìˆ˜ì •**í•˜ì—¬ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

#### ë°©ë²• 1: Validation Set í‰ê°€ (ê¶Œì¥)

```bash
# eval_official.pyë¥¼ ì‚¬ìš©í•˜ì—¬ validation set í‰ê°€
python scripts/eval_official.py \
    --checkpoint checkpoints/resnetsan01_dual_head_640x384/epoch_30.ckpt \
    --config configs/train_resnet_san_ncdb_dual_head_640x384.yaml \
    --split val
```

**`eval_official.py` ìˆ˜ì • ì‚¬í•­**:

ê¸°ì¡´ íŒŒì¼ì€ `val`/`test` splitì„ ì§€ì›í•©ë‹ˆë‹¤. Dual-Head ëª¨ë¸ì— ëŒ€í•´ì„œëŠ” ìˆ˜ì • ë¶ˆí•„ìš”í•˜ì§€ë§Œ, 
`use_dual_head=true` ì„¤ì •ì´ YAMLì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

```python
# scripts/eval_official.py (ê¸°ì¡´ íŒŒì¼ ì‚¬ìš© ê°€ëŠ¥)
#!/usr/bin/env python3
"""
Official evaluation script modified to support validation set evaluation
Based on scripts/eval.py
"""

import argparse
import torch

from packnet_sfm.models.model_wrapper import ModelWrapper
from packnet_sfm.trainers.horovod_trainer import HorovodTrainer
from packnet_sfm.utils.config import parse_test_file
from packnet_sfm.utils.load import set_debug
from packnet_sfm.utils.horovod import hvd_init


def parse_args():
    """Parse arguments for evaluation script"""
    parser = argparse.ArgumentParser(description='PackNet-SfM evaluation script (with val/test support)')
    parser.add_argument('--checkpoint', type=str, required=True, help='Checkpoint (.ckpt)')
    parser.add_argument('--config', type=str, default=None, help='Configuration (.yaml)')
    parser.add_argument('--split', type=str, default='test', choices=['val', 'test'],
                       help='Dataset split to evaluate (val or test)')
    parser.add_argument('--half', action="store_true", help='Use half precision (fp16)')
    args = parser.parse_args()
    return args


def evaluate(ckpt_file, cfg_file, split, half):
    """Evaluation function"""
    # Initialize horovod
    hvd_init()

    # Parse arguments
    config, state_dict = parse_test_file(ckpt_file, cfg_file)

    # Set debug if requested
    set_debug(config.debug)

    # Initialize model wrapper
    model_wrapper = ModelWrapper(config)
    model_wrapper.load_state_dict(state_dict, strict=False)

    # Change to half precision if requested
    config.arch["dtype"] = torch.float16 if half else None

    # Create trainer
    trainer = HorovodTrainer(**config.arch)

    # Choose evaluation method based on split
    if split == 'val':
        print("\n" + "="*80)
        print(f"ğŸ“Š VALIDATION SET EVALUATION")
        print("="*80)
        
        # Send module to GPU
        model_wrapper = model_wrapper.to('cuda', dtype=trainer.dtype)
        # Get validation dataloaders
        val_dataloaders = model_wrapper.val_dataloader()
        # Run validation
        trainer.validate(val_dataloaders, model_wrapper)
        
    else:  # test
        print("\n" + "="*80)
        print(f"ğŸ“Š TEST SET EVALUATION")
        print("="*80)
        
        # Use standard test method
        trainer.test(model_wrapper)


if __name__ == '__main__':
    args = parse_args()
    evaluate(args.checkpoint, args.config, args.split, args.half)
```

#### ë°©ë²• 2: PyTorch ì˜ˆì¸¡ ìƒì„± í›„ ë³„ë„ í‰ê°€

```bash
# Step 1: PyTorch ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„± (.npy íŒŒì¼)
python scripts/generate_pytorch_predictions.py \
    --checkpoint checkpoints/resnetsan01_dual_head_640x384/epoch_30.ckpt \
    --config configs/train_resnet_san_ncdb_dual_head_640x384.yaml \
    --output_dir outputs/pytorch_fp32_predictions

# Step 2: ìƒì„±ëœ ì˜ˆì¸¡ì„ í‰ê°€
python scripts/evaluate_predictions.py \
    --pred_dir outputs/pytorch_fp32_predictions \
    --test_json /workspace/data/ncdb-cls-640x384/splits/combined_test.json
```

**`generate_pytorch_predictions.py` ì‚¬ìš© ë°©ë²•**:

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” **ê³µì‹ í‰ê°€ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•œ ë°©ì‹**ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤:

```python
# scripts/generate_pytorch_predictions.py (ê¸°ì¡´ íŒŒì¼ í™œìš©)
"""
Generate PyTorch FP32 predictions using the same pipeline as official eval.
This ensures predictions match exactly what the official evaluation uses.
"""
# (ê¸°ì¡´ íŒŒì¼ ì°¸ì¡° - ìˆ˜ì • ë¶ˆí•„ìš”)
```

**ì‚¬ìš© ì˜ˆì‹œ**:

```bash
# Dual-Head ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìƒì„±
python scripts/generate_pytorch_predictions.py \
    --checkpoint checkpoints/resnetsan01_dual_head_640x384/epoch_30.ckpt \
    --config configs/train_resnet_san_ncdb_dual_head_640x384.yaml \
    --output_dir outputs/dual_head_fp32_predictions

# ì¶œë ¥: outputs/dual_head_fp32_predictions/*.npy (ê° ì´ë¯¸ì§€ë³„ depth map)
```

#### ì˜ˆìƒ FP32 ê²°ê³¼

```json
{
    "abs_rel": 0.038,
    "sq_rel": 0.045,
    "rmse": 0.350,
    "rmse_log": 0.055,
    "a1": 0.982,
    "a2": 0.996,
    "a3": 0.999
}
```

**ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸**:

1. **`eval_official.py` ì‚¬ìš©**:
   - Validation set í‰ê°€ì— ìµœì í™”
   - ê³µì‹ í‰ê°€ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼
   - `--split val` ë˜ëŠ” `--split test` ì„ íƒ ê°€ëŠ¥

2. **`generate_pytorch_predictions.py` ì‚¬ìš©**:
   - NPU ê²°ê³¼ì™€ ì§ì ‘ ë¹„êµ ê°€ëŠ¥í•œ .npy íŒŒì¼ ìƒì„±
   - ë™ì¼í•œ í›„ì²˜ë¦¬ ì ìš© ë³´ì¥
   - ë””ë²„ê¹… ë° ë¶„ì„ì— ìœ ìš©

3. **YAML ì„¤ì • í™•ì¸**:
   ```yaml
   depth_net:
       name: 'ResNetSAN01'
       use_dual_head: true  # âœ… í•„ìˆ˜!
   ```

### 4.7. ONNX Export

```bash
# Dual-Head ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
python scripts/export_to_onnx.py \
    --checkpoint checkpoints/resnetsan01_dual_head_640x384/epoch_30.ckpt \
    --output onnx/resnetsan_dual_head.onnx \
    --dual_head  # ğŸ†• Dual output í”Œë˜ê·¸
```

**Export ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • í•„ìš”**:

```python
# scripts/export_to_onnx.py (ìˆ˜ì • í•„ìš”)
import torch
from packnet_sfm.networks.depth.ResNetSAN01 import ResNetSAN01

# Load model
model = ResNetSAN01(version='18A', use_dual_head=True, max_depth=15.0)
checkpoint = torch.load(args.checkpoint)
model.load_state_dict(checkpoint['state_dict'])
model.eval()

# Dummy input
dummy_input = torch.randn(1, 3, 384, 640)

# Export with dual outputs
torch.onnx.export(
    model,
    dummy_input,
    args.output,
    input_names=['rgb'],
    output_names=['integer_sigmoid', 'fractional_sigmoid'],  # ğŸ†• ëª…ì‹œ
    dynamic_axes={'rgb': {0: 'batch_size'}},
    opset_version=11
)
```

### 4.8. NPU ë³€í™˜ ë° í‰ê°€

```bash
# ONNX â†’ NPU ë³€í™˜ (Pulsar2 ì‚¬ìš©)
pulsar2 build \
    --input onnx/resnetsan_dual_head.onnx \
    --output npu/resnetsan_dual_head.joint \
    --config configs/npu_config_dual_head.json \
    --calibration_data calibration_data_300/

# NPU INT8 í‰ê°€
python scripts/evaluate_npu_dual_head.py \
    --npu_model npu/resnetsan_dual_head.joint \
    --output_dir outputs/dual_head_npu_results
```

**âš ï¸ NPU í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ë°©ë²•**:

ê¸°ì¡´ `scripts/evaluate_npu_direct_depth_official.py`ë¥¼ **ìˆ˜ì •**í•˜ì—¬ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# scripts/evaluate_npu_dual_head.py
"""
NPU Dual-Head ëª¨ë¸ í‰ê°€
Integer/Fractional ë‘ ì¶œë ¥ì„ ë°›ì•„ì„œ depth ë³µì›

ğŸ“ NOTE: 
ê¸°ì¡´ evaluate_npu_direct_depth_official.pyë¥¼ ì°¸ê³ í•˜ë˜, 
Dual-Head ì¶œë ¥ ì²˜ë¦¬ ë¡œì§ìœ¼ë¡œ ìˆ˜ì • í•„ìš”
"""

import numpy as np
import json
import torch
from pathlib import Path
from PIL import Image
from packnet_sfm.networks.layers.resnet.layers import dual_head_to_depth


def load_gt_depth(new_filename, test_json_path):
    """GT depth ë¡œë“œ (combined_test.json ê¸°ë°˜)"""
    with open(test_json_path, 'r') as f:
        data = json.load(f)
    
    for entry in data:
        if entry['new_filename'] == new_filename:
            dataset_root = entry['dataset_root']
            depth_path = Path(dataset_root) / 'newest_depth_maps' / f'{new_filename}.png'
            
            if not depth_path.exists():
                raise FileNotFoundError(f"GT depth not found: {depth_path}")
            
            # PNG 16-bit ë¡œë“œ â†’ meters
            depth_img = Image.open(depth_path)
            depth = np.array(depth_img, dtype=np.float32) / 256.0
            
            return depth
    
    raise ValueError(f"new_filename {new_filename} not found in {test_json_path}")


def compute_depth_metrics(gt, pred, min_depth=0.5, max_depth=15.0):
    """ê³µì‹ eval.pyì˜ compute_depth_metrics() ì¬í˜„"""
    valid_mask = (gt > min_depth) & (gt < max_depth)
    
    if valid_mask.sum() == 0:
        return None
    
    gt_valid = gt[valid_mask]
    pred_valid = pred[valid_mask]
    
    # Metrics
    thresh = np.maximum((gt_valid / pred_valid), (pred_valid / gt_valid))
    a1 = (thresh < 1.25).mean()
    a2 = (thresh < 1.25 ** 2).mean()
    a3 = (thresh < 1.25 ** 3).mean()
    
    abs_rel = np.mean(np.abs(gt_valid - pred_valid) / gt_valid)
    sq_rel = np.mean(((gt_valid - pred_valid) ** 2) / gt_valid)
    
    rmse = np.sqrt(np.mean((gt_valid - pred_valid) ** 2))
    rmse_log = np.sqrt(np.mean((np.log(gt_valid) - np.log(pred_valid)) ** 2))
    
    return {
        'abs_rel': abs_rel,
        'sq_rel': sq_rel,
        'rmse': rmse,
        'rmse_log': rmse_log,
        'a1': a1,
        'a2': a2,
        'a3': a3,
        'valid_pixels': int(valid_mask.sum())
    }


def main():
    import argparse
    import onnxruntime as ort
    
    parser = argparse.ArgumentParser(description='Evaluate NPU Dual-Head model')
    parser.add_argument('--npu_dir', type=str, required=True,
                       help='NPU output directory with .npy files')
    parser.add_argument('--test_json', type=str, 
                       default='/workspace/data/ncdb-cls-640x384/splits/combined_test.json',
                       help='Test JSON path')
    parser.add_argument('--min_depth', type=float, default=0.5, help='Min depth')
    parser.add_argument('--max_depth', type=float, default=15.0, help='Max depth')
    args = parser.parse_args()
    
    # Configuration
    npu_output_dir = Path(args.npu_dir)
    test_json = args.test_json
    min_depth = args.min_depth
    max_depth = args.max_depth
    
    # NPU ì¶œë ¥ íŒŒì¼ ë¡œë“œ (integer_*.npy, fractional_*.npy í˜•ì‹)
    integer_files = sorted(npu_output_dir.glob('integer_*.npy'))
    fractional_files = sorted(npu_output_dir.glob('fractional_*.npy'))
    
    print("="*80)
    print("ğŸš€ NPU Dual-Head í‰ê°€")
    print("="*80)
    print(f"ğŸ“ NPU output dir: {npu_output_dir}")
    print(f"ğŸ“Š Depth range: [{min_depth}, {max_depth}]m")
    print(f"ğŸ“Š Integer outputs: {len(integer_files)}")
    print(f"ğŸ“Š Fractional outputs: {len(fractional_files)}")
    print()
    
    all_metrics = []
    
    for int_file, frac_file in zip(integer_files, fractional_files):
        # íŒŒì¼ëª…ì—ì„œ new_filename ì¶”ì¶œ
        new_filename = int_file.stem.replace('integer_', '')
        
        # Load NPU Dual-Head outputs
        integer_sigmoid = np.load(int_file)
        fractional_sigmoid = np.load(frac_file)
        
        # Shape normalization
        while integer_sigmoid.ndim > 2:
            integer_sigmoid = integer_sigmoid.squeeze(0)
        while fractional_sigmoid.ndim > 2:
            fractional_sigmoid = fractional_sigmoid.squeeze(0)
        
        # Convert to torch tensors
        integer_sigmoid = torch.from_numpy(integer_sigmoid).unsqueeze(0).unsqueeze(0)  # [1,1,H,W]
        fractional_sigmoid = torch.from_numpy(fractional_sigmoid).unsqueeze(0).unsqueeze(0)
        
        # Depth ë³µì› (dual_head_to_depth ì‚¬ìš©)
        depth_pred = dual_head_to_depth(
            integer_sigmoid, fractional_sigmoid, max_depth=max_depth
        )
        depth_pred = depth_pred.squeeze().numpy()
        
        # Load GT depth
        try:
            gt_depth = load_gt_depth(new_filename, test_json)
        except Exception as e:
            print(f"âš ï¸  SKIP: {new_filename} - GT loading failed: {e}")
            continue
        
        # Compute metrics
        metrics = compute_depth_metrics(gt_depth, depth_pred, min_depth, max_depth)
        
        if metrics is None:
            print(f"âš ï¸  SKIP: {new_filename} - No valid pixels")
            continue
        
        all_metrics.append(metrics)
        
        print(f"âœ… {new_filename}")
        print(f"   abs_rel: {metrics['abs_rel']:.4f}, "
              f"rmse: {metrics['rmse']:.4f}m, Î´<1.25: {metrics['a1']:.4f}")
        print()
    
    if not all_metrics:
        print("âŒ No valid results")
        return
    
    # Average metrics
    print("="*80)
    print("ğŸ“Š AVERAGE METRICS (NPU Dual-Head INT8)")
    print("="*80)
    
    for key in ['abs_rel', 'sq_rel', 'rmse', 'rmse_log', 'a1', 'a2', 'a3']:
        avg_val = np.mean([m[key] for m in all_metrics])
        print(f"   {key:12s}: {avg_val:.4f}")
    
    print(f"\nâœ… Total evaluated: {len(all_metrics)} images")
    print("="*80)


if __name__ == '__main__':
    main()
```

**ğŸ”‘ í•µì‹¬ ì°¨ì´ì **:

1. **ê¸°ì¡´ Direct Depth ìŠ¤í¬ë¦½íŠ¸**:
   - ë‹¨ì¼ ì¶œë ¥ (.npy íŒŒì¼)
   - Depth ë³€í™˜ ì—†ìŒ (ì´ë¯¸ Linear depth)

2. **Dual-Head ìŠ¤í¬ë¦½íŠ¸** (ìˆ˜ì • í•„ìš”):
   - ë‘ ê°œ ì¶œë ¥ (integer_*.npy, fractional_*.npy)
   - `dual_head_to_depth()` í•¨ìˆ˜ë¡œ depth ë³µì› í•„ìš”
   
**ì‚¬ìš© ë°©ë²•**:

```bash
# 1. NPU ì¶”ë¡  ì‹¤í–‰ (ë‘ ì¶œë ¥ ì €ì¥)
python scripts/run_npu_inference_dual_head.py \
    --npu_model npu/resnetsan_dual_head.joint \
    --output_dir outputs/dual_head_npu_outputs

# 2. í‰ê°€ ì‹¤í–‰
python scripts/evaluate_npu_dual_head.py \
    --npu_dir outputs/dual_head_npu_outputs \
    --test_json /workspace/data/ncdb-cls-640x384/splits/combined_test.json
```

---

## ì˜ˆìƒ ê²°ê³¼

### 4.9. FP32 ì„±ëŠ¥ (PyTorch)

| Metric | Single-Head (Baseline) | Dual-Head (Expected) | Improvement |
|--------|------------------------|----------------------|-------------|
| **abs_rel** | 0.0434 | **0.038~0.042** | **10-15%** |
| **rmse** | 0.391m | **0.35~0.38m** | **10-15%** |
| **Î´<1.25** | 0.9759 | **0.980~0.985** | **+0.5%** |

**ë¶„ì„**:
- Dual-HeadëŠ” FP32ì—ì„œë„ ì•½ê°„ì˜ ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ
- ì´ìœ : ë” ëª…ì‹œì ì¸ í‘œí˜„ (ì •ìˆ˜ë¶€ + ì†Œìˆ˜ë¶€)
- íŠ¹íˆ ì¤‘ê±°ë¦¬(5-10m)ì—ì„œ ì •ë°€ë„ í–¥ìƒ

### 4.10. INT8 ì„±ëŠ¥ (NPU)

| Metric | Phase 1 (300 cal) | Dual-Head INT8 | Improvement |
|--------|-------------------|----------------|-------------|
| **abs_rel** | 0.1139 | **0.055~0.065** | **47-52%** |
| **rmse** | 0.751m | **0.45~0.55m** | **33-40%** |
| **Î´<1.25** | 0.9061 | **0.965~0.975** | **6-7%** |

**ëª©í‘œ ë‹¬ì„± í‰ê°€**:

| ëª©í‘œ | í˜„ì¬ | ì˜ˆìƒ | ë‹¬ì„± ê°€ëŠ¥ì„± |
|------|------|------|-------------|
| abs_rel < 0.09 | 0.1139 | **0.055~0.065** | âœ… **ë†’ìŒ** |
| FP32 ëŒ€ë¹„ ê²©ì°¨ ì¶•ì†Œ | 2.6x | **1.5x** | âœ… **ë‹¬ì„±** |
| ì–‘ìí™” ì˜¤ì°¨ ê°ì†Œ | Â±28mm | **Â±2mm** | âœ… **14ë°° ê°œì„ ** |

### 4.11. ì •ë°€ë„ ë¶„ì„

**ì–‘ìí™” ê°„ê²© ë¹„êµ**:

| ë°©ì‹ | Integer Head | Fractional Head | ì „ì²´ ì •ë°€ë„ |
|------|--------------|-----------------|-------------|
| **Single-Head** | N/A | N/A | 56.9mm |
| **Dual-Head** | 58.8mm (15/255) | **3.92mm** (1/255) | **Â±2mm** |

**ê±°ë¦¬ë³„ ì˜¤ì°¨ ë¶„ì„**:

| ê±°ë¦¬ ë²”ìœ„ | Single-Head ì˜¤ì°¨ | Dual-Head ì˜¤ì°¨ | ê°œì„ ìœ¨ |
|-----------|------------------|----------------|--------|
| 0-1m | Â±28mm | Â±2mm | **14ë°°** |
| 1-5m | Â±28mm | Â±2mm | **14ë°°** |
| 5-10m | Â±28mm | Â±2mm | **14ë°°** |
| 10-15m | Â±28mm | Â±2mm | **14ë°°** |

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
- Fractional Headê°€ ì „ì²´ ì •ë°€ë„ ê²°ì • (3.92mm)
- ëª¨ë“  ê±°ë¦¬ ë²”ìœ„ì—ì„œ ê· ì¼í•œ ì •ë°€ë„ (ê±°ë¦¬ ë…ë¦½ì )
- Integer HeadëŠ” ì •í™•í•œ ë¯¸í„° ë‹¨ìœ„ ì„ íƒ ë‹´ë‹¹

---

## í•™ìŠµ ìŠ¤ì¼€ì¤„

### Week 1: êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸ (Day 1-5)

- **Day 1**: DualHeadDepthDecoder êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- **Day 2**: Helper functions ë° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- **Day 3**: ResNetSAN01 í†µí•© ë° í†µí•© í…ŒìŠ¤íŠ¸
- **Day 4**: Loss function êµ¬í˜„ ë° ê²€ì¦
- **Day 5**: YAML config ì¤€ë¹„ ë° í•™ìŠµ ì‹œì‘

### Week 2: í•™ìŠµ (Day 6-12)

- **Day 6-8**: ì´ˆê¸° í•™ìŠµ (Epoch 1-10)
  - Integer loss ìˆ˜ë ´ í™•ì¸
  - Fractional loss ê°ì†Œ ì¶”ì„¸ í™•ì¸
  
- **Day 9-10**: ì¤‘ê¸° í•™ìŠµ (Epoch 11-20)
  - Validation metrics ëª¨ë‹ˆí„°ë§
  - Learning rate schedule í™•ì¸
  
- **Day 11-12**: í›„ê¸° í•™ìŠµ (Epoch 21-30)
  - ìµœì¢… ìˆ˜ë ´ í™•ì¸
  - Best checkpoint ì„ ì •

### Week 3: í‰ê°€ ë° ë°°í¬ (Day 13-15)

- **Day 13**: FP32 í‰ê°€
  - Validation set í‰ê°€
  - Test set í‰ê°€
  - ë©”íŠ¸ë¦­ ë¹„êµ (vs Single-Head)
  
- **Day 14**: NPU ë³€í™˜
  - ONNX export
  - NPU quantization
  - INT8 ì •í™•ë„ ê²€ì¦
  
- **Day 15**: ìµœì¢… í‰ê°€ ë° ë¶„ì„
  - NPU test set í‰ê°€
  - ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì‘ì„±
  - ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸

---

## Success Criteria

### í•„ìˆ˜ ì¡°ê±´

- âœ… **FP32 abs_rel < 0.045**: Dual-Head ëª¨ë¸ì´ baseline ìœ ì§€ ë˜ëŠ” ê°œì„ 
- âœ… **INT8 abs_rel < 0.065**: ëª©í‘œ ë‹¬ì„± (í˜„ì¬ 0.1139 ëŒ€ë¹„ 47% ê°œì„ )
- âœ… **ì–‘ìí™” ì˜¤ì°¨ < 5mm**: Fractional head ì •ë°€ë„ (ëª©í‘œ 3.92mm)
- âœ… **Backward compatibility**: ê¸°ì¡´ ì½”ë“œ ì •ìƒ ë™ì‘

### ì„ íƒ ì¡°ê±´

- ğŸ¯ **FP32 abs_rel < 0.040**: ì´ˆê³¼ ë‹¬ì„±
- ğŸ¯ **INT8 abs_rel < 0.060**: ì´ˆê³¼ ë‹¬ì„±
- ğŸ¯ **FP32 ëŒ€ë¹„ ê²©ì°¨ < 1.5ë°°**: í˜„ì¬ 2.6ë°° ëŒ€ë¹„ ëŒ€í­ ê°œì„ 

### ì‹¤íŒ¨ ê¸°ì¤€

- âŒ **FP32 abs_rel > 0.050**: Baseline ëŒ€ë¹„ ì„±ëŠ¥ ì €í•˜
- âŒ **INT8 abs_rel > 0.090**: ëª©í‘œ ë¯¸ë‹¬ì„±
- âŒ **í•™ìŠµ ë¶ˆì•ˆì •**: NaN loss, gradient explosion ë“±

â†’ **ë‹¤ìŒ**: [Troubleshooting](05_Troubleshooting.md)
