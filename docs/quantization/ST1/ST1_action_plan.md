# ST1: Advanced PTQ Calibration - ì‹¤í–‰ ê³„íš

**ëª©í‘œ**: NCDB ë°ì´í„°ì…‹ì—ì„œ Representative Calibration ì´ë¯¸ì§€ 300ê°œë¥¼ ì„ ë³„í•˜ì—¬ NPU PTQì— ì‚¬ìš©  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 2-3ì‹œê°„  
**ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ **: abs_rel 0.1133 â†’ 0.085 (25% ê°œì„ )

---

## ğŸ“‹ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìš”ì•½

```
Step 1: ë©”íƒ€ë°ì´í„° ìƒì„± (1-2ì‹œê°„)
   â†“
Step 2: Stratified Sampling (30ë¶„)
   â†“
Step 3: ì´ë¯¸ì§€ ë³µì‚¬ (10ë¶„)
   â†“
Step 3.5: ë¶„ì„ ë° ì‹œê°í™” (ì„ íƒ, 5ë¶„)
   â†“
Step 4: NPU PTQ ì‹¤í–‰ (30ë¶„-1ì‹œê°„)
   â†“
Step 5: ì„±ëŠ¥ í‰ê°€ (30ë¶„)
```

---

## Step 1: ë©”íƒ€ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ë° ì‹¤í–‰

### 1.1. ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

`create_ncdb_metadata.py` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

```python
# create_ncdb_metadata.py
import os
import json
import numpy as np
import pandas as pd
from PIL import Image
from pathlib import Path
from tqdm import tqdm

class NcdbMetadataGenerator:
    """NCDB ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ìƒì„±ê¸°"""
    
    # ncdb_dataset.pyì™€ ë™ì¼í•œ ìš°ì„ ìˆœìœ„
    DEFAULT_DEPTH_VARIANTS = [
        'newest_depth_maps',
        'newest_synthetic_depth_maps',
        'new_depth_maps',
        'depth_maps',
    ]
    
    def __init__(self, dataset_root, depth_variants=None):
        self.dataset_root = Path(dataset_root)
        self.depth_variants = depth_variants or self.DEFAULT_DEPTH_VARIANTS
        
    def _load_depth_png(self, depth_path):
        """ncdb_dataset.pyì˜ _load_depth_pngì™€ ë™ì¼í•œ ë¡œì§"""
        try:
            depth_png = Image.open(depth_path)
            arr16 = np.asarray(depth_png, dtype=np.uint16)
            depth = arr16.astype(np.float32)
            
            # KITTI ìŠ¤íƒ€ì¼ë¡œ 256ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
            if depth.max() > 255:
                depth /= 256.0
            
            # ìœ íš¨í•˜ì§€ ì•Šì€ í”½ì…€ì„ 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
            depth[arr16 == 0] = 0
            
            return depth
        except (FileNotFoundError, OSError) as e:
            print(f"Depth load failed: {depth_path} ({e})")
            return None
    
    def _resolve_depth_path(self, base_dir, stem):
        """variant ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì¡´ì¬í•˜ëŠ” depth ê²½ë¡œ ë°˜í™˜"""
        for variant in self.depth_variants:
            depth_path = base_dir / variant / f"{stem}.png"
            if depth_path.exists():
                return depth_path, variant
        return None, None
    
    def analyze_split(self, split_file):
        """
        JSON split íŒŒì¼ì„ ì½ì–´ì„œ ê° ìƒ˜í”Œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            split_file: JSON split íŒŒì¼ ê²½ë¡œ (ì˜ˆ: 'train_split.json')
        
        Returns:
            DataFrame: ê° ìƒ˜í”Œì˜ ë©”íƒ€ë°ì´í„°
        """
        # Split íŒŒì¼ ë¡œë“œ
        split_path = self.dataset_root / split_file
        if not split_path.exists():
            raise FileNotFoundError(f"Split file not found: {split_path}")
        
        with open(split_path, 'r') as f:
            split_data = json.load(f)
        
        print(f"ì´ {len(split_data)}ê°œì˜ ìƒ˜í”Œ ë¶„ì„ ì¤‘...")
        
        metadata = []
        skipped = 0
        
        for entry in tqdm(split_data):
            dataset_root = entry.get('dataset_root', '')
            stem = entry.get('new_filename', '')
            
            if not stem:
                skipped += 1
                continue
            
            # ê²½ë¡œ êµ¬ì„±
            base_dir = self.dataset_root / dataset_root
            image_path = base_dir / 'image_a6' / f"{stem}.png"
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not image_path.exists():
                skipped += 1
                continue
            
            # Depth íŒŒì¼ íƒìƒ‰ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
            depth_path, depth_variant = self._resolve_depth_path(base_dir, stem)
            
            if depth_path is None:
                # Depthê°€ ì—†ëŠ” ìƒ˜í”Œì€ ìŠ¤í‚µ
                skipped += 1
                continue
            
            # ê¹Šì´ ë°ì´í„° ë¡œë“œ ë° ë¶„ì„
            try:
                depth = self._load_depth_png(depth_path)
                
                if depth is None:
                    skipped += 1
                    continue
                
                # ìœ íš¨í•œ ê¹Šì´ ê°’ë§Œ ì„ íƒ
                valid_depth = depth[depth > 0]
                
                if len(valid_depth) == 0:
                    skipped += 1
                    continue
                
                # ì´ë¯¸ì§€ í¬ê¸°
                img = Image.open(image_path)
                width, height = img.size
                
                # ê¹Šì´ í†µê³„ ê³„ì‚°
                mean_depth = float(np.mean(valid_depth))
                median_depth = float(np.median(valid_depth))
                min_depth = float(np.min(valid_depth))
                max_depth = float(np.max(valid_depth))
                std_depth = float(np.std(valid_depth))
                p50 = float(np.percentile(valid_depth, 50))
                p90 = float(np.percentile(valid_depth, 90))
                p95 = float(np.percentile(valid_depth, 95))
                
                # Scene íƒ€ì… ì¶”ì • (í‰ê·  ê¹Šì´ ê¸°ë°˜)
                if mean_depth < 5.0:
                    scene_type = 'indoor'
                elif mean_depth < 15.0:
                    scene_type = 'outdoor_near'
                else:
                    scene_type = 'outdoor_far'
                
                metadata.append({
                    'dataset_root': dataset_root,
                    'filename': stem,
                    'image_path': str(image_path.relative_to(self.dataset_root)),
                    'depth_path': str(depth_path.relative_to(self.dataset_root)),
                    'depth_variant': depth_variant,
                    'mean_depth': mean_depth,
                    'median_depth': median_depth,
                    'min_depth': min_depth,
                    'max_depth': max_depth,
                    'std_depth': std_depth,
                    'p50': p50,
                    'p90': p90,
                    'p95': p95,
                    'width': width,
                    'height': height,
                    'scene_type': scene_type,
                    'valid_pixels': len(valid_depth),
                    'total_pixels': depth.size,
                })
                
            except Exception as e:
                print(f"Error processing {stem}: {e}")
                skipped += 1
                continue
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(metadata)
        
        # í†µê³„ ì¶œë ¥
        print("\n" + "="*60)
        print("NCDB ë°ì´í„°ì…‹ ê¹Šì´ ë¶„í¬ í†µê³„")
        print("="*60)
        print(f"ì´ ìƒ˜í”Œ ìˆ˜: {len(df)}")
        print(f"ìŠ¤í‚µëœ ìƒ˜í”Œ: {skipped}")
        print(f"\ní‰ê·  ê¹Šì´ í†µê³„:")
        print(f"  Mean: {df['mean_depth'].mean():.2f}m (std: {df['mean_depth'].std():.2f}m)")
        print(f"  Median: {df['median_depth'].median():.2f}m")
        print(f"  Range: [{df['min_depth'].min():.2f}m, {df['max_depth'].max():.2f}m]")
        print(f"\nScene íƒ€ì… ë¶„í¬:")
        print(df['scene_type'].value_counts())
        print(f"\nDepth Variant ì‚¬ìš© ë¶„í¬:")
        print(df['depth_variant'].value_counts())
        print("="*60)
        
        return df

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Generate NCDB metadata for calibration')
    parser.add_argument('--dataset_root', type=str, required=True,
                        help='NCDB ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ (ì˜ˆ: /data/ncdb)')
    parser.add_argument('--split_file', type=str, default='train_split.json',
                        help='Split íŒŒì¼ëª… (ê¸°ë³¸: train_split.json)')
    parser.add_argument('--output', type=str, default='ncdb_metadata.csv',
                        help='ì¶œë ¥ CSV íŒŒì¼ëª… (ê¸°ë³¸: ncdb_metadata.csv)')
    
    args = parser.parse_args()
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = NcdbMetadataGenerator(args.dataset_root)
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata_df = generator.analyze_split(args.split_file)
    
    # CSV ì €ì¥
    metadata_df.to_csv(args.output, index=False)
    print(f"\nâœ… ë©”íƒ€ë°ì´í„°ê°€ '{args.output}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

### 1.2. ì‹¤í–‰

```bash
# NCDB ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½
python create_ncdb_metadata.py \
    --dataset_root /data/ncdb \
    --split_file train_split.json \
    --output ncdb_train_metadata.csv
```

### 1.3. ê²°ê³¼ í™•ì¸

```bash
# ìƒì„±ëœ CSV íŒŒì¼ í™•ì¸
head -n 5 ncdb_train_metadata.csv
wc -l ncdb_train_metadata.csv
```

---

## Step 2: Calibration Dataset ì„ ë³„ (Stratified Sampling)

### 2.1. ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

`create_calibration_split.py` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

```python
# create_calibration_split.py
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

class CalibrationDatasetCreator:
    """Representative Calibration Dataset ìƒì„±ê¸°"""
    
    def __init__(self, metadata_path):
        self.df = pd.read_csv(metadata_path)
        print(f"ì´ {len(self.df)}ê°œì˜ ìƒ˜í”Œì´ ë©”íƒ€ë°ì´í„°ì— ìˆìŠµë‹ˆë‹¤.")
    
    def create_stratified_split(self, target_size=300, output_file='calibration_split.json',
                                depth_bins=None, sampling_ratios=None):
        """
        Depth ë¶„í¬ì— ê¸°ë°˜í•˜ì—¬ ê³„ì¸µí™”ëœ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # ê¸°ë³¸ê°’ ì„¤ì •
        if depth_bins is None:
            depth_bins = [0, 3, 8, 15, 100]
        
        if sampling_ratios is None:
            # ê·¼ê±°ë¦¬(25%), ì¤‘ê±°ë¦¬(40%), ì›ê±°ë¦¬(25%), ì´ˆì›ê±°ë¦¬(10%)
            sampling_ratios = [0.25, 0.40, 0.25, 0.10]
        
        # êµ¬ê°„ ë¼ë²¨
        labels = ['near', 'mid', 'far', 'very_far'][:len(depth_bins)-1]
        
        # Depth ë²”ìœ„ë³„ë¡œ ë¶„ë¥˜
        self.df['depth_range'] = pd.cut(
            self.df['mean_depth'], 
            bins=depth_bins, 
            labels=labels, 
            right=True
        )
        
        # ê° ë²”ìœ„ë³„ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        print("\n" + "="*60)
        print("Depth Range ë¶„í¬")
        print("="*60)
        range_counts = self.df['depth_range'].value_counts(sort=False)
        print(range_counts)
        print("\në¹„ìœ¨:")
        print(self.df['depth_range'].value_counts(normalize=True, sort=False))
        
        # ê° êµ¬ê°„ë³„ ìƒ˜í”Œë§ í¬ê¸° ê²°ì •
        sampled_dfs = []
        total_sampled = 0
        
        print("\n" + "="*60)
        print("ìƒ˜í”Œë§ ê³„íš")
        print("="*60)
        
        for i, label in enumerate(labels):
            available = range_counts.get(label, 0)
            target = int(target_size * sampling_ratios[i])
            actual = min(target, available)
            
            if actual > 0:
                samples = self.df[self.df['depth_range'] == label].sample(
                    n=actual, replace=False, random_state=42
                )
                sampled_dfs.append(samples)
                total_sampled += actual
                print(f"{label:10s} ({depth_bins[i]:>5.1f}-{depth_bins[i+1]:>5.1f}m): "
                      f"ëª©í‘œ {target:3d}, ì‹¤ì œ {actual:3d} (ê°€ìš© {available:3d})")
        
        # ëª©í‘œ í¬ê¸°ì— ë¯¸ë‹¬í•˜ë©´ ê°€ì¥ ë§ì€ ë²”ìœ„ì—ì„œ ì¶”ê°€ ìƒ˜í”Œë§
        if total_sampled < target_size:
            shortage = target_size - total_sampled
            mid_available = range_counts.get('mid', 0) - int(target_size * sampling_ratios[1])
            if mid_available > 0:
                additional = min(shortage, mid_available)
                already_sampled = sampled_dfs[1] if len(sampled_dfs) > 1 else pd.DataFrame()
                mid_pool = self.df[self.df['depth_range'] == 'mid']
                mid_pool = mid_pool[~mid_pool.index.isin(already_sampled.index)]
                
                if len(mid_pool) >= additional:
                    extra_samples = mid_pool.sample(n=additional, replace=False, random_state=42)
                    sampled_dfs.append(extra_samples)
                    total_sampled += additional
                    print(f"\nì¤‘ê±°ë¦¬ì—ì„œ {additional}ê°œ ì¶”ê°€ ìƒ˜í”Œë§")
        
        print(f"\nì´ ìƒ˜í”Œë§: {total_sampled}ê°œ")
        print("="*60)
        
        # ìµœì¢… ë°ì´í„°ì…‹ ë³‘í•©
        representative_df = pd.concat(sampled_dfs, ignore_index=True)
        
        # JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        calibration_data = []
        for _, row in representative_df.iterrows():
            calibration_data.append({
                'dataset_root': row['dataset_root'],
                'new_filename': row['filename']
            })
        
        # JSON ì €ì¥
        with open(output_file, 'w') as f:
            json.dump(calibration_data, f, indent=2)
        
        print(f"\nâœ… '{output_file}' ìƒì„± ì™„ë£Œ ({len(calibration_data)}ê°œ ìƒ˜í”Œ)")
        
        # ì‹œê°í™”
        self.visualize_distribution(self.df, representative_df, output_file)
        
        return representative_df
    
    def visualize_distribution(self, original_df, sampled_df, output_file):
        """ì›ë³¸ê³¼ ìƒ˜í”Œë§ëœ ë°ì´í„°ì…‹ì˜ ë¶„í¬ ë¹„êµ"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. ì›ë³¸ ë¶„í¬ - íˆìŠ¤í† ê·¸ë¨
        axes[0, 0].hist(original_df['mean_depth'], bins=50, alpha=0.7, 
                        color='blue', edgecolor='black')
        axes[0, 0].set_title(f'Original Dataset (n={len(original_df)})')
        axes[0, 0].set_xlabel('Mean Depth (m)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ìƒ˜í”Œë§ëœ ë¶„í¬ - íˆìŠ¤í† ê·¸ë¨
        axes[0, 1].hist(sampled_df['mean_depth'], bins=50, alpha=0.7, 
                        color='green', edgecolor='black')
        axes[0, 1].set_title(f'Calibration Dataset (n={len(sampled_df)})')
        axes[0, 1].set_xlabel('Mean Depth (m)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. Depth Rangeë³„ ë¹„êµ
        range_labels = ['near\n(0-3m)', 'mid\n(3-8m)', 'far\n(8-15m)', 'very_far\n(15m+)']
        original_counts = original_df['depth_range'].value_counts(sort=False)
        sampled_counts = sampled_df['depth_range'].value_counts(sort=False)
        
        x = np.arange(len(range_labels))
        width = 0.35
        
        axes[1, 0].bar(x - width/2, original_counts.values, width, 
                       label='Original', alpha=0.7, color='blue')
        axes[1, 0].bar(x + width/2, sampled_counts.values, width, 
                       label='Calibration', alpha=0.7, color='green')
        axes[1, 0].set_xlabel('Depth Range')
        axes[1, 0].set_ylabel('Count')
        axes[1, 0].set_title('Depth Range Distribution Comparison')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(range_labels)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3, axis='y')
        
        # 4. Scene Typeë³„ ë¹„êµ
        if 'scene_type' in original_df.columns and 'scene_type' in sampled_df.columns:
            scene_orig = original_df['scene_type'].value_counts()
            scene_samp = sampled_df['scene_type'].value_counts()
            
            scene_labels = list(set(scene_orig.index) | set(scene_samp.index))
            x_scene = np.arange(len(scene_labels))
            
            orig_vals = [scene_orig.get(label, 0) for label in scene_labels]
            samp_vals = [scene_samp.get(label, 0) for label in scene_labels]
            
            axes[1, 1].bar(x_scene - width/2, orig_vals, width, 
                           label='Original', alpha=0.7, color='blue')
            axes[1, 1].bar(x_scene + width/2, samp_vals, width, 
                           label='Calibration', alpha=0.7, color='green')
            axes[1, 1].set_xlabel('Scene Type')
            axes[1, 1].set_ylabel('Count')
            axes[1, 1].set_title('Scene Type Distribution Comparison')
            axes[1, 1].set_xticks(x_scene)
            axes[1, 1].set_xticklabels(scene_labels, rotation=15)
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plot_file = output_file.replace('.json', '_distribution.png')
        plt.savefig(plot_file, dpi=150)
        print(f"âœ… ë¶„í¬ ë¹„êµ ê·¸ë˜í”„ê°€ '{plot_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        plt.close()

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Create calibration split from metadata')
    parser.add_argument('--metadata', type=str, required=True,
                        help='ë©”íƒ€ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--target_size', type=int, default=300,
                        help='ëª©í‘œ ìƒ˜í”Œ ê°œìˆ˜ (ê¸°ë³¸: 300)')
    parser.add_argument('--output', type=str, default='calibration_split.json',
                        help='ì¶œë ¥ JSON íŒŒì¼ëª…')
    
    args = parser.parse_args()
    
    # Calibration Dataset ìƒì„±ê¸° ì´ˆê¸°í™”
    creator = CalibrationDatasetCreator(args.metadata)
    
    # Stratified Sampling ìˆ˜í–‰
    creator.create_stratified_split(
        target_size=args.target_size,
        output_file=args.output
    )
```

### 2.2. ì‹¤í–‰

```bash
python create_calibration_split.py \
    --metadata ncdb_train_metadata.csv \
    --target_size 300 \
    --output calibration_split.json
```

### 2.3. ê²°ê³¼ í™•ì¸

```bash
# JSON íŒŒì¼ í™•ì¸
python -c "import json; data=json.load(open('calibration_split.json')); print(f'ì´ {len(data)}ê°œ ìƒ˜í”Œ')"

# ë¶„í¬ ê·¸ë˜í”„ í™•ì¸
ls -lh calibration_split_distribution.png
```

---

## Step 3: â­ Calibration ì´ë¯¸ì§€ ë³µì‚¬ (í•µì‹¬!)

### 3.1. ì´ë¯¸ì§€ ë³µì‚¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

`copy_calibration_images.py` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

```python
# copy_calibration_images.py
import json
import shutil
from pathlib import Path
from tqdm import tqdm

def copy_calibration_images(
    dataset_root,
    calibration_split_json,
    output_dir='calibration_images'
):
    """
    calibration_split.jsonì— ì§€ì •ëœ ì´ë¯¸ì§€ë“¤ì„ output_dirë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.
    
    Args:
        dataset_root: NCDB ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ
        calibration_split_json: Calibration split JSON íŒŒì¼ ê²½ë¡œ
        output_dir: ì´ë¯¸ì§€ë¥¼ ë³µì‚¬í•  ì¶œë ¥ ë””ë ‰í† ë¦¬
    
    Returns:
        ë³µì‚¬ëœ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    dataset_root = Path(dataset_root)
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # JSON íŒŒì¼ ë¡œë“œ
    with open(calibration_split_json, 'r') as f:
        split_data = json.load(f)
    
    print("\n" + "="*60)
    print(f"Calibration ì´ë¯¸ì§€ ë³µì‚¬")
    print("="*60)
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir.absolute()}")
    print(f"ì´ {len(split_data)}ê°œ ì´ë¯¸ì§€ ë³µì‚¬ ì¤‘...")
    
    image_list = []
    copied = 0
    failed = 0
    
    for entry in tqdm(split_data):
        base_dir = dataset_root / entry['dataset_root']
        stem = entry['new_filename']
        
        src_path = base_dir / 'image_a6' / f"{stem}.png"
        
        # íŒŒì¼ëª… ì¶©ëŒ ë°©ì§€: dataset_root ê²½ë¡œë¥¼ íŒŒì¼ëª…ì— í¬í•¨
        # ì˜ˆ: synced_data/scene_001/frame_0001 -> scene_001_frame_0001.png
        safe_name = entry['dataset_root'].replace('/', '_').replace('synced_data_', '')
        dst_filename = f"{safe_name}_{stem}.png"
        dst_path = output_dir / dst_filename
        
        if src_path.exists():
            try:
                shutil.copy2(src_path, dst_path)
                image_list.append({
                    'original_path': str(src_path),
                    'copied_path': str(dst_path),
                    'filename': dst_filename
                })
                copied += 1
            except Exception as e:
                print(f"\nâš ï¸ ë³µì‚¬ ì‹¤íŒ¨: {src_path} -> {e}")
                failed += 1
        else:
            print(f"\nâš ï¸ íŒŒì¼ ì—†ìŒ: {src_path}")
            failed += 1
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì €ì¥ (ì ˆëŒ€ ê²½ë¡œ)
    list_file = output_dir / 'image_list.txt'
    with open(list_file, 'w') as f:
        for img in image_list:
            f.write(f"{Path(img['copied_path']).absolute()}\n")
    
    # ê°„ë‹¨í•œ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ë„ ì €ì¥ (ìƒëŒ€ ê²½ë¡œ)
    simple_list_file = output_dir / 'image_filenames.txt'
    with open(simple_list_file, 'w') as f:
        for img in image_list:
            f.write(f"{img['filename']}\n")
    
    # ë©”íƒ€ë°ì´í„° JSON ì €ì¥
    meta_file = output_dir / 'calibration_metadata.json'
    with open(meta_file, 'w') as f:
        json.dump(image_list, f, indent=2)
    
    print("\n" + "="*60)
    print("ë³µì‚¬ ì™„ë£Œ!")
    print("="*60)
    print(f"âœ… ì„±ê³µ: {copied}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
    print(f"\nìƒì„±ëœ íŒŒì¼:")
    print(f"  - ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {output_dir.absolute()}")
    print(f"  - ì ˆëŒ€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸: {list_file}")
    print(f"  - íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸: {simple_list_file}")
    print(f"  - ë©”íƒ€ë°ì´í„°: {meta_file}")
    print("="*60)
    
    return image_list

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Copy calibration images to a separate folder'
    )
    parser.add_argument('--dataset_root', type=str, required=True,
                        help='NCDB ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ')
    parser.add_argument('--calibration_split', type=str, 
                        default='calibration_split.json',
                        help='Calibration split JSON íŒŒì¼')
    parser.add_argument('--output_dir', type=str, 
                        default='calibration_images',
                        help='ì´ë¯¸ì§€ë¥¼ ë³µì‚¬í•  ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤í–‰
    copy_calibration_images(
        dataset_root=args.dataset_root,
        calibration_split_json=args.calibration_split,
        output_dir=args.output_dir
    )
```

### 3.2. ì‹¤í–‰

```bash
# Calibration ì´ë¯¸ì§€ë¥¼ ë³„ë„ í´ë”ë¡œ ë³µì‚¬
python copy_calibration_images.py \
    --dataset_root /data/ncdb \
    --calibration_split calibration_split.json \
    --output_dir calibration_images
```

### 3.3. ê²°ê³¼ í™•ì¸

```bash
# ë³µì‚¬ëœ ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸
ls calibration_images/*.png | wc -l

# ì²˜ìŒ 5ê°œ íŒŒì¼ í™•ì¸
ls calibration_images/ | head -n 5

# ë””ë ‰í† ë¦¬ í¬ê¸° í™•ì¸
du -sh calibration_images/

# ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ í™•ì¸
head -n 5 calibration_images/image_list.txt
```

**ìƒì„±ë˜ëŠ” íŒŒì¼ êµ¬ì¡°**:
```
calibration_images/
â”œâ”€â”€ scene_001_frame_0001.png
â”œâ”€â”€ scene_001_frame_0145.png
â”œâ”€â”€ scene_003_frame_0032.png
â”œâ”€â”€ ...
â”œâ”€â”€ image_list.txt              # ì ˆëŒ€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
â”œâ”€â”€ image_filenames.txt         # íŒŒì¼ëª…ë§Œ ë¦¬ìŠ¤íŠ¸
â””â”€â”€ calibration_metadata.json   # ìƒì„¸ ë©”íƒ€ë°ì´í„°
```

---

## Step 3.5: ğŸ“Š Calibration Dataset ë¶„ì„ ë° ì‹œê°í™” (ì„ íƒì )

### 3.5.1. ë¶„ì„ ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

ë³µì‚¬ëœ Calibration ì´ë¯¸ì§€ë“¤ì˜ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

`analyze_calibration_dataset.py` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

```python
# analyze_calibration_dataset.py
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from pathlib import Path
from PIL import Image
from collections import Counter

def analyze_calibration_dataset(
    calibration_metadata_json='calibration_images/calibration_metadata.json',
    ncdb_metadata_csv='ncdb_train_metadata.csv',
    output_dir='calibration_analysis'
):
    """
    Calibration ë°ì´í„°ì…‹ì˜ ìƒì„¸ ë¶„ì„ ë° ì‹œê°í™”
    
    Args:
        calibration_metadata_json: ë³µì‚¬ëœ ì´ë¯¸ì§€ì˜ ë©”íƒ€ë°ì´í„° JSON
        ncdb_metadata_csv: ì „ì²´ NCDB ë©”íƒ€ë°ì´í„° CSV
        output_dir: ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # 1. ë©”íƒ€ë°ì´í„° ë¡œë“œ
    print("\n" + "="*70)
    print("Calibration Dataset ë¶„ì„")
    print("="*70)
    
    with open(calibration_metadata_json, 'r') as f:
        calib_meta = json.load(f)
    
    full_meta = pd.read_csv(ncdb_metadata_csv)
    
    # ë³µì‚¬ëœ ì´ë¯¸ì§€ë“¤ì˜ filename ì¶”ì¶œ
    calib_filenames = set()
    for item in calib_meta:
        # original_pathì—ì„œ filename ì¶”ì¶œ
        # ì˜ˆ: /data/ncdb/synced_data/scene_001/image_a6/frame_0001.png
        original_path = Path(item['original_path'])
        stem = original_path.stem
        calib_filenames.add(stem)
    
    # ì „ì²´ ë©”íƒ€ë°ì´í„°ì—ì„œ calibrationì— ì‚¬ìš©ëœ ìƒ˜í”Œë§Œ í•„í„°ë§
    calib_df = full_meta[full_meta['filename'].isin(calib_filenames)].copy()
    
    print(f"\nâœ… ë¡œë“œ ì™„ë£Œ:")
    print(f"  - ì „ì²´ NCDB ìƒ˜í”Œ: {len(full_meta)}ê°œ")
    print(f"  - Calibration ìƒ˜í”Œ: {len(calib_df)}ê°œ")
    print(f"  - ë³µì‚¬ëœ ì´ë¯¸ì§€: {len(calib_meta)}ê°œ")
    
    # 2. ê¸°ë³¸ í†µê³„ ê³„ì‚°
    print("\n" + "="*70)
    print("ê¹Šì´ ë¶„í¬ í†µê³„")
    print("="*70)
    
    stats = {
        'mean': calib_df['mean_depth'].mean(),
        'median': calib_df['median_depth'].median(),
        'std': calib_df['mean_depth'].std(),
        'min': calib_df['min_depth'].min(),
        'max': calib_df['max_depth'].max(),
        'p25': calib_df['mean_depth'].quantile(0.25),
        'p50': calib_df['mean_depth'].quantile(0.50),
        'p75': calib_df['mean_depth'].quantile(0.75),
        'p90': calib_df['mean_depth'].quantile(0.90),
        'p95': calib_df['mean_depth'].quantile(0.95),
    }
    
    print(f"í‰ê·  ê¹Šì´: {stats['mean']:.2f}m (Â± {stats['std']:.2f}m)")
    print(f"ì¤‘ì•™ê°’: {stats['median']:.2f}m")
    print(f"ë²”ìœ„: [{stats['min']:.2f}m, {stats['max']:.2f}m]")
    print(f"ë°±ë¶„ìœ„: p25={stats['p25']:.2f}m, p50={stats['p50']:.2f}m, "
          f"p75={stats['p75']:.2f}m, p90={stats['p90']:.2f}m, p95={stats['p95']:.2f}m")
    
    # Scene type ë¶„í¬
    print(f"\nScene íƒ€ì… ë¶„í¬:")
    scene_counts = calib_df['scene_type'].value_counts()
    for scene, count in scene_counts.items():
        pct = count / len(calib_df) * 100
        print(f"  {scene:15s}: {count:3d}ê°œ ({pct:5.1f}%)")
    
    # Depth variant ë¶„í¬
    print(f"\nDepth Variant ë¶„í¬:")
    variant_counts = calib_df['depth_variant'].value_counts()
    for variant, count in variant_counts.items():
        pct = count / len(calib_df) * 100
        print(f"  {variant:30s}: {count:3d}ê°œ ({pct:5.1f}%)")
    
    # 3. ìƒì„¸ ì‹œê°í™”
    print("\n" + "="*70)
    print("ì‹œê°í™” ìƒì„± ì¤‘...")
    print("="*70)
    
    create_comprehensive_visualization(calib_df, full_meta, output_dir)
    create_depth_analysis_visualization(calib_df, output_dir)
    create_image_samples_grid(calib_meta, output_dir)
    
    # 4. í†µê³„ ë¦¬í¬íŠ¸ ì €ì¥
    report_path = output_dir / 'calibration_statistics.json'
    report = {
        'total_samples': len(calib_df),
        'depth_statistics': stats,
        'scene_type_distribution': scene_counts.to_dict(),
        'depth_variant_distribution': variant_counts.to_dict(),
    }
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ '{output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*70)

def create_comprehensive_visualization(calib_df, full_meta, output_dir):
    """ì¢…í•© ë¶„ì„ ì‹œê°í™” (6ê°œ ì„œë¸Œí”Œë¡¯)"""
    fig = plt.figure(figsize=(18, 12))
    gs = gridspec.GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)
    
    # 1. ê¹Šì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (ì „ì²´ vs calibration)
    ax1 = fig.add_subplot(gs[0, :2])
    ax1.hist(full_meta['mean_depth'], bins=60, alpha=0.5, 
             label=f'Full Dataset (n={len(full_meta)})', color='blue', edgecolor='black')
    ax1.hist(calib_df['mean_depth'], bins=60, alpha=0.7, 
             label=f'Calibration (n={len(calib_df)})', color='green', edgecolor='black')
    ax1.set_xlabel('Mean Depth (m)', fontsize=11)
    ax1.set_ylabel('Frequency', fontsize=11)
    ax1.set_title('Depth Distribution Comparison', fontsize=13, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # 2. ê¹Šì´ ë°•ìŠ¤í”Œë¡¯
    ax2 = fig.add_subplot(gs[0, 2])
    box_data = [full_meta['mean_depth'], calib_df['mean_depth']]
    bp = ax2.boxplot(box_data, labels=['Full', 'Calib'], patch_artist=True)
    bp['boxes'][0].set_facecolor('lightblue')
    bp['boxes'][1].set_facecolor('lightgreen')
    ax2.set_ylabel('Mean Depth (m)', fontsize=11)
    ax2.set_title('Depth Boxplot', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    
    # 3. Scene Type ë¶„í¬
    ax3 = fig.add_subplot(gs[1, 0])
    scene_counts = calib_df['scene_type'].value_counts()
    colors_scene = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    wedges, texts, autotexts = ax3.pie(
        scene_counts.values, 
        labels=scene_counts.index,
        autopct='%1.1f%%',
        startangle=90,
        colors=colors_scene[:len(scene_counts)]
    )
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(10)
    ax3.set_title('Scene Type Distribution', fontsize=13, fontweight='bold')
    
    # 4. Depth Variant ë¶„í¬
    ax4 = fig.add_subplot(gs[1, 1])
    variant_counts = calib_df['depth_variant'].value_counts()
    variant_labels = [v.replace('_', '\n') for v in variant_counts.index]
    bars = ax4.bar(range(len(variant_counts)), variant_counts.values, 
                   color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E1D3'][:len(variant_counts)])
    ax4.set_xticks(range(len(variant_counts)))
    ax4.set_xticklabels(variant_labels, fontsize=9, rotation=0)
    ax4.set_ylabel('Count', fontsize=11)
    ax4.set_title('Depth Variant Distribution', fontsize=13, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='y')
    
    # ê° ë°” ìœ„ì— ê°œìˆ˜ í‘œì‹œ
    for i, (bar, count) in enumerate(zip(bars, variant_counts.values)):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(count)}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # 5. Depth Range ë¶„í¬ (4ê°œ êµ¬ê°„)
    ax5 = fig.add_subplot(gs[1, 2])
    bins = [0, 3, 8, 15, 100]
    labels = ['Near\n(0-3m)', 'Mid\n(3-8m)', 'Far\n(8-15m)', 'Very Far\n(15m+)']
    calib_df['depth_range'] = pd.cut(calib_df['mean_depth'], bins=bins, labels=labels)
    range_counts = calib_df['depth_range'].value_counts(sort=False)
    
    colors_range = ['#FF6B6B', '#FFD93D', '#6BCB77', '#4D96FF']
    bars2 = ax5.bar(range(len(range_counts)), range_counts.values, color=colors_range)
    ax5.set_xticks(range(len(range_counts)))
    ax5.set_xticklabels(labels, fontsize=9)
    ax5.set_ylabel('Count', fontsize=11)
    ax5.set_title('Depth Range Distribution', fontsize=13, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    
    for bar, count in zip(bars2, range_counts.values):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(count)}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # 6. ê¹Šì´ í†µê³„ ëˆ„ì  ë¶„í¬ (CDF)
    ax6 = fig.add_subplot(gs[2, :])
    full_sorted = np.sort(full_meta['mean_depth'])
    calib_sorted = np.sort(calib_df['mean_depth'])
    full_cdf = np.arange(1, len(full_sorted) + 1) / len(full_sorted)
    calib_cdf = np.arange(1, len(calib_sorted) + 1) / len(calib_sorted)
    
    ax6.plot(full_sorted, full_cdf, label='Full Dataset', color='blue', linewidth=2, alpha=0.6)
    ax6.plot(calib_sorted, calib_cdf, label='Calibration', color='green', linewidth=2)
    ax6.set_xlabel('Mean Depth (m)', fontsize=11)
    ax6.set_ylabel('Cumulative Probability', fontsize=11)
    ax6.set_title('Cumulative Distribution Function (CDF)', fontsize=13, fontweight='bold')
    ax6.legend(fontsize=10)
    ax6.grid(True, alpha=0.3)
    ax6.set_xlim(0, 30)  # 30mê¹Œì§€ë§Œ í‘œì‹œ
    
    plt.savefig(output_dir / 'calibration_comprehensive_analysis.png', dpi=150, bbox_inches='tight')
    print(f"  âœ… ì¢…í•© ë¶„ì„: calibration_comprehensive_analysis.png")
    plt.close()

def create_depth_analysis_visualization(calib_df, output_dir):
    """ê¹Šì´ ë¶„ì„ ìƒì„¸ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. Mean vs Median Depth
    axes[0, 0].scatter(calib_df['mean_depth'], calib_df['median_depth'], 
                       alpha=0.5, s=30, c='green', edgecolors='black', linewidth=0.5)
    axes[0, 0].plot([0, 30], [0, 30], 'r--', linewidth=2, label='y=x')
    axes[0, 0].set_xlabel('Mean Depth (m)', fontsize=11)
    axes[0, 0].set_ylabel('Median Depth (m)', fontsize=11)
    axes[0, 0].set_title('Mean vs Median Depth', fontsize=12, fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Depth Standard Deviation
    axes[0, 1].hist(calib_df['std_depth'], bins=30, color='orange', 
                    edgecolor='black', alpha=0.7)
    axes[0, 1].set_xlabel('Depth Std Dev (m)', fontsize=11)
    axes[0, 1].set_ylabel('Frequency', fontsize=11)
    axes[0, 1].set_title('Depth Variation Distribution', fontsize=12, fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Valid Pixels Ratio
    calib_df['valid_ratio'] = calib_df['valid_pixels'] / calib_df['total_pixels']
    axes[1, 0].hist(calib_df['valid_ratio'] * 100, bins=30, color='skyblue',
                    edgecolor='black', alpha=0.7)
    axes[1, 0].set_xlabel('Valid Depth Pixels (%)', fontsize=11)
    axes[1, 0].set_ylabel('Frequency', fontsize=11)
    axes[1, 0].set_title('Depth Coverage Distribution', fontsize=12, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Scene Typeë³„ í‰ê·  ê¹Šì´
    scene_depth = calib_df.groupby('scene_type')['mean_depth'].agg(['mean', 'std'])
    x_pos = np.arange(len(scene_depth))
    bars = axes[1, 1].bar(x_pos, scene_depth['mean'], 
                          yerr=scene_depth['std'], capsize=5,
                          color=['#FF6B6B', '#4ECDC4', '#45B7D1'][:len(scene_depth)],
                          alpha=0.7, edgecolor='black')
    axes[1, 1].set_xticks(x_pos)
    axes[1, 1].set_xticklabels(scene_depth.index, rotation=15)
    axes[1, 1].set_ylabel('Mean Depth (m)', fontsize=11)
    axes[1, 1].set_title('Average Depth by Scene Type', fontsize=12, fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    # ë°” ìœ„ì— í‰ê· ê°’ í‘œì‹œ
    for bar, mean_val in zip(bars, scene_depth['mean']):
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,
                       f'{mean_val:.1f}m',
                       ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'calibration_depth_analysis.png', dpi=150, bbox_inches='tight')
    print(f"  âœ… ê¹Šì´ ë¶„ì„: calibration_depth_analysis.png")
    plt.close()

def create_image_samples_grid(calib_meta, output_dir, n_samples=12):
    """ìƒ˜í”Œ ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ í‘œì‹œ (ëœë¤í•˜ê²Œ ì„ íƒ)"""
    import random
    
    # ëœë¤í•˜ê²Œ n_samplesê°œ ì„ íƒ
    if len(calib_meta) > n_samples:
        samples = random.sample(calib_meta, n_samples)
    else:
        samples = calib_meta
        n_samples = len(samples)
    
    # ê·¸ë¦¬ë“œ í¬ê¸° ê²°ì •
    n_cols = 4
    n_rows = (n_samples + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))
    axes = axes.flatten() if n_samples > 1 else [axes]
    
    for i, sample in enumerate(samples):
        try:
            img_path = sample['copied_path']
            img = Image.open(img_path)
            
            # ì´ë¯¸ì§€ í‘œì‹œ
            axes[i].imshow(img)
            axes[i].axis('off')
            
            # íŒŒì¼ëª… í‘œì‹œ
            filename = sample['filename']
            # íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ì¤„ì„
            if len(filename) > 30:
                filename = filename[:27] + '...'
            axes[i].set_title(filename, fontsize=8)
            
        except Exception as e:
            axes[i].text(0.5, 0.5, f'Error loading\n{sample["filename"]}',
                        ha='center', va='center', fontsize=8)
            axes[i].axis('off')
    
    # ë‚¨ì€ ë¹ˆ ì„œë¸Œí”Œë¡¯ ìˆ¨ê¸°ê¸°
    for i in range(n_samples, len(axes)):
        axes[i].axis('off')
    
    plt.suptitle(f'Calibration Dataset Sample Images (Random {n_samples})', 
                 fontsize=14, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.savefig(output_dir / 'calibration_sample_images.png', dpi=120, bbox_inches='tight')
    print(f"  âœ… ìƒ˜í”Œ ì´ë¯¸ì§€: calibration_sample_images.png")
    plt.close()

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Analyze and visualize calibration dataset'
    )
    parser.add_argument('--calibration_meta', type=str,
                        default='calibration_images/calibration_metadata.json',
                        help='Calibration ë©”íƒ€ë°ì´í„° JSON íŒŒì¼')
    parser.add_argument('--ncdb_meta', type=str,
                        default='ncdb_train_metadata.csv',
                        help='NCDB ì „ì²´ ë©”íƒ€ë°ì´í„° CSV íŒŒì¼')
    parser.add_argument('--output_dir', type=str,
                        default='calibration_analysis',
                        help='ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # ë¶„ì„ ì‹¤í–‰
    analyze_calibration_dataset(
        calibration_metadata_json=args.calibration_meta,
        ncdb_metadata_csv=args.ncdb_meta,
        output_dir=args.output_dir
    )
```

### 3.5.2. ì‹¤í–‰

```bash
# Calibration ë°ì´í„°ì…‹ ë¶„ì„ ë° ì‹œê°í™”
python analyze_calibration_dataset.py \
    --calibration_meta calibration_images/calibration_metadata.json \
    --ncdb_meta ncdb_train_metadata.csv \
    --output_dir calibration_analysis
```

### 3.5.3. ìƒì„±ë˜ëŠ” ì‹œê°í™” ê²°ê³¼

ì‹¤í–‰ í›„ `calibration_analysis/` ë””ë ‰í† ë¦¬ì— ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

**1. `calibration_comprehensive_analysis.png`** (ì¢…í•© ë¶„ì„)
   - ê¹Šì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (ì „ì²´ vs calibration)
   - ê¹Šì´ ë°•ìŠ¤í”Œë¡¯
   - Scene Type ë¶„í¬ (íŒŒì´ ì°¨íŠ¸)
   - Depth Variant ë¶„í¬ (ë°” ì°¨íŠ¸)
   - Depth Range ë¶„í¬ (4ê°œ êµ¬ê°„)
   - ëˆ„ì  ë¶„í¬ í•¨ìˆ˜ (CDF)

**2. `calibration_depth_analysis.png`** (ê¹Šì´ ìƒì„¸ ë¶„ì„)
   - Mean vs Median Depth (ì‚°ì ë„)
   - Depth í‘œì¤€í¸ì°¨ ë¶„í¬
   - Valid Pixels ë¹„ìœ¨ ë¶„í¬
   - Scene Typeë³„ í‰ê·  ê¹Šì´ (ì—ëŸ¬ë°” í¬í•¨)

**3. `calibration_sample_images.png`** (ìƒ˜í”Œ ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ)
   - ëœë¤í•˜ê²Œ ì„ íƒëœ 12ê°œ ì´ë¯¸ì§€ í‘œì‹œ
   - ê° ì´ë¯¸ì§€ì˜ íŒŒì¼ëª… í‘œì‹œ

**4. `calibration_statistics.json`** (í†µê³„ ë¦¬í¬íŠ¸)
   - JSON í˜•ì‹ì˜ ìƒì„¸ í†µê³„ ë°ì´í„°

### 3.5.4. ê²°ê³¼ í™•ì¸

```bash
# ìƒì„±ëœ íŒŒì¼ í™•ì¸
ls -lh calibration_analysis/

# í†µê³„ ë¦¬í¬íŠ¸ í™•ì¸
cat calibration_analysis/calibration_statistics.json | python -m json.tool

# ì´ë¯¸ì§€ ë·°ì–´ë¡œ ì‹œê°í™” ê²°ê³¼ í™•ì¸
# Linux
xdg-open calibration_analysis/calibration_comprehensive_analysis.png

# macOS
open calibration_analysis/calibration_comprehensive_analysis.png

# ë˜ëŠ” VSCodeì—ì„œ ì§ì ‘ ì—´ê¸°
code calibration_analysis/
```

**ì˜ˆìƒ ì¶œë ¥**:
```
======================================================================
Calibration Dataset ë¶„ì„
======================================================================

âœ… ë¡œë“œ ì™„ë£Œ:
  - ì „ì²´ NCDB ìƒ˜í”Œ: 4856ê°œ
  - Calibration ìƒ˜í”Œ: 300ê°œ
  - ë³µì‚¬ëœ ì´ë¯¸ì§€: 300ê°œ

======================================================================
ê¹Šì´ ë¶„í¬ í†µê³„
======================================================================
í‰ê·  ê¹Šì´: 8.45m (Â± 4.23m)
ì¤‘ì•™ê°’: 7.82m
ë²”ìœ„: [0.50m, 98.50m]
ë°±ë¶„ìœ„: p25=5.12m, p50=7.82m, p75=11.34m, p90=14.56m, p95=18.23m

Scene íƒ€ì… ë¶„í¬:
  outdoor_near   : 135ê°œ ( 45.0%)
  indoor         :  98ê°œ ( 32.7%)
  outdoor_far    :  67ê°œ ( 22.3%)

Depth Variant ë¶„í¬:
  newest_depth_maps              : 245ê°œ ( 81.7%)
  newest_synthetic_depth_maps    :  42ê°œ ( 14.0%)
  new_depth_maps                 :  13ê°œ (  4.3%)

======================================================================
ì‹œê°í™” ìƒì„± ì¤‘...
======================================================================
  âœ… ì¢…í•© ë¶„ì„: calibration_comprehensive_analysis.png
  âœ… ê¹Šì´ ë¶„ì„: calibration_depth_analysis.png
  âœ… ìƒ˜í”Œ ì´ë¯¸ì§€: calibration_sample_images.png

âœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ 'calibration_analysis' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.
======================================================================
```

---

## Step 4: NPU PTQ ì‹¤í–‰

### 4.1. ë°©ë²• 1: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì§ì ‘ ì‚¬ìš©

```bash
# NPU íˆ´ì²´ì¸ì´ ë””ë ‰í† ë¦¬ë¥¼ ì§ì ‘ ì½ì„ ìˆ˜ ìˆëŠ” ê²½ìš°
npu_quantize \
    --model /path/to/resnetsan.onnx \
    --output resnetsan_int8.bin \
    --calibration_dir calibration_images/ \
    --num_samples 300
```

### 4.2. ë°©ë²• 2: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì‚¬ìš©

```bash
# NPU íˆ´ì²´ì¸ì´ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ëŠ” ê²½ìš°
npu_quantize \
    --model /path/to/resnetsan.onnx \
    --output resnetsan_int8.bin \
    --calibration_list calibration_images/image_list.txt \
    --num_samples 300
```

### 4.3. ë°©ë²• 3: Python API ì‚¬ìš© (NPU íˆ´ì²´ì¸ì— ë”°ë¼ ë‹¤ë¦„)

```python
# ì˜ˆì‹œ: NPU Python APIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
import npu_toolkit  # ê°€ìƒì˜ NPU íˆ´í‚·

calibration_images = []
with open('calibration_images/image_list.txt', 'r') as f:
    calibration_images = [line.strip() for line in f]

quantizer = npu_toolkit.PTQQuantizer(
    model_path='resnetsan.onnx',
    calibration_images=calibration_images,
    output_path='resnetsan_int8.bin'
)

quantizer.run()
```

---

## Step 5: ì„±ëŠ¥ í‰ê°€

### 5.1. INT8 ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰

```bash
# NCDB validation setìœ¼ë¡œ í‰ê°€
python scripts/infer.py \
    --checkpoint resnetsan_int8.bin \
    --input /data/ncdb/val_split.json \
    --output results_int8_calibrated/
```

### 5.2. Metric ê³„ì‚°

```bash
# Depth estimation metric ê³„ì‚°
python scripts/eval_depth.py \
    --pred_dir results_int8_calibrated/ \
    --gt_split /data/ncdb/val_split.json \
    --output metrics_int8_calibrated.json
```

### 5.3. ê²°ê³¼ ë¹„êµ

```bash
# FP32 vs INT8 (ê¸°ì¡´) vs INT8 (calibrated) ë¹„êµ
python -c "
import json

# FP32 ê²°ê³¼
fp32_metrics = {'abs_rel': 0.0304}

# INT8 (100 samples)
int8_old = {'abs_rel': 0.1133}

# INT8 (300 samples, calibrated)
with open('metrics_int8_calibrated.json', 'r') as f:
    int8_new = json.load(f)

print('='*60)
print('ì„±ëŠ¥ ë¹„êµ')
print('='*60)
print(f'FP32 (baseline)        : abs_rel = {fp32_metrics[\"abs_rel\"]:.4f}')
print(f'INT8 (100 samples)     : abs_rel = {int8_old[\"abs_rel\"]:.4f}')
print(f'INT8 (300 samples, new): abs_rel = {int8_new[\"abs_rel\"]:.4f}')
print(f'')
print(f'ê°œì„ ìœ¨: {(int8_old[\"abs_rel\"] - int8_new[\"abs_rel\"]) / int8_old[\"abs_rel\"] * 100:.1f}%')
print('='*60)
"
```

---

## ğŸ“Š ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¤€ë¹„ ë‹¨ê³„
- [ ] NCDB ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸ (`/data/ncdb`)
- [ ] `train_split.json` íŒŒì¼ ì¡´ì¬ í™•ì¸
- [ ] Python í™˜ê²½ í™•ì¸ (pandas, numpy, PIL, tqdm, matplotlib)

### Step 1: ë©”íƒ€ë°ì´í„° ìƒì„±
- [ ] `create_ncdb_metadata.py` ì‘ì„±
- [ ] ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ
- [ ] `ncdb_train_metadata.csv` ìƒì„± í™•ì¸
- [ ] CSV íŒŒì¼ ë‚´ìš© ê²€ì¦ (ê¹Šì´ í†µê³„, Scene íƒ€ì… ë“±)

### Step 2: Calibration Split ìƒì„±
- [ ] `create_calibration_split.py` ì‘ì„±
- [ ] ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ
- [ ] `calibration_split.json` ìƒì„± í™•ì¸ (300ê°œ ìƒ˜í”Œ)
- [ ] `calibration_split_distribution.png` í™•ì¸ (ë¶„í¬ê°€ ì ì ˆí•œì§€)

### Step 3: ì´ë¯¸ì§€ ë³µì‚¬ â­
- [ ] `copy_calibration_images.py` ì‘ì„±
- [ ] ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ
- [ ] `calibration_images/` ë””ë ‰í† ë¦¬ ìƒì„± í™•ì¸
- [ ] ì´ë¯¸ì§€ 300ê°œ ë³µì‚¬ ì™„ë£Œ í™•ì¸
- [ ] `image_list.txt` ìƒì„± í™•ì¸
- [ ] (ì„ íƒ) `analyze_calibration_dataset.py` ì‹¤í–‰í•˜ì—¬ ì‹œê°í™” ìƒì„±
- [ ] (ì„ íƒ) `calibration_analysis/` ê²°ê³¼ í™•ì¸

### Step 4: NPU PTQ
- [ ] ONNX ëª¨ë¸ ì¤€ë¹„ (`resnetsan.onnx`)
- [ ] NPU íˆ´ì²´ì¸ ì„¤ì¹˜ ë° ì„¤ì • í™•ì¸
- [ ] Calibration ì´ë¯¸ì§€ë¡œ PTQ ì‹¤í–‰
- [ ] INT8 ëª¨ë¸ ìƒì„± í™•ì¸ (`resnetsan_int8.bin`)

### Step 5: í‰ê°€
- [ ] Validation set ì¶”ë¡  ì‹¤í–‰
- [ ] Metric ê³„ì‚° ì™„ë£Œ
- [ ] ì„±ëŠ¥ ê°œì„  í™•ì¸ (`abs_rel < 0.090` ëª©í‘œ)

---

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼

| ë‹¨ê³„ | Calibration ì„¤ì • | abs_rel | ê°œì„ ìœ¨ |
|------|-----------------|---------|--------|
| Baseline | 100 samples (random) | 0.1133 | - |
| **í˜„ì¬ ëª©í‘œ** | **300 samples (stratified)** | **~0.085** | **~25%** |
| í™•ì¥ (ì„ íƒ) | 500 samples (stratified) | ~0.075 | ~34% |

**ì„±ê³µ ê¸°ì¤€**:
- âœ… **abs_rel < 0.090**: Phase 1 ì„±ê³µ! â†’ Phase 2 (Dual-Head) ì§„í–‰ ê°€ëŠ¥
- âš ï¸ **abs_rel 0.090~0.100**: ë°ì´í„°ì…‹ 500ê°œë¡œ í™•ëŒ€ ë˜ëŠ” Weight Normalization ì‹œë„
- âŒ **abs_rel > 0.100**: NPU ìŠ¤í™ ì¬í™•ì¸ í•„ìš”, ì¦‰ì‹œ Phase 2ë¡œ ì§„í–‰ ê³ ë ¤

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: "No module named 'pandas'"

```bash
pip install pandas numpy pillow tqdm matplotlib
```

### ë¬¸ì œ 2: "ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

```bash
# NCDB ë°ì´í„°ì…‹ êµ¬ì¡° í™•ì¸
ls -la /data/ncdb/synced_data/scene_001/image_a6/

# Split íŒŒì¼ ê²½ë¡œ í™•ì¸
cat /data/ncdb/train_split.json | head -n 5
```

### ë¬¸ì œ 3: "ë©”ëª¨ë¦¬ ë¶€ì¡±"

ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ ë°œìƒí•˜ë©´, ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤:

```python
# create_ncdb_metadata.pyì—ì„œ ì²­í¬ ì²˜ë¦¬ ì¶”ê°€
CHUNK_SIZE = 1000
for i in range(0, len(split_data), CHUNK_SIZE):
    chunk = split_data[i:i+CHUNK_SIZE]
    # ì²­í¬ë³„ ì²˜ë¦¬
```

### ë¬¸ì œ 4: "ë³µì‚¬ ì†ë„ê°€ ëŠë¦¼"

í•˜ë“œ ë§í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì‚¬ ëŒ€ì‹  ë§í¬ ìƒì„±:

```python
# copy_calibration_images.pyì—ì„œ shutil.copy2 ëŒ€ì‹ 
os.link(src_path, dst_path)  # í•˜ë“œ ë§í¬ (ê°™ì€ íŒŒì¼ì‹œìŠ¤í…œì¼ ë•Œë§Œ)
# ë˜ëŠ”
os.symlink(src_path, dst_path)  # ì‹¬ë³¼ë¦­ ë§í¬
```

### ë¬¸ì œ 5: "ì‹œê°í™” ìƒì„± ì‹¤íŒ¨"

ì´ë¯¸ì§€ë¥¼ ì—´ ìˆ˜ ì—†ëŠ” ê²½ìš°:

```bash
# PIL ì¬ì„¤ì¹˜
pip install --upgrade pillow

# ë˜ëŠ” ì†ìƒëœ ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸
python -c "
from PIL import Image
from pathlib import Path
import json

with open('calibration_images/calibration_metadata.json', 'r') as f:
    meta = json.load(f)

for item in meta:
    try:
        Image.open(item['copied_path']).verify()
    except Exception as e:
        print(f'ì†ìƒëœ ì´ë¯¸ì§€: {item[\"copied_path\"]} - {e}')
"
```

### ë¬¸ì œ 6: "matplotlibì—ì„œ í•œê¸€ ê¹¨ì§"

í•œê¸€ í°íŠ¸ ì„¤ì •:

```python
# analyze_calibration_dataset.py ìƒë‹¨ì— ì¶”ê°€
import matplotlib.pyplot as plt
import platform

# í•œê¸€ í°íŠ¸ ì„¤ì •
if platform.system() == 'Darwin':  # macOS
    plt.rc('font', family='AppleGothic')
elif platform.system() == 'Windows':
    plt.rc('font', family='Malgun Gothic')
else:  # Linux
    plt.rc('font', family='NanumGothic')

plt.rc('axes', unicode_minus=False)  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
```

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

ì´ Action Plan ì™„ë£Œ í›„:

1. **ì„±ëŠ¥ì´ ëª©í‘œ ë„ë‹¬ ì‹œ** (`abs_rel < 0.090`):
   - âœ… Phase 1 ì™„ë£Œ!
   - ğŸ“– `ST2_dual_head_architecture.md` ì°¸ì¡°í•˜ì—¬ Phase 2 ì§„í–‰

2. **ì„±ëŠ¥ì´ ëª©í‘œ ë¯¸ë‹¬ ì‹œ** (`abs_rel > 0.090`):
   - ğŸ”„ Calibration ë°ì´í„°ì…‹ 500ê°œë¡œ í™•ëŒ€
   - ğŸ”„ Weight Normalization ì ìš© (ST1_advanced_PTQ_Calibration.md ì°¸ì¡°)
   - ğŸ”„ NPU ìŠ¤í™ ì¬í™•ì¸ (Asymmetric Quantization ì§€ì› ì—¬ë¶€)

3. **ë¬¸ì„œ ì •ë¦¬**:
   - ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì´ ë¬¸ì„œì— ê¸°ë¡
   - ë°œìƒí•œ ë¬¸ì œì™€ í•´ê²° ë°©ë²• ì¶”ê°€
