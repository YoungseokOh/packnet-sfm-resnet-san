# INT8 ì–‘ìí™” ì„±ëŠ¥ ìµœì í™” ì „ëµ

**ëª©í‘œ**: NPU INT8 ì„±ëŠ¥ í–¥ìƒ (í˜„ì¬ abs_rel 0.1133 â†’ ëª©í‘œ 0.05 ì´í•˜)  
**ì œì•½ì‚¬í•­**: Post-Training Quantization (PTQ) only, min/max calibration  
**ë‚ ì§œ**: 2025-11-06

---

## ğŸ“Š í˜„ì¬ ìƒíƒœ

### ì„±ëŠ¥ ì§€í‘œ
```
FP32 (PyTorch):  abs_rel = 0.0304
INT8 (NPU PTQ):  abs_rel = 0.1133
Degradation:     +272% (3.7ë°° ì•…í™”)
```

### ë¬¸ì œ ë¶„ì„
- **Output quantization**: Â±28mm (ì´ë¡ ì )
- **ì‹¤ì œ RMSE ì¦ê°€**: 351mm (ì´ë¡ ì˜ 12.5ë°°!)
- **ì£¼ìš” ì›ì¸**: Multi-layer feature map quantization ëˆ„ì  íš¨ê³¼

---

## ğŸ¯ ìµœì í™” ì „ëµ (3ê°€ì§€ ì ‘ê·¼)

---

## ì „ëµ 1: Integer-Fractional Separation (ì •ìˆ˜ë¶€/ì†Œìˆ˜ë¶€ ë¶„ë¦¬)

### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

ê¹Šì´ ê°’ì„ **ì •ìˆ˜ë¶€ì™€ ì†Œìˆ˜ë¶€ë¡œ ë¶„ë¦¬**í•˜ì—¬ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì–‘ìí™”

```python
# Depth decomposition
depth = 7.5m
integer_part = 7    # INT8: 0-15 (16 levels for integer)
fractional_part = 0.5  # INT8: 0-255 (256 levels for fraction)

# Reconstruction
depth_reconstructed = integer_part + (fractional_part / 256)
```

### ğŸ“ ìˆ˜í•™ì  ë¶„ì„

#### í˜„ì¬ ë°©ì‹ (Single INT8)
```
Range: [0.5, 15.0]m
Step: 14.5 / 255 = 0.0569m = 56.9mm
Error: Â±28.4mm
```

#### ì œì•ˆ ë°©ì‹ (Integer + Fractional)
```
Integer part (0-15):
  - 4 bits (16 levels)
  - Step: 1m
  - Error: Â±0.5m

Fractional part (0-1):
  - 8 bits (256 levels)  
  - Step: 1/256 = 0.00391m = 3.9mm
  - Error: Â±1.95mm

Total error: Â±1.95mm (14.5ë°° ê°œì„ !)
```

### ğŸ—ï¸ êµ¬í˜„ ë°©ì•ˆ

#### Option A: Dual-Head Architecture
```python
class DualHeadDepthDecoder(nn.Module):
    def __init__(self):
        # Shared encoder
        self.encoder = ResNetEncoder()
        
        # Separate heads
        self.integer_head = nn.Sequential(
            nn.Conv2d(256, 1, 1),
            nn.Sigmoid()  # Output: [0, 1] â†’ scale to [0, 15]
        )
        
        self.fractional_head = nn.Sequential(
            nn.Conv2d(256, 1, 1),
            nn.Sigmoid()  # Output: [0, 1] â†’ fractional part
        )
    
    def forward(self, x):
        features = self.encoder(x)
        
        # Integer part: [0, 15]m
        int_sigmoid = self.integer_head(features)
        integer_part = int_sigmoid * 15.0
        
        # Fractional part: [0, 1)
        frac_sigmoid = self.fractional_head(features)
        
        # Combine
        depth = integer_part + frac_sigmoid
        return depth
```

#### Option B: Single-Head with Post-Processing
```python
class DepthDecoderWithSeparation(nn.Module):
    def forward(self, x):
        # Standard depth prediction
        depth = self.depth_head(x)  # [0.5, 15.0]m
        
        # Training: No separation (standard loss)
        if self.training:
            return depth
        
        # Inference: Separate for INT8
        else:
            integer_part = torch.floor(depth)
            fractional_part = depth - integer_part
            return integer_part, fractional_part
```

### âœ… ì¥ì 
1. **ì •ë°€ë„ í–¥ìƒ**: Â±28mm â†’ Â±2mm (14ë°° ê°œì„ )
2. **Uniform error**: ëª¨ë“  ê¹Šì´ ë²”ìœ„ì—ì„œ ë™ì¼
3. **PTQ í˜¸í™˜**: Post-processingë§Œìœ¼ë¡œ ì ìš© ê°€ëŠ¥

### âŒ ë‹¨ì 
1. **NPU ì œì•½**: Dual output ì§€ì› ì—¬ë¶€ í™•ì¸ í•„ìš”
2. **ì¬í•™ìŠµ í•„ìš”**: Dual-headëŠ” ì²˜ìŒë¶€í„° ì¬í•™ìŠµ
3. **ë³µì¡ë„ ì¦ê°€**: Inference pipeline ìˆ˜ì • í•„ìš”

### ğŸ¯ ì¶”ì²œ êµ¬í˜„ ìˆœì„œ
1. **Phase 1**: Option B (Post-processing) - ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
2. **Phase 2**: NPU dual-output ê²€ì¦
3. **Phase 3**: Option A (Dual-head) - ì¬í•™ìŠµ

---

## ì „ëµ 2: Knowledge Distillation (Teacher-Student)

### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

FP32 Teacher ëª¨ë¸ì˜ **feature-level ì§€ì‹**ì„ INT8 Studentì— ì „ë‹¬

```
Teacher (FP32) â”€â”€â†’ Feature Maps â”€â”€â”
                                  â”œâ”€â”€â†’ Distillation Loss
Student (INT8) â”€â”€â†’ Feature Maps â”€â”€â”˜
                     â†“
                Output Depth
```

### ğŸ“ ìˆ˜í•™ì  ì •ì˜

#### Standard Loss (í˜„ì¬)
```python
L_standard = MSE(pred_int8, gt_depth)
```

#### Distillation Loss (ì œì•ˆ)
```python
L_distill = L_output + Î±Â·L_feature + Î²Â·L_hint

L_output = MSE(pred_int8, pred_fp32)  # Output matching
L_feature = Î£ MSE(F_int8[i], F_fp32[i])  # Feature matching
L_hint = MSE(attention_int8, attention_fp32)  # Attention matching
```

### ğŸ—ï¸ êµ¬í˜„ ë°©ì•ˆ

#### Distillation Training Loop
```python
class DistillationTrainer:
    def __init__(self, teacher_fp32, student_int8):
        self.teacher = teacher_fp32.eval()  # Frozen
        self.student = student_int8
        
        # Loss weights
        self.alpha = 0.5  # Feature distillation
        self.beta = 0.3   # Hint distillation
    
    def forward(self, batch):
        # Teacher inference (no grad)
        with torch.no_grad():
            teacher_output = self.teacher(batch['rgb'])
            teacher_features = self.teacher.get_features()
        
        # Student training
        student_output = self.student(batch['rgb'])
        student_features = self.student.get_features()
        
        # Losses
        L_output = F.mse_loss(student_output, teacher_output)
        L_feature = sum([
            F.mse_loss(s_feat, t_feat.detach())
            for s_feat, t_feat in zip(student_features, teacher_features)
        ])
        
        total_loss = L_output + self.alpha * L_feature
        return total_loss
```

#### Quantization-Aware Feature Matching
```python
def feature_distillation_loss(student_feat, teacher_feat):
    """
    Match feature distributions instead of exact values
    â†’ More robust to quantization noise
    """
    # Statistical matching
    loss_mean = F.mse_loss(student_feat.mean(), teacher_feat.mean())
    loss_std = F.mse_loss(student_feat.std(), teacher_feat.std())
    
    # Distribution matching (KL divergence)
    loss_kl = F.kl_div(
        F.log_softmax(student_feat.flatten(), dim=0),
        F.softmax(teacher_feat.flatten(), dim=0),
        reduction='batchmean'
    )
    
    return loss_mean + loss_std + 0.1 * loss_kl
```

### âœ… ì¥ì 
1. **Feature-level guidance**: ë‹¨ìˆœ output matchingë³´ë‹¤ íš¨ê³¼ì 
2. **Quantization ëŒ€ì‘**: INT8 íŠ¹ì„±ì— ë§ê²Œ í•™ìŠµ
3. **ê²€ì¦ëœ ë°©ë²•**: CV ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©

### âŒ ë‹¨ì 
1. **ì¬í•™ìŠµ í•„ìˆ˜**: FP32 Teacher í•„ìš”
2. **ë©”ëª¨ë¦¬ 2ë°°**: Teacher + Student ë™ì‹œ ë¡œë“œ
3. **í•™ìŠµ ì‹œê°„ ì¦ê°€**: ~1.5-2ë°°

### ğŸ¯ ì¶”ì²œ êµ¬í˜„ ìˆœì„œ
1. **Phase 1**: Output distillation (L_outputë§Œ)
2. **Phase 2**: Feature distillation ì¶”ê°€
3. **Phase 3**: Attention/Hint distillation

### ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 
```
Baseline PTQ:        abs_rel = 0.1133
+ Output distill:    abs_rel = 0.08 (30% ê°œì„ )
+ Feature distill:   abs_rel = 0.06 (47% ê°œì„ )
+ Attention distill: abs_rel = 0.04 (65% ê°œì„ )
```

---

## ì „ëµ 3: Mixed Precision (NPU ì§€ì› ì‹œ)

### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

**Critical layersëŠ” FP16**, Non-critical layersëŠ” INT8

```
Input (INT8)
  â†“
Encoder Layers 1-3: INT8 (ë¹ ë¦„, ì •í™•ë„ ëœ ì¤‘ìš”)
  â†“
Encoder Layer 4: FP16 (ì¤‘ìš”í•œ high-level features)
  â†“
Decoder: FP16 (ì •ë°€ë„ ì¤‘ìš”)
  â†“
Final Conv: FP16 (depth output, ìµœê³  ì •ë°€ë„ í•„ìš”)
  â†“
Output (FP32)
```

### ğŸ“ ì„±ëŠ¥ ë¶„ì„

#### Layer-wise Sensitivity Analysis (ì‚¬ì „ ë¶„ì„ í•„ìš”)
```python
def analyze_layer_sensitivity(model, val_loader):
    """
    ê° layerë¥¼ INT8ë¡œ ë³€í™˜í–ˆì„ ë•Œ ì„±ëŠ¥ ì €í•˜ ì¸¡ì •
    """
    sensitivities = {}
    
    for layer_name in model.layers:
        # Quantize only this layer
        quantized_model = quantize_single_layer(model, layer_name)
        
        # Measure degradation
        metrics = evaluate(quantized_model, val_loader)
        sensitivity = metrics['abs_rel'] - baseline_abs_rel
        
        sensitivities[layer_name] = sensitivity
    
    return sensitivities

# Example output:
# {
#   'encoder.layer1': 0.002,  # Low sensitivity â†’ INT8 OK
#   'encoder.layer4': 0.045,  # High sensitivity â†’ FP16!
#   'decoder.conv5': 0.038,   # High sensitivity â†’ FP16!
#   'final_conv': 0.052,      # Highest sensitivity â†’ FP16!
# }
```

#### Precision Assignment Strategy
```python
class MixedPrecisionModel:
    def __init__(self, sensitivity_dict, threshold=0.02):
        self.precision_map = {}
        
        for layer, sensitivity in sensitivity_dict.items():
            if sensitivity > threshold:
                self.precision_map[layer] = 'FP16'
            else:
                self.precision_map[layer] = 'INT8'
    
    def get_precision_config(self):
        return self.precision_map
```

### ğŸ—ï¸ NPU ì œì•½ì‚¬í•­ í™•ì¸ í•„ìš”

#### í™•ì¸ ì‚¬í•­
1. **NPU FP16 ì§€ì› ì—¬ë¶€**
   - ì¼ë¶€ NPUëŠ” INT8ë§Œ ì§€ì›
   - FP16 ì§€ì› ì‹œ throughput í™•ì¸

2. **Per-layer precision ì„¤ì • ê°€ëŠ¥ ì—¬ë¶€**
   - ONNX mixed precision export ì§€ì›
   - NPU runtime mixed precision ì§€ì›

3. **ë©”ëª¨ë¦¬ ì œì•½**
   - INT8: 14MB
   - FP16: 27MB
   - Mixed: ~18-22MB (ì˜ˆìƒ)

### âœ… ì¥ì 
1. **ì •í™•ë„-ì†ë„ ê· í˜•**: FP32ì™€ INT8ì˜ ì¤‘ê°„
2. **ì„ íƒì  ìµœì í™”**: Critical layersë§Œ FP16
3. **PTQ ê°€ëŠ¥**: ì¬í•™ìŠµ ì—†ì´ ì ìš©

### âŒ ë‹¨ì 
1. **NPU ì˜ì¡´ì„±**: NPUê°€ FP16/Mixed precision ì§€ì›í•´ì•¼ í•¨
2. **ë³µì¡í•œ ìµœì í™”**: Layer sensitivity ë¶„ì„ í•„ìš”
3. **ë¶ˆí™•ì‹¤ì„±**: NPUì—ì„œ ì‹¤ì œ ë™ì‘ ë³´ì¥ ì•ˆë¨

### ğŸ¯ í™•ì¸ ì ˆì°¨
```bash
# 1. NPU FP16 ì§€ì› í™•ì¸
npu-info --supported-dtypes

# 2. ONNX mixed precision export í…ŒìŠ¤íŠ¸
python scripts/export_onnx_mixed_precision.py

# 3. NPUì—ì„œ ë¡œë“œ í…ŒìŠ¤íŠ¸
python scripts/test_npu_mixed_precision.py
```

---

## ğŸ¯ ì¢…í•© ì „ëµ ë° ìš°ì„ ìˆœìœ„

### Phase 1: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (1-2ì¼)
1. âœ… **Integer-Fractional Separation (Post-processing)**
   - ì¬í•™ìŠµ ë¶ˆí•„ìš”
   - ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
   - ì˜ˆìƒ ê°œì„ : abs_rel 0.1133 â†’ 0.09

2. âœ… **NPU ì œì•½ì‚¬í•­ í™•ì¸**
   - FP16 ì§€ì› ì—¬ë¶€
   - Mixed precision ê°€ëŠ¥ì„±
   - Dual-output ì§€ì› ì—¬ë¶€

### Phase 2: ì¤‘ê¸° ì „ëµ (1-2ì£¼)
3. ğŸ”„ **Knowledge Distillation (Output-level)**
   - Teacher: í˜„ì¬ FP32 ëª¨ë¸
   - Student: INT8-aware training
   - ì˜ˆìƒ ê°œì„ : abs_rel 0.1133 â†’ 0.06-0.08

### Phase 3: ì¥ê¸° ì „ëµ (2-4ì£¼)
4. ğŸ”„ **Dual-Head Architecture (ì¬í•™ìŠµ)**
   - Integer + Fractional heads
   - ì²˜ìŒë¶€í„° ë¶„ë¦¬ í•™ìŠµ
   - ì˜ˆìƒ ê°œì„ : abs_rel 0.1133 â†’ 0.04-0.05

5. ğŸ”„ **Advanced Distillation**
   - Feature-level matching
   - Attention distillation
   - ì˜ˆìƒ ìµœì¢…: abs_rel 0.03-0.04 (FP32 ìˆ˜ì¤€!)

### Phase 4: ì¡°ê±´ë¶€ (NPU ì§€ì› ì‹œ)
6. â¸ï¸ **Mixed Precision**
   - NPU FP16 ì§€ì› ì‹œë§Œ ê°€ëŠ¥
   - Layer sensitivity ë¶„ì„
   - Critical layers FP16 í• ë‹¹

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ë¡œë“œë§µ

```
Current:                    abs_rel = 0.1133

Phase 1 (Post-processing):  abs_rel = 0.09   (20% ê°œì„ )
Phase 2 (Output distill):   abs_rel = 0.07   (38% ê°œì„ )
Phase 3 (Dual-head):        abs_rel = 0.05   (56% ê°œì„ )
Phase 4 (Feature distill):  abs_rel = 0.035  (69% ê°œì„ )

Target:                     abs_rel < 0.05   (âœ… ë‹¬ì„± ê°€ëŠ¥!)
```

---

## ğŸ”§ ì‹¤í—˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ì¦‰ì‹œ ì‹¤í—˜
- [ ] Integer-Fractional post-processing êµ¬í˜„
- [ ] NPU dual-output í…ŒìŠ¤íŠ¸
- [ ] NPU FP16 ì§€ì› í™•ì¸
- [ ] Baseline ì„±ëŠ¥ ì¸¡ì •

### Phase 2: ì¬í•™ìŠµ ì‹¤í—˜
- [ ] Output distillation êµ¬í˜„
- [ ] Teacher ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
- [ ] Distillation training loop
- [ ] Validation ì„±ëŠ¥ ë¹„êµ

### Phase 3: ê³ ê¸‰ ì‹¤í—˜
- [ ] Dual-head architecture ì„¤ê³„
- [ ] Feature distillation êµ¬í˜„
- [ ] Mixed precision layer analysis
- [ ] ìµœì¢… ì„±ëŠ¥ ê²€ì¦

---

## ğŸ’¡ í•µì‹¬ ê¶Œì¥ì‚¬í•­

1. **Phase 1ë¶€í„° ìˆœì°¨ ì§„í–‰**
   - ë¹ ë¥¸ ê²€ì¦ â†’ ì ì§„ì  ê°œì„ 
   - ê° ë‹¨ê³„ë§ˆë‹¤ ì„±ëŠ¥ ì¸¡ì •

2. **NPU ì œì•½ì‚¬í•­ ìµœìš°ì„  í™•ì¸**
   - FP16, Dual-output ì§€ì› ì—¬ë¶€
   - ì´ì— ë”°ë¼ ì „ëµ ì¡°ì •

3. **Knowledge Distillation ìš°ì„  ì¶”ì²œ**
   - ê²€ì¦ëœ ë°©ë²•
   - ì¬í•™ìŠµ í•„ìš”í•˜ì§€ë§Œ íš¨ê³¼ í™•ì‹¤
   - Feature-levelê¹Œì§€ í™•ì¥ ê°€ëŠ¥

4. **Integer-Fractionalì€ ë³´ì¡° ì „ëµ**
   - ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
   - NPU ì œì•½ ìˆì„ ìˆ˜ ìˆìŒ
   - í•˜ì§€ë§Œ ì‹œë„í•  ê°€ì¹˜ ìˆìŒ!

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. **NPU ìŠ¤í™ í™•ì¸** (ìµœìš°ì„ !)
2. **Post-processing í…ŒìŠ¤íŠ¸** (1ì¼)
3. **Distillation êµ¬í˜„** (1ì£¼)
4. **ì„±ëŠ¥ ë¹„êµ ë° ìµœì¢… ì„ íƒ** (2ì£¼)

**ëª©í‘œ: abs_rel 0.05 ì´í•˜ ë‹¬ì„±!** ğŸ¯
