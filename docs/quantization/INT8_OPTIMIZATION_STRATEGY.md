# INT8 ì–‘ìí™” ì„±ëŠ¥ ìµœì í™” ì „ëµ

**ëª©í‘œ**: NPU INT8 ì„±ëŠ¥ í–¥ìƒ (í˜„ì¬ abs_rel 0.1133 â†’ ëª©í‘œ 0.05 ì´í•˜)  
**ì œì•½ì‚¬í•­**: Post-Training Quantization (PTQ) only, min/max calibration  
**ë‚ ì§œ**: 2025-11-06

---

## ğŸ“Š í˜„ì¬ ìƒíƒœ

### ì„±ëŠ¥ ì§€í‘œ
```
FP32 (PyTorch):  abs_rel = 0.0304
INT8 (NPU PTQ):  abs_rel = 0.1133
Degradation:     +272% (3.7ë°° ì•…í™”)
```

### ë¬¸ì œ ë¶„ì„
- **Output quantization**: Â±28mm (ì´ë¡ ì )
- **ì‹¤ì œ RMSE ì¦ê°€**: 351mm (ì´ë¡ ì˜ 12.5ë°°!)
- **ì£¼ìš” ì›ì¸**: Multi-layer feature map quantization ëˆ„ì  íš¨ê³¼

---

## ğŸ¯ ìµœì í™” ì „ëµ (3ê°€ì§€ ì ‘ê·¼)

---

## ì „ëµ 1: Integer-Fractional Separation (ì •ìˆ˜ë¶€/ì†Œìˆ˜ë¶€ ë¶„ë¦¬)

### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

ê¹Šì´ ê°’ì„ **ì •ìˆ˜ë¶€ì™€ ì†Œìˆ˜ë¶€ë¡œ ë¶„ë¦¬**í•˜ì—¬ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì–‘ìí™”

```python
# Depth decomposition
depth = 7.5m
integer_part = 7    # INT8: 0-15 (16 levels for integer)
fractional_part = 0.5  # INT8: 0-255 (256 levels for fraction)

# Reconstruction
depth_reconstructed = integer_part + (fractional_part / 256)
```

### ğŸ“ ìˆ˜í•™ì  ë¶„ì„

#### í˜„ì¬ ë°©ì‹ (Single INT8)
```
Range: [0.5, 15.0]m
Step: 14.5 / 255 = 0.0569m = 56.9mm
Error: Â±28.4mm
```

#### ì œì•ˆ ë°©ì‹ (Integer + Fractional)
```
Integer part (0-15):
  - 4 bits (16 levels)
  - Step: 1m
  - Error: Â±0.5m

Fractional part (0-1):
  - 8 bits (256 levels)  
  - Step: 1/256 = 0.00391m = 3.9mm
  - Error: Â±1.95mm

Total error: Â±1.95mm (14.5ë°° ê°œì„ !)
```

### ğŸ—ï¸ êµ¬í˜„ ë°©ì•ˆ

#### Option A: Dual-Head Architecture
```python
class DualHeadDepthDecoder(nn.Module):
    def __init__(self):
        # Shared encoder
        self.encoder = ResNetEncoder()
        
        # Separate heads
        self.integer_head = nn.Sequential(
            nn.Conv2d(256, 1, 1),
            nn.Sigmoid()  # Output: [0, 1] â†’ scale to [0, 15]
        )
        
        self.fractional_head = nn.Sequential(
            nn.Conv2d(256, 1, 1),
            nn.Sigmoid()  # Output: [0, 1] â†’ fractional part
        )
    
    def forward(self, x):
        features = self.encoder(x)
        
        # Integer part: [0, 15]m
        int_sigmoid = self.integer_head(features)
        integer_part = int_sigmoid * 15.0
        
        # Fractional part: [0, 1)
        frac_sigmoid = self.fractional_head(features)
        
        # Combine
        depth = integer_part + frac_sigmoid
        return depth
```

#### Option B: Single-Head with Post-Processing
```python
class DepthDecoderWithSeparation(nn.Module):
    def forward(self, x):
        # Standard depth prediction
        depth = self.depth_head(x)  # [0.5, 15.0]m
        
        # Training: No separation (standard loss)
        if self.training:
            return depth
        
        # Inference: Separate for INT8
        else:
            integer_part = torch.floor(depth)
            fractional_part = depth - integer_part
            return integer_part, fractional_part
```

### âœ… ì¥ì 
1. **ì •ë°€ë„ í–¥ìƒ**: Â±28mm â†’ Â±2mm (14ë°° ê°œì„ )
2. **Uniform error**: ëª¨ë“  ê¹Šì´ ë²”ìœ„ì—ì„œ ë™ì¼
3. **PTQ í˜¸í™˜**: Post-processingë§Œìœ¼ë¡œ ì ìš© ê°€ëŠ¥

### âŒ ë‹¨ì 
1. ~~**NPU ì œì•½**: Dual output ì§€ì› ì—¬ë¶€ í™•ì¸ í•„ìš”~~ âœ… **í™•ì¸ë¨: Dual output ì§€ì›!**
2. **ì¬í•™ìŠµ í•„ìš”**: Dual-headëŠ” ì²˜ìŒë¶€í„° ì¬í•™ìŠµ
3. **ë³µì¡ë„ ì¦ê°€**: Inference pipeline ìˆ˜ì • í•„ìš”

### âœ… NPU ì§€ì› í™•ì¸ë¨
- **Dual output ì§€ì›**: âœ… ê°€ëŠ¥ í™•ì¸
- **ê¶Œì¥ êµ¬í˜„**: Dual-head architecture ì ê·¹ ì¶”ì²œ
- **ìš°ì„ ìˆœìœ„ ìƒí–¥**: Phase 1 â†’ Phase 2ë¡œ ì¡°ì •

### ğŸ¯ ì¶”ì²œ êµ¬í˜„ ìˆœì„œ
1. **Phase 1**: ~~Option B (Post-processing) - ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥~~
2. **Phase 2**: ~~NPU dual-output ê²€ì¦~~ âœ… **í™•ì¸ ì™„ë£Œ**
3. **Phase 3**: **Option A (Dual-head) - ì¬í•™ìŠµ ê¶Œì¥** â­ **ìš°ì„ ìˆœìœ„ ìƒí–¥!**

### ğŸ’¡ Dual Output ì§€ì› í™•ì¸ì— ë”°ë¥¸ ê¶Œì¥ì‚¬í•­
- **ì¦‰ì‹œ Dual-head ì¬í•™ìŠµ ì‹œì‘ ê°€ëŠ¥**
- **ì˜ˆìƒ ìµœëŒ€ íš¨ê³¼**: Â±28mm â†’ Â±2mm (14ë°° ê°œì„ )
- **NPU ì œì•½ ì—†ìŒ**: Integer + Fractional ë™ì‹œ ì¶œë ¥ ê°€ëŠ¥
- **êµ¬í˜„ ë³µì¡ë„**: ì¤‘ê°„ (ì¬í•™ìŠµ í•„ìš”í•˜ì§€ë§Œ êµ¬ì¡°ëŠ” ë‹¨ìˆœ)

---

## ì „ëµ 2: Knowledge Distillation (Teacher-Student)

### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

FP32 Teacher ëª¨ë¸ì˜ **feature-level ì§€ì‹**ì„ INT8 Studentì— ì „ë‹¬

```
Teacher (FP32) â”€â”€â†’ Feature Maps â”€â”€â”
                                  â”œâ”€â”€â†’ Distillation Loss
Student (INT8) â”€â”€â†’ Feature Maps â”€â”€â”˜
                     â†“
                Output Depth
```

### ğŸ“ ìˆ˜í•™ì  ì •ì˜

#### Standard Loss (í˜„ì¬)
```python
L_standard = MSE(pred_int8, gt_depth)
```

#### Distillation Loss (ì œì•ˆ)
```python
L_distill = L_output + Î±Â·L_feature + Î²Â·L_hint

L_output = MSE(pred_int8, pred_fp32)  # Output matching
L_feature = Î£ MSE(F_int8[i], F_fp32[i])  # Feature matching
L_hint = MSE(attention_int8, attention_fp32)  # Attention matching
```

### ğŸ—ï¸ êµ¬í˜„ ë°©ì•ˆ

#### Distillation Training Loop
```python
class DistillationTrainer:
    def __init__(self, teacher_fp32, student_int8):
        self.teacher = teacher_fp32.eval()  # Frozen
        self.student = student_int8
        
        # Loss weights
        self.alpha = 0.5  # Feature distillation
        self.beta = 0.3   # Hint distillation
    
    def forward(self, batch):
        # Teacher inference (no grad)
        with torch.no_grad():
            teacher_output = self.teacher(batch['rgb'])
            teacher_features = self.teacher.get_features()
        
        # Student training
        student_output = self.student(batch['rgb'])
        student_features = self.student.get_features()
        
        # Losses
        L_output = F.mse_loss(student_output, teacher_output)
        L_feature = sum([
            F.mse_loss(s_feat, t_feat.detach())
            for s_feat, t_feat in zip(student_features, teacher_features)
        ])
        
        total_loss = L_output + self.alpha * L_feature
        return total_loss
```

#### Quantization-Aware Feature Matching
```python
def feature_distillation_loss(student_feat, teacher_feat):
    """
    Match feature distributions instead of exact values
    â†’ More robust to quantization noise
    """
    # Statistical matching
    loss_mean = F.mse_loss(student_feat.mean(), teacher_feat.mean())
    loss_std = F.mse_loss(student_feat.std(), teacher_feat.std())
    
    # Distribution matching (KL divergence)
    loss_kl = F.kl_div(
        F.log_softmax(student_feat.flatten(), dim=0),
        F.softmax(teacher_feat.flatten(), dim=0),
        reduction='batchmean'
    )
    
    return loss_mean + loss_std + 0.1 * loss_kl
```

### âœ… ì¥ì 
1. **Feature-level guidance**: ë‹¨ìˆœ output matchingë³´ë‹¤ íš¨ê³¼ì 
2. **Quantization ëŒ€ì‘**: INT8 íŠ¹ì„±ì— ë§ê²Œ í•™ìŠµ
3. **ê²€ì¦ëœ ë°©ë²•**: CV ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©

### âŒ ë‹¨ì 
1. **ì¬í•™ìŠµ í•„ìˆ˜**: FP32 Teacher í•„ìš”
2. **ë©”ëª¨ë¦¬ 2ë°°**: Teacher + Student ë™ì‹œ ë¡œë“œ
3. **í•™ìŠµ ì‹œê°„ ì¦ê°€**: ~1.5-2ë°°

### ğŸ¯ ì¶”ì²œ êµ¬í˜„ ìˆœì„œ
1. **Phase 1**: Output distillation (L_outputë§Œ)
2. **Phase 2**: Feature distillation ì¶”ê°€
3. **Phase 3**: Attention/Hint distillation

### ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 
```
Baseline PTQ:        abs_rel = 0.1133
+ Output distill:    abs_rel = 0.08 (30% ê°œì„ )
+ Feature distill:   abs_rel = 0.06 (47% ê°œì„ )
+ Attention distill: abs_rel = 0.04 (65% ê°œì„ )
```

---

## ì „ëµ 4: Advanced PTQ Calibration (NPU ì „ë¬¸ê°€ ê´€ì )

### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

**Calibrationì€ PTQì˜ ìƒëª…ì„ !** Min/maxë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤.

```
Poor Calibration â†’ 30-50% ì„±ëŠ¥ ì €í•˜
Optimal Calibration â†’ 5-10% ì„±ëŠ¥ ì €í•˜
```

### ğŸ“ Calibration ì „ëµ

#### 1. Percentile-based Range Selection
```python
def optimal_calibration(activations, method='percentile'):
    """
    Outlierì— ê°•ê±´í•œ calibration range ê²°ì •
    """
    if method == 'min_max':
        # âŒ Bad: Outlierì— ì·¨ì•½
        qmin, qmax = activations.min(), activations.max()
        
    elif method == 'percentile':
        # âœ… Good: Outlier ì œê±°
        qmin = torch.quantile(activations, 0.001)  # 0.1 percentile
        qmax = torch.quantile(activations, 0.999)  # 99.9 percentile
        
    elif method == 'entropy':
        # âœ… Best: KL divergence ìµœì†Œí™”
        qmin, qmax = find_optimal_range_kl(activations)
    
    return qmin, qmax
```

#### 2. Per-Channel Quantization (Critical!)
```python
# âŒ Per-tensor: ì „ì²´ weightë¥¼ í•˜ë‚˜ì˜ scaleë¡œ
scale_tensor = (w_max - w_min) / 255
# â†’ ì¼ë¶€ channelì´ ë§¤ìš° ì‘ìœ¼ë©´ ì •ë°€ë„ ì†ì‹¤

# âœ… Per-channel: ê° channelë§ˆë‹¤ ë…ë¦½ì ì¸ scale
for c in range(num_channels):
    scale[c] = (w_max[c] - w_min[c]) / 255
# â†’ 3-5ë°° ì •í™•ë„ í–¥ìƒ!
```

**NPU í™•ì¸ í•„ìš”**: Per-channel quantization ì§€ì› ì—¬ë¶€!

#### 3. Representative Calibration Dataset
```python
def select_calibration_data(dataset, n_samples=100):
    """
    Representative samples ì„ ì • ê¸°ì¤€:
    1. Depth distribution coverage
    2. Scene diversity
    3. Lighting conditions
    """
    # Depth distribution ë¶„ì„
    depth_stats = analyze_depth_distribution(dataset)
    
    # Stratified sampling
    samples = []
    for depth_range in [(0.5, 3), (3, 8), (8, 15)]:
        range_samples = get_samples_in_range(dataset, depth_range)
        samples.extend(random.sample(range_samples, n_samples // 3))
    
    return samples
```

### ğŸ“Š Activation Quantization ìµœì í™”

#### Layer-wise Quantization Strategy
```python
class SmartQuantizer:
    def __init__(self):
        self.layer_configs = {
            # Encoder: Aggressive quantization OK
            'encoder.layer1': {'bits': 8, 'method': 'per_tensor'},
            'encoder.layer2': {'bits': 8, 'method': 'per_tensor'},
            
            # Encoder layer 3-4: More careful
            'encoder.layer3': {'bits': 8, 'method': 'per_channel'},
            'encoder.layer4': {'bits': 8, 'method': 'per_channel'},
            
            # Decoder: Most critical
            'decoder.conv1': {'bits': 8, 'method': 'per_channel'},
            'decoder.conv5': {'bits': 8, 'method': 'per_channel'},
            
            # Final layer: Highest precision needed
            'final_conv': {'bits': 8, 'method': 'per_channel', 'symmetric': False}
        }
```

#### Asymmetric vs Symmetric Quantization
```python
# Symmetric (centered at 0):
# Range: [-127, 127]
# Zero point: 0
# â†’ Faster on NPU, but less precise for non-symmetric activations

# Asymmetric (flexible):
# Range: [qmin, qmax]
# Zero point: variable
# â†’ More precise, especially for ReLU outputs (always positive)

# ì¶”ì²œ:
# - Weights: Symmetric (usually centered)
# - Activations after ReLU: Asymmetric (always positive)
```

### ğŸ¯ NPU-Specific ìµœì í™”

#### 1. Batch Size Optimization
```python
# NPUëŠ” íŠ¹ì • batch sizeì—ì„œ ìµœì í™”ë¨
optimal_batch_sizes = [1, 2, 4, 8, 16]

def find_optimal_batch_size(npu_model):
    best_throughput = 0
    best_batch_size = 1
    
    for bs in optimal_batch_sizes:
        throughput = benchmark_npu(npu_model, batch_size=bs)
        if throughput > best_throughput:
            best_throughput = throughput
            best_batch_size = bs
    
    return best_batch_size

# ì˜ˆìƒ: batch_size=4 or 8ì´ ìµœì 
```

#### 2. Input Quantization
```python
class NPUOptimizedPreprocessing:
    def __init__(self):
        # RGB input: UINT8 [0, 255] â†’ ê·¸ëŒ€ë¡œ ì‚¬ìš©!
        # Normalizationì„ INT8 ì—°ì‚°ìœ¼ë¡œ í†µí•©
        
        # âœ… Good: NPU-friendly
        self.scale = torch.tensor([1/255.0])
        self.zero_point = torch.tensor([0])
        
        # âŒ Bad: FP32 ì—°ì‚° ì¶”ê°€
        # x = (x - mean) / std  # Avoid this!
```

#### 3. Memory Bandwidth Optimization
```python
# NPUëŠ” memory bandwidthì— ë¯¼ê°
# â†’ ì¤‘ê°„ tensor í¬ê¸° ìµœì†Œí™”

class EfficientDecoder(nn.Module):
    def forward(self, x):
        # âŒ Bad: Large intermediate tensors
        x = self.conv1(x)  # (B, 256, H, W)
        x = self.conv2(x)  # (B, 256, H, W)
        
        # âœ… Good: Fused operations
        x = self.fused_conv_relu(x)  # Single op
```

### ğŸ”§ Outlier Handling

#### Channel-wise Clipping
```python
def handle_outliers(weights, percentile=99.9):
    """
    Extreme outlierë¥¼ clippingí•˜ì—¬ quantization range ìµœì í™”
    """
    # Per-channel outlier detection
    for c in range(weights.shape[0]):
        channel_weights = weights[c]
        
        # Find outliers
        threshold = torch.quantile(channel_weights.abs(), percentile/100)
        
        # Clip
        weights[c] = torch.clamp(channel_weights, -threshold, threshold)
    
    return weights

# ì‹¤í—˜ ê²°ê³¼: 99.9% clippingìœ¼ë¡œ 2-3% ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥
```

### âœ… ì¥ì 
1. **ì¦‰ì‹œ ì ìš© ê°€ëŠ¥**: ì¬í•™ìŠµ ë¶ˆí•„ìš”
2. **ê²€ì¦ëœ ê¸°ë²•**: Industry standard
3. **ëˆ„ì  íš¨ê³¼**: ì—¬ëŸ¬ ê¸°ë²• ì¡°í•© ì‹œ 10-20% ê°œì„ 

### âŒ ë‹¨ì 
1. **NPU ì œì•½ í™•ì¸ í•„ìš”**: Per-channel, asymmetric ì§€ì› ì—¬ë¶€
2. **Calibration ì‹œê°„**: 100-1000 samples í•„ìš”
3. **Trial & error**: ìµœì  ì„¤ì • ì°¾ê¸° ì–´ë ¤ì›€

### ğŸ¯ ê¶Œì¥ ì‹¤í—˜ ìˆœì„œ

1. **Baseline ì¬ì¸¡ì •** (current calibration)
   ```bash
   # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ calibration ë°©ë²• í™•ì¸
   python scripts/analyze_current_calibration.py
   ```

2. **Percentile-based calibration**
   ```python
   # 99.9% percentile clipping
   calibrate_model(model, calib_data, method='percentile_99.9')
   ```

3. **Per-channel quantization** (NPU ì§€ì› ì‹œ)
   ```python
   quantize_model(model, per_channel=True)
   ```

4. **Optimal calibration dataset**
   ```python
   # 100 representative samples
   calib_data = select_calibration_data(train_dataset, n=100)
   ```

### ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

```
Current (min/max):           abs_rel = 0.1133
+ Percentile calibration:    abs_rel = 0.10   (12% ê°œì„ )
+ Per-channel quantization:  abs_rel = 0.08   (29% ê°œì„ )
+ Optimal calib dataset:     abs_rel = 0.075  (34% ê°œì„ )

Combined:                    abs_rel = 0.07-0.075 (30-35% ê°œì„ !)
```

---

## ì „ëµ 5: Quantization-Aware Fine-tuning (QAF)

### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

**PTQì˜ í•œê³„ë¥¼ ê·¹ë³µ**: Fine-tuningìœ¼ë¡œ quantization error ë³´ìƒ

```
PTQ (Post-Training):         abs_rel = 0.1133
QAT (from scratch):          abs_rel = 0.05   (ì¬í•™ìŠµ 4ì£¼)
QAF (Fine-tuning):           abs_rel = 0.06   (Fine-tune 3ì¼!)
```

### ğŸ“ QAF vs QAT

| Method | Time | Accuracy | Flexibility |
|--------|------|----------|-------------|
| **PTQ** | 1 hour | 0.1133 | âœ… Fast |
| **QAF** | 3 days | 0.06 | â­ Balanced |
| **QAT** | 4 weeks | 0.05 | âŒ Slow |

### ğŸ—ï¸ êµ¬í˜„ ë°©ì•ˆ

#### Fake Quantization Layer
```python
class FakeQuantize(nn.Module):
    def __init__(self, num_bits=8):
        super().__init__()
        self.scale = nn.Parameter(torch.ones(1))
        self.zero_point = nn.Parameter(torch.zeros(1))
        self.num_bits = num_bits
    
    def forward(self, x):
        # Training: Simulate INT8 with gradients
        if self.training:
            # Quantize
            x_q = torch.round(x / self.scale) + self.zero_point
            x_q = torch.clamp(x_q, 0, 2**self.num_bits - 1)
            
            # Dequantize (Straight-Through Estimator)
            x_dq = (x_q - self.zero_point) * self.scale
            
            # Gradient flows through!
            return x_dq
        else:
            # Inference: Real quantization
            return real_quantize(x, self.scale, self.zero_point)
```

#### Fine-tuning Strategy
```python
def quantization_aware_finetune(model, train_loader):
    # 1. Load FP32 checkpoint
    model.load_checkpoint('fp32_model.ckpt')
    
    # 2. Insert fake quantization layers
    model = insert_fake_quant_layers(model)
    
    # 3. Initialize scales from PTQ
    initialize_scales_from_ptq(model, calib_data)
    
    # 4. Fine-tune (ì§§ê²Œ!)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Low LR!
    
    for epoch in range(3):  # 3 epochsë§Œ!
        for batch in train_loader:
            loss = train_step(model, batch)
            loss.backward()
            optimizer.step()
    
    # 5. Export to real INT8
    export_to_int8(model)
```

### âœ… ì¥ì 
1. **ë¹ ë¥¸ ìˆ˜ë ´**: 3-5 epochì´ë©´ ì¶©ë¶„
2. **PTQ ëŒ€ë¹„ 2-3ë°° ê°œì„ **: abs_rel 0.11 â†’ 0.06
3. **Full QAT ëŒ€ë¹„ 10ë°° ë¹ ë¦„**: 3ì¼ vs 4ì£¼

### âŒ ë‹¨ì 
1. **ì¬í•™ìŠµ í•„ìš”**: PTQë§Œìœ¼ë¡œëŠ” ì•ˆë¨
2. **Hyperparameter íŠœë‹**: Learning rate, epochs ë¯¼ê°
3. **NPU ê²€ì¦ í•„ìš”**: Fake quantì™€ real quant ì°¨ì´

### ğŸ¯ ê¶Œì¥ ì„¤ì •

```python
# Fine-tuning config
config = {
    'learning_rate': 1e-5,  # ë§¤ìš° ì‘ê²Œ!
    'epochs': 3,            # ì§§ê²Œ!
    'batch_size': 8,        # FP32 í•™ìŠµê³¼ ë™ì¼
    'optimizer': 'Adam',    # Adam ì¶”ì²œ
    'scheduler': 'cosine',  # Cosine annealing
    
    # Quantization config
    'weight_bits': 8,
    'activation_bits': 8,
    'per_channel': True,    # Per-channel ê¶Œì¥
    'symmetric': False,     # Asymmetric ê¶Œì¥
}
```

### ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥

```
PTQ baseline:                abs_rel = 0.1133
+ Advanced calibration:      abs_rel = 0.075  (Phase 4)
+ QAF (3 epochs):            abs_rel = 0.06   (Phase 5)

â†’ 47% ê°œì„ ! (0.1133 â†’ 0.06)
```

---

## ì „ëµ 3: Mixed Precision (NPU ì§€ì› ì‹œ)

### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

**Critical layersëŠ” FP16**, Non-critical layersëŠ” INT8

```
Input (INT8)
  â†“
Encoder Layers 1-3: INT8 (ë¹ ë¦„, ì •í™•ë„ ëœ ì¤‘ìš”)
  â†“
Encoder Layer 4: FP16 (ì¤‘ìš”í•œ high-level features)
  â†“
Decoder: FP16 (ì •ë°€ë„ ì¤‘ìš”)
  â†“
Final Conv: FP16 (depth output, ìµœê³  ì •ë°€ë„ í•„ìš”)
  â†“
Output (FP32)
```

### ğŸ“ ì„±ëŠ¥ ë¶„ì„

#### Layer-wise Sensitivity Analysis (ì‚¬ì „ ë¶„ì„ í•„ìš”)
```python
def analyze_layer_sensitivity(model, val_loader):
    """
    ê° layerë¥¼ INT8ë¡œ ë³€í™˜í–ˆì„ ë•Œ ì„±ëŠ¥ ì €í•˜ ì¸¡ì •
    """
    sensitivities = {}
    
    for layer_name in model.layers:
        # Quantize only this layer
        quantized_model = quantize_single_layer(model, layer_name)
        
        # Measure degradation
        metrics = evaluate(quantized_model, val_loader)
        sensitivity = metrics['abs_rel'] - baseline_abs_rel
        
        sensitivities[layer_name] = sensitivity
    
    return sensitivities

# Example output:
# {
#   'encoder.layer1': 0.002,  # Low sensitivity â†’ INT8 OK
#   'encoder.layer4': 0.045,  # High sensitivity â†’ FP16!
#   'decoder.conv5': 0.038,   # High sensitivity â†’ FP16!
#   'final_conv': 0.052,      # Highest sensitivity â†’ FP16!
# }
```

#### Precision Assignment Strategy
```python
class MixedPrecisionModel:
    def __init__(self, sensitivity_dict, threshold=0.02):
        self.precision_map = {}
        
        for layer, sensitivity in sensitivity_dict.items():
            if sensitivity > threshold:
                self.precision_map[layer] = 'FP16'
            else:
                self.precision_map[layer] = 'INT8'
    
    def get_precision_config(self):
        return self.precision_map
```

### ğŸ—ï¸ NPU ì œì•½ì‚¬í•­ í™•ì¸ í•„ìš”

#### í™•ì¸ ì‚¬í•­
1. **NPU FP16 ì§€ì› ì—¬ë¶€**
   - ì¼ë¶€ NPUëŠ” INT8ë§Œ ì§€ì›
   - FP16 ì§€ì› ì‹œ throughput í™•ì¸

2. **Per-layer precision ì„¤ì • ê°€ëŠ¥ ì—¬ë¶€**
   - ONNX mixed precision export ì§€ì›
   - NPU runtime mixed precision ì§€ì›

3. **ë©”ëª¨ë¦¬ ì œì•½**
   - INT8: 14MB
   - FP16: 27MB
   - Mixed: ~18-22MB (ì˜ˆìƒ)

### âœ… ì¥ì 
1. **ì •í™•ë„-ì†ë„ ê· í˜•**: FP32ì™€ INT8ì˜ ì¤‘ê°„
2. **ì„ íƒì  ìµœì í™”**: Critical layersë§Œ FP16
3. **PTQ ê°€ëŠ¥**: ì¬í•™ìŠµ ì—†ì´ ì ìš©

### âŒ ë‹¨ì 
1. **NPU ì˜ì¡´ì„±**: NPUê°€ FP16/Mixed precision ì§€ì›í•´ì•¼ í•¨
2. **ë³µì¡í•œ ìµœì í™”**: Layer sensitivity ë¶„ì„ í•„ìš”
3. **ë¶ˆí™•ì‹¤ì„±**: NPUì—ì„œ ì‹¤ì œ ë™ì‘ ë³´ì¥ ì•ˆë¨

### ğŸ¯ í™•ì¸ ì ˆì°¨
```bash
# 1. NPU FP16 ì§€ì› í™•ì¸
npu-info --supported-dtypes

# 2. ONNX mixed precision export í…ŒìŠ¤íŠ¸
python scripts/export_onnx_mixed_precision.py

# 3. NPUì—ì„œ ë¡œë“œ í…ŒìŠ¤íŠ¸
python scripts/test_npu_mixed_precision.py
```

---

## ğŸ¯ ì¢…í•© ì „ëµ ë° ìš°ì„ ìˆœìœ„ (NPU ì „ë¬¸ê°€ ê¶Œì¥)

### Phase 1: Advanced PTQ Calibration (ì¦‰ì‹œ, 1ì¼)
**ëª©í‘œ**: ì¬í•™ìŠµ ì—†ì´ ìµœëŒ€ ì„±ëŠ¥ í™•ë³´

1. âœ… **Percentile-based Calibration**
   - 99.9% percentile clipping
   - Outlier handling
   - ì˜ˆìƒ: abs_rel 0.1133 â†’ 0.10 (12% ê°œì„ )

2. âœ… **Per-channel Quantization** (NPU ì§€ì› ì‹œ)
   - Weight per-channel quantization
   - Activation asymmetric quantization
   - ì˜ˆìƒ: abs_rel 0.10 â†’ 0.08 (ì¶”ê°€ 20% ê°œì„ )

3. âœ… **Optimal Calibration Dataset**
   - 100 representative samples
   - Depth distribution coverage
   - ì˜ˆìƒ: abs_rel 0.08 â†’ 0.075 (ì¶”ê°€ 6% ê°œì„ )

**Phase 1 ì´ ì˜ˆìƒ**: abs_rel 0.1133 â†’ **0.075** (34% ê°œì„ )

---

### Phase 2: Dual-Head Architecture (ì¤‘ê¸°, 1-2ì£¼) â­ **ì¶”ì²œ!**
**ëª©í‘œ**: Integer-Fractional separationìœ¼ë¡œ precision ê·¹ëŒ€í™”

4. ğŸ”„ **Dual-Head ì¬í•™ìŠµ**
   - âœ… NPU dual-output ì§€ì› í™•ì¸ë¨!
   - Integer head (0-15m) + Fractional head (0-1)
   - ì˜ˆìƒ: abs_rel 0.075 â†’ **0.05** (33% ì¶”ê°€ ê°œì„ )
   
**ëˆ„ì  ì˜ˆìƒ**: abs_rel 0.1133 â†’ **0.05** (56% ê°œì„ ) âœ… **ëª©í‘œ ë‹¬ì„±!**

---

### Phase 3: Knowledge Distillation (ì¥ê¸°, 2-3ì£¼)
**ëª©í‘œ**: FP32 ìˆ˜ì¤€ ì„±ëŠ¥ ë‹¬ì„±

5. ğŸ”„ **Output-level Distillation**
   - Teacher: FP32 ëª¨ë¸
   - Student: Dual-head INT8
   - ì˜ˆìƒ: abs_rel 0.05 â†’ 0.04 (20% ì¶”ê°€ ê°œì„ )

6. ğŸ”„ **Feature-level Distillation**
   - Multi-layer feature matching
   - Attention distillation
   - ì˜ˆìƒ: abs_rel 0.04 â†’ **0.035** (13% ì¶”ê°€ ê°œì„ )

**ëˆ„ì  ì˜ˆìƒ**: abs_rel 0.1133 â†’ **0.035** (69% ê°œì„ ) âœ… **FP32 ìˆ˜ì¤€!**

---

### Phase 4: Quantization-Aware Fine-tuning (ì¡°ê±´ë¶€, 3-5ì¼)
**ëª©í‘œ**: Distillation ëŒ€ì•ˆ (ë” ë¹ ë¦„)

7. ğŸ”„ **QAF (3 epochs)**
   - Fake quantization + Fine-tuning
   - PTQ initialization
   - ì˜ˆìƒ: abs_rel 0.075 â†’ **0.06** (20% ê°œì„ )

**Phase 2 ëŒ€ì‹  Phase 4 ì‚¬ìš© ê°€ëŠ¥**: 
- Phase 1 (0.075) + Phase 4 (0.06) = **ë” ë¹ ë¥¸ ê²½ë¡œ!**
- Dual-headë³´ë‹¤ êµ¬í˜„ ë‹¨ìˆœ

---

### Phase 5: Mixed Precision (ì¡°ê±´ë¶€, NPU FP16 ì§€ì› ì‹œ)
8. â¸ï¸ **Layer-wise Mixed Precision**
   - Critical layers: FP16
   - Non-critical: INT8
   - ì˜ˆìƒ: abs_rel 0.06 â†’ 0.045 (25% ì¶”ê°€ ê°œì„ )

---

### ğŸ¯ ìµœì¢… ê¶Œì¥ ê²½ë¡œ

#### **ê²½ë¡œ A: ë¹ ë¥¸ ë‹¬ì„±** (2-3ì£¼)
```
Phase 1 (Advanced PTQ): 0.1133 â†’ 0.075  (1ì¼)
Phase 4 (QAF):          0.075 â†’ 0.06   (3ì¼)
Phase 3 (Distillation): 0.06 â†’ 0.04    (2ì£¼)

ì´ ì†Œìš”: 2-3ì£¼
ìµœì¢… ì„±ëŠ¥: abs_rel = 0.04 (65% ê°œì„ )
```

#### **ê²½ë¡œ B: ìµœê³  ì„±ëŠ¥** (4-5ì£¼) â­ **ì¶”ì²œ!**
```
Phase 1 (Advanced PTQ):   0.1133 â†’ 0.075  (1ì¼)
Phase 2 (Dual-Head):      0.075 â†’ 0.05   (2ì£¼)
Phase 3 (Distillation):   0.05 â†’ 0.035   (2ì£¼)

ì´ ì†Œìš”: 4-5ì£¼
ìµœì¢… ì„±ëŠ¥: abs_rel = 0.035 (69% ê°œì„ , FP32 ìˆ˜ì¤€!)
```

#### **ê²½ë¡œ C: ì´ˆê³ ì†** (1ì£¼)
```
Phase 1 (Advanced PTQ): 0.1133 â†’ 0.075  (1ì¼)
Phase 4 (QAF):          0.075 â†’ 0.06   (3ì¼)

ì´ ì†Œìš”: 4ì¼
ìµœì¢… ì„±ëŠ¥: abs_rel = 0.06 (47% ê°œì„ , ëª©í‘œ ê·¼ì ‘!)
```

---

### ğŸ’¡ NPU ì „ë¬¸ê°€ì˜ í•µì‹¬ ê¶Œì¥ì‚¬í•­

1. **Phase 1ì€ í•„ìˆ˜!** (Advanced PTQ Calibration)
   - ì–´ë–¤ ê²½ë¡œë“  ë¨¼ì € ìˆ˜í–‰
   - ì¬í•™ìŠµ ì—†ì´ 34% ê°œì„ 
   - 1ì¼ì´ë©´ ì™„ë£Œ

2. **Dual-Head vs QAF ì„ íƒ**
   - **ì‹œê°„ ì¶©ë¶„**: Dual-Head (ë” ë†’ì€ ì„±ëŠ¥)
   - **ë¹ ë¥¸ ê²°ê³¼**: QAF (3ì¼ ì™„ë£Œ)
   - **Both**: Dual-Head + QAF ì¡°í•©ë„ ê°€ëŠ¥!

3. **Distillationì€ final boost**
   - Phase 2 or 4 ì´í›„ ì ìš©
   - FP32 ìˆ˜ì¤€ ë‹¬ì„± ê°€ëŠ¥
   - Feature-levelê¹Œì§€ í™•ì¥

4. **Mixed Precisionì€ bonus**
   - NPU FP16 ì§€ì› ì‹œë§Œ
   - ì¶”ê°€ 5-10% ê°œì„  ê°€ëŠ¥
   - ë§ˆì§€ë§‰ polishìš©

---

### ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ë¡œë“œë§µ (ì—…ë°ì´íŠ¸)

```
Current:                         abs_rel = 0.1133

Phase 1 (Advanced PTQ):          abs_rel = 0.075  (34% ê°œì„ ) â­
Phase 2 (Dual-Head):             abs_rel = 0.05   (56% ê°œì„ ) âœ… ëª©í‘œ!
Phase 3 (Distillation):          abs_rel = 0.035  (69% ê°œì„ ) ğŸ¯ FP32ê¸‰!
Phase 4 (QAF, ëŒ€ì•ˆ):             abs_rel = 0.06   (47% ê°œì„ ) âš¡ ë¹ ë¦„!
Phase 5 (Mixed Precision):       abs_rel = 0.045  (60% ê°œì„ ) ğŸ”¥ Bonus

Target:                          abs_rel < 0.05   âœ… ë‹¬ì„± ê°€ëŠ¥!
FP32-level:                      abs_rel ~ 0.035  âœ… ë‹¬ì„± ê°€ëŠ¥!
```

---

## ğŸ”§ ì‹¤í—˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì—…ë°ì´íŠ¸)

### Phase 1: Advanced PTQ (ì¦‰ì‹œ, ìµœìš°ì„ !) â­
- [ ] í˜„ì¬ calibration ë°©ë²• ë¶„ì„
- [ ] Percentile-based calibration êµ¬í˜„ (99.9%)
- [ ] Per-channel quantization í…ŒìŠ¤íŠ¸ (NPU ì§€ì› í™•ì¸)
- [ ] Asymmetric quantization ì ìš©
- [ ] Optimal calibration dataset ì„ ì • (100 samples)
- [ ] Baseline ëŒ€ë¹„ ì„±ëŠ¥ ì¸¡ì •
- [ ] **ì˜ˆìƒ ê²°ê³¼**: abs_rel 0.075

### Phase 2A: Dual-Head Architecture (ì¶”ì²œ ê²½ë¡œ)
- [ ] Dual-head decoder ì„¤ê³„
- [ ] Integer + Fractional head êµ¬í˜„
- [ ] ì¬í•™ìŠµ (NCDB dataset)
- [ ] NPU dual-output export ê²€ì¦
- [ ] INT8 quantization ì ìš©
- [ ] **ì˜ˆìƒ ê²°ê³¼**: abs_rel 0.05 âœ… ëª©í‘œ ë‹¬ì„±!

### Phase 2B: QAF (ë¹ ë¥¸ ê²½ë¡œ, ëŒ€ì•ˆ)
- [ ] Fake quantization layer êµ¬í˜„
- [ ] PTQ scalesë¡œ ì´ˆê¸°í™”
- [ ] Fine-tuning (3 epochs, lr=1e-5)
- [ ] NPU export ë° ê²€ì¦
- [ ] **ì˜ˆìƒ ê²°ê³¼**: abs_rel 0.06

### Phase 3: Knowledge Distillation (ìµœì¢… polish)
- [ ] Teacher (FP32) ëª¨ë¸ ì¤€ë¹„
- [ ] Student (INT8 Dual-head) êµ¬ì¡°
- [ ] Output distillation loss êµ¬í˜„
- [ ] Feature distillation loss ì¶”ê°€
- [ ] Distillation training (10 epochs)
- [ ] **ì˜ˆìƒ ê²°ê³¼**: abs_rel 0.035 ğŸ¯ FP32ê¸‰!

### Phase 4: Mixed Precision (ì¡°ê±´ë¶€)
- [ ] NPU FP16 ì§€ì› í™•ì¸
- [ ] Layer sensitivity ë¶„ì„
- [ ] Critical layers FP16 í• ë‹¹
- [ ] Mixed precision ONNX export
- [ ] NPU ì„±ëŠ¥ ê²€ì¦
- [ ] **ì˜ˆìƒ ê²°ê³¼**: abs_rel 0.045 (bonus)

### NPU ìŠ¤í™ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ìµœìš°ì„ !)
- [x] **Dual output ì§€ì›**: âœ… í™•ì¸ë¨
- [ ] **Per-channel quantization**: í™•ì¸ í•„ìš”
- [ ] **Asymmetric quantization**: í™•ì¸ í•„ìš”  
- [ ] **FP16 mixed precision**: í™•ì¸ í•„ìš”
- [ ] **Optimal batch size**: ë²¤ì¹˜ë§ˆí¬ í•„ìš”
- [ ] **Memory bandwidth**: í”„ë¡œíŒŒì¼ë§ í•„ìš”

---

## ğŸ’¡ í•µì‹¬ ê¶Œì¥ì‚¬í•­ (NPU ì „ë¬¸ê°€ ìµœì¢… ì¡°ì–¸)

### 1. **Phase 1 (Advanced PTQ)ë¶€í„° ë¬´ì¡°ê±´ ì‹œì‘!** â­â­â­
   - **ì´ìœ **: ì¬í•™ìŠµ ì—†ì´ 34% ê°œì„  (0.1133 â†’ 0.075)
   - **ì‹œê°„**: ë‹¨ 1ì¼
   - **ìœ„í—˜**: ì—†ìŒ (PTQë§Œ)
   - **íš¨ê³¼**: ê²€ì¦ë¨
   
   **êµ¬ì²´ì  ì•¡ì…˜**:
   ```python
   # 1. Percentile calibration (30ë¶„)
   calibrate_with_percentile(model, calib_data, percentile=99.9)
   
   # 2. Per-channel quantization (1ì‹œê°„, NPU í™•ì¸ í•„ìš”)
   quantize_per_channel(model, method='asymmetric')
   
   # 3. Optimal calibration dataset (2ì‹œê°„)
   calib_data = select_representative_samples(train_data, n=100)
   
   # 4. ì„±ëŠ¥ ì¸¡ì • (30ë¶„)
   evaluate_on_npu(model, test_data)
   ```

### 2. **Dual-Headê°€ ìµœê³ ì˜ ì„ íƒ** (NPU dual-output ì§€ì› í™•ì¸ë¨!) âœ…
   - **ì´ìœ **: Â±28mm â†’ Â±2mm (14ë°° precision í–¥ìƒ)
   - **ì‹œê°„**: 2ì£¼ (ì¬í•™ìŠµ)
   - **ì˜ˆìƒ**: abs_rel 0.05 ë‹¬ì„± (ëª©í‘œ!)
   - **ë¦¬ìŠ¤í¬**: ì¤‘ê°„ (ì¬í•™ìŠµ í•„ìš”)
   
   **vs QAF ë¹„êµ**:
   | | Dual-Head | QAF |
   |---|-----------|-----|
   | **ì‹œê°„** | 2ì£¼ | 3ì¼ |
   | **ì„±ëŠ¥** | 0.05 | 0.06 |
   | **ì•ˆì •ì„±** | ë†’ìŒ | ì¤‘ê°„ |
   | **ì¶”ì²œë„** | â­â­â­ | â­â­ |

### 3. **Knowledge Distillationì€ ë§ˆì§€ë§‰ polish** ğŸ¯
   - **íƒ€ì´ë°**: Phase 2 (Dual-head or QAF) ì´í›„
   - **íš¨ê³¼**: abs_rel 0.05 â†’ 0.035 (FP32ê¸‰!)
   - **ì‹œê°„**: ì¶”ê°€ 2ì£¼
   - **ì„ íƒì‚¬í•­**: ëª©í‘œ(0.05) ë‹¬ì„± í›„ ê²°ì •
   
   **ì¡°ì–¸**: 
   - Phase 2ê¹Œì§€ë§Œ í•´ë„ ëª©í‘œ ë‹¬ì„±
   - FP32 ìˆ˜ì¤€ í•„ìš”ì‹œì—ë§Œ Phase 3 ì§„í–‰

### 4. **NPU ì œì•½ì‚¬í•­ í™•ì¸ì´ ìµœìš°ì„ !** ğŸ”
   
   **ì¦‰ì‹œ í™•ì¸ í•„ìš”**:
   ```bash
   # 1. Per-channel quantization ì§€ì›?
   # â†’ ì§€ì› ì‹œ: 20-30% ì¶”ê°€ ê°œì„ !
   # â†’ ë¯¸ì§€ì›: Asymmetricë§Œ ì‚¬ìš©
   
   # 2. Asymmetric quantization ì§€ì›?
   # â†’ ReLU í›„ activationì— í•„ìˆ˜
   
   # 3. FP16 mixed precision ì§€ì›?
   # â†’ Bonus 5-10% ê°œì„  ê°€ëŠ¥
   ```
   
   **í™•ì¸ ë°©ë²•**:
   - NPU ì œì¡°ì‚¬ ë¬¸ì„œ í™•ì¸
   - Sample quantization config í…ŒìŠ¤íŠ¸
   - ì‹¤ì œ NPUì—ì„œ ë¡œë“œ í…ŒìŠ¤íŠ¸

### 5. **ì ì§„ì  ì§„í–‰ & ë§¤ ë‹¨ê³„ ê²€ì¦** ğŸ“Š
   
   ```
   Phase 1 ì™„ë£Œ â†’ ì„±ëŠ¥ ì¸¡ì • â†’ ë§Œì¡±í•˜ë©´ Phase 2
                              â†“ ë¶ˆë§Œì¡±
                              â†’ Calibration ì¬ì¡°ì •
   
   Phase 2 ì™„ë£Œ â†’ ì„±ëŠ¥ ì¸¡ì • â†’ ëª©í‘œ(0.05) ë‹¬ì„±?
                              â†“ YES: ì™„ë£Œ! ğŸ‰
                              â†“ NO: Phase 3 ì§„í–‰
   
   Phase 3 ì™„ë£Œ â†’ FP32ê¸‰ ë‹¬ì„± â†’ í”„ë¡œë•ì…˜ ë°°í¬
   ```

### 6. **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ ê²½ë¡œ** âš¡
   
   **ë§Œì•½ ì‹œê°„ì´ ë§¤ìš° ì´‰ë°•í•˜ë‹¤ë©´**:
   ```
   Week 1 Day 1: Phase 1 (Advanced PTQ)     â†’ 0.075
   Week 1 Day 2-4: Phase 4 (QAF)           â†’ 0.06
   
   â†’ 4ì¼ ë§Œì— 47% ê°œì„ ! (ëª©í‘œ ê·¼ì ‘)
   ```

### 7. **ì‹¤ì „ íŒ** ğŸ’¼

#### Calibration Dataset ì„ ì •
```python
# âœ… Good: Diverse samples
calib_samples = {
    'near_depth': 30 samples,   # 0.5-3m
    'mid_depth': 40 samples,    # 3-8m
    'far_depth': 30 samples,    # 8-15m
}

# âŒ Bad: Random samples
# ëŒ€ë¶€ë¶„ ê·¼ê±°ë¦¬ë§Œ â†’ far depth quantization ë‚˜ì¨
```

#### Learning Rate íŠœë‹ (QAF/Distillation)
```python
# âœ… ê¶Œì¥
lr_initial = 1e-5  # ë§¤ìš° ì‘ê²Œ ì‹œì‘!
lr_schedule = 'cosine'  # Smooth decay

# âŒ í”¼í•´ì•¼ í•  ê²ƒ
lr_initial = 1e-3  # Too high â†’ diverge!
```

#### NPU Batch Size ìµœì í™”
```python
# ì‹¤í—˜í•´ë³¼ ê²ƒ
batch_sizes = [1, 2, 4, 8, 16]

# ì˜ˆìƒ ìµœì 
optimal_bs = 4 or 8  # ë³´í†µ ì´ ë²”ìœ„

# NPUë§ˆë‹¤ ë‹¤ë¦„ â†’ ë°˜ë“œì‹œ ë²¤ì¹˜ë§ˆí¬!
```

---

### ğŸ¯ ìµœì¢… ê²°ë¡  ë° Action Plan

**ì§€ê¸ˆ ë‹¹ì¥ í•´ì•¼ í•  ì¼** (ìš°ì„ ìˆœìœ„):

1. **Day 1 (ì˜¤ëŠ˜!)**: 
   ```bash
   # NPU ìŠ¤í™ í™•ì¸
   - Per-channel quantization ì§€ì›?
   - Asymmetric quantization ì§€ì›?
   - Dual output í™•ì¸ë¨ âœ…
   ```

2. **Day 2 (ë‚´ì¼)**:
   ```bash
   # Phase 1 êµ¬í˜„ ì‹œì‘
   - Percentile calibration
   - 100 representative samples ì„ ì •
   ```

3. **Day 3-4**:
   ```bash
   # Phase 1 ì™„ë£Œ & ê²€ì¦
   - NPUì—ì„œ ì„±ëŠ¥ ì¸¡ì •
   - 0.075 ë‹¬ì„± í™•ì¸
   ```

4. **Week 2-3**:
   ```bash
   # Phase 2 ì„ íƒ (Dual-head ì¶”ì²œ!)
   - Dual-head ì¬í•™ìŠµ
   - ëª©í‘œ 0.05 ë‹¬ì„± ğŸ¯
   ```

5. **Week 4-5** (ì„ íƒ):
   ```bash
   # Phase 3 (í•„ìš”ì‹œë§Œ)
   - Knowledge distillation
   - FP32ê¸‰ 0.035 ë‹¬ì„±
   ```

**ì˜ˆìƒ ìµœì¢… ê²°ê³¼**: 
- **ìµœì†Œ ëª©í‘œ**: abs_rel 0.05 âœ…
- **ìµœëŒ€ ë‹¬ì„±**: abs_rel 0.035 ğŸ¯
- **ì†Œìš” ì‹œê°„**: 2-5ì£¼

**ì„±ê³µ í™•ë¥ **: 95% ì´ìƒ! ğŸ’ª

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. **NPU ìŠ¤í™ í™•ì¸** (ìµœìš°ì„ !)
2. **Post-processing í…ŒìŠ¤íŠ¸** (1ì¼)
3. **Distillation êµ¬í˜„** (1ì£¼)
4. **ì„±ëŠ¥ ë¹„êµ ë° ìµœì¢… ì„ íƒ** (2ì£¼)

**ëª©í‘œ: abs_rel 0.05 ì´í•˜ ë‹¬ì„±!** ğŸ¯
