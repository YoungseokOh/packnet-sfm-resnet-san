# Gradient Loss êµ¬í˜„ ì „ëµ

## 1. ê°œìš”

### 1.1 ëª©í‘œ
í˜„ì¬ SSI-Silog Lossì— **Multi-Scale Gradient Loss**ë¥¼ ì¶”ê°€í•˜ì—¬ depth mapì˜ edge ë³´ì¡´ ë° ì „ì²´ êµ¬ì¡°ì  ì¼ê´€ì„±ì„ í–¥ìƒì‹œí‚¨ë‹¤.

### 1.2 ê¸°ëŒ€ íš¨ê³¼
- ê°ì²´ ê²½ê³„(edge)ì—ì„œì˜ depth ì„ ëª…ë„ í–¥ìƒ
- ì „ì²´ depth mapì˜ êµ¬ì¡°ì  ì¼ê´€ì„± ê°œì„ 
- "ì „ì²´ ë§µì´ ì•ˆ ì¢‹ë‹¤"ëŠ” ë¬¸ì œ í•´ê²°

### 1.3 í˜„ì¬ Loss êµ¬ì¡°
```
í˜„ì¬: total_loss = ssi_weight Ã— SSI_Loss + silog_weight Ã— Silog_Loss
ëª©í‘œ: total_loss = ssi_weight Ã— SSI_Loss + silog_weight Ã— Silog_Loss + gradient_weight Ã— Gradient_Loss
```

---

## 2. ê¸°ì¡´ ì½”ë“œ ë¶„ì„

### 2.1 í˜„ì¬ Loss êµ¬ì¡° (`ssi_silog_loss.py`)

```python
class SSISilogLoss(LossBase):
    def __init__(self, alpha=0.85, silog_ratio=10, silog_ratio2=0.85, 
                 ssi_weight=0.7, silog_weight=0.3,
                 min_depth=None, max_depth=None):
        # ...
        self.ssi_weight = ssi_weight
        self.silog_weight = silog_weight
    
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
        # SSI Loss (inverse depth domain)
        ssi_loss = self.compute_ssi_loss_inv(pred_inv_depth, gt_inv_depth, mask)
        
        # Silog Loss (depth domain)
        pred_depth = inv2depth(pred_inv_depth)
        gt_depth = inv2depth(gt_inv_depth)
        silog_loss = self.compute_silog_loss(pred_depth, gt_depth, mask)
        
        # Combined loss
        total_loss = self.ssi_weight * ssi_loss + self.silog_weight * silog_loss
        return total_loss
```

### 2.2 G2-MonoDepthì˜ Gradient Loss êµ¬í˜„ (`losses.py`)

```python
class Gradient2D(Module):
    """Sobel gradient ê³„ì‚°"""
    def __init__(self):
        kernel_x = torch.FloatTensor([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]])
        kernel_y = torch.FloatTensor([[-1., -2., -1.], [0., 0., 0.], [1., 2., 1.]])
        self.weight_x = Parameter(data=kernel_x.unsqueeze(0).unsqueeze(0), requires_grad=False)
        self.weight_y = Parameter(data=kernel_y.unsqueeze(0).unsqueeze(0), requires_grad=False)

    def forward(self, x):
        grad_x = conv2d(x, self.weight_x)
        grad_y = conv2d(x, self.weight_y)
        return grad_x, grad_y


class WeightedMSGradLoss(Module):
    """Multi-Scale Gradient Loss"""
    def __init__(self, k=4, sobel=True):
        self.grad_fun = Gradient2D().cuda()
        self.k = k  # number of scales

    def forward(self, output, target, hole_target):
        residual = hole_target * output + (1.0 - hole_target) * target - target
        loss = 0.
        for i in range(self.k):
            scale_factor = 1.0 / (2 ** i)
            k_residual = interpolate(residual, scale_factor=scale_factor) if i > 0 else residual
            loss += self.__gradient_loss__(k_residual)
        return loss / number_valid
```

### 2.3 YAML Config êµ¬ì¡°

```yaml
model:
    loss:
        supervised_method: 'sparse-ssi-silog'
        ssi_weight: 0.5
        silog_weight: 0.5
        alpha: 0.85
        silog_ratio2: 0.85
        # ì¶”ê°€ ì˜ˆì •
        gradient_weight: 0.0  # ë˜ëŠ” 0.2
```

---

## 3. êµ¬í˜„ ì „ëµ

### 3.1 êµ¬í˜„ ì˜µì…˜ ë¹„êµ

| ì˜µì…˜ | ì„¤ëª… | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| **A. SSISilogLoss ë‚´ë¶€ ì¶”ê°€** | ê¸°ì¡´ í´ë˜ìŠ¤ì— gradient ê³„ì‚° ì¶”ê°€ | ê¸°ì¡´ êµ¬ì¡° ìœ ì§€, í˜¸í™˜ì„± ì¢‹ìŒ | í´ë˜ìŠ¤ê°€ ë³µì¡í•´ì§ |
| **B. ë³„ë„ GradientLoss í´ë˜ìŠ¤** | ë…ë¦½ì ì¸ Loss í´ë˜ìŠ¤ ìƒì„± | ëª¨ë“ˆí™”, ì¬ì‚¬ìš© ê°€ëŠ¥ | í†µí•© ë¡œì§ í•„ìš” |
| **C. SSISilogGradientLoss ìƒˆ í´ë˜ìŠ¤** | SSI+Silog+Gradient í†µí•© í´ë˜ìŠ¤ | ê¹”ë”í•œ ì¸í„°í˜ì´ìŠ¤ | ì½”ë“œ ì¤‘ë³µ |

### 3.2 ì„ íƒ: **ì˜µì…˜ A - SSISilogLoss í™•ì¥**

**ì´ìœ **:
1. ê¸°ì¡´ YAML configì™€ í˜¸í™˜ì„± ìœ ì§€
2. `supervised_loss.py`ì˜ `get_loss_func()` ìˆ˜ì • ìµœì†Œí™”
3. gradient_weight=0ì´ë©´ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ë™ì‘ (í•˜ìœ„ í˜¸í™˜)

### 3.3 êµ¬í˜„ ê³„íš

```
íŒŒì¼ ìˆ˜ì • ëª©ë¡:
1. packnet_sfm/losses/ssi_silog_loss.py  â† Gradient Loss ì¶”ê°€
2. packnet_sfm/losses/supervised_loss.py  â† gradient_weight íŒŒë¼ë¯¸í„° ì „ë‹¬
3. configs/*.yaml  â† gradient_weight ì„¤ì • ì¶”ê°€
```

---

## 4. ìƒì„¸ êµ¬í˜„ ì„¤ê³„

### 4.1 Gradient2D í´ë˜ìŠ¤ (ìƒˆë¡œ ì¶”ê°€)

```python
class Gradient2D(nn.Module):
    """
    Sobel filterë¥¼ ì‚¬ìš©í•œ 2D gradient ê³„ì‚°
    
    Sobel X:          Sobel Y:
    [-1, 0, 1]       [-1, -2, -1]
    [-2, 0, 2]       [ 0,  0,  0]
    [-1, 0, 1]       [ 1,  2,  1]
    """
    def __init__(self):
        super().__init__()
        kernel_x = torch.tensor([
            [-1., 0., 1.],
            [-2., 0., 2.],
            [-1., 0., 1.]
        ]).view(1, 1, 3, 3)
        kernel_y = torch.tensor([
            [-1., -2., -1.],
            [ 0.,  0.,  0.],
            [ 1.,  2.,  1.]
        ]).view(1, 1, 3, 3)
        
        # Non-learnable parameters
        self.register_buffer('weight_x', kernel_x)
        self.register_buffer('weight_y', kernel_y)
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: [B, 1, H, W] depth map
        Returns:
            grad_x: [B, 1, H-2, W-2] horizontal gradient
            grad_y: [B, 1, H-2, W-2] vertical gradient
        """
        grad_x = F.conv2d(x, self.weight_x, padding=0)
        grad_y = F.conv2d(x, self.weight_y, padding=0)
        return grad_x, grad_y
```

### 4.2 SSISilogLoss í™•ì¥

```python
class SSISilogLoss(LossBase):
    def __init__(self, 
                 alpha=0.85, silog_ratio=10, silog_ratio2=0.85, 
                 ssi_weight=0.7, silog_weight=0.3,
                 gradient_weight=0.0,  # ğŸ†• ì¶”ê°€
                 gradient_scales=4,     # ğŸ†• ì¶”ê°€
                 min_depth=None, max_depth=None):
        super().__init__()
        # ê¸°ì¡´ íŒŒë¼ë¯¸í„°
        self.ssi_weight = ssi_weight
        self.silog_weight = silog_weight
        
        # ğŸ†• Gradient Loss íŒŒë¼ë¯¸í„°
        self.gradient_weight = gradient_weight
        self.gradient_scales = gradient_scales
        
        # ğŸ†• Gradient ê³„ì‚°ê¸° ì´ˆê¸°í™” (weight > 0ì¼ ë•Œë§Œ)
        if gradient_weight > 0:
            self.gradient_fn = Gradient2D()
            print(f"   Gradient weight: {gradient_weight}")
            print(f"   Gradient scales: {gradient_scales}")
    
    def compute_gradient_loss(self, pred_depth, gt_depth, mask):
        """
        Multi-scale gradient loss ê³„ì‚°
        
        Args:
            pred_depth: [B, 1, H, W] ì˜ˆì¸¡ depth
            gt_depth: [B, 1, H, W] GT depth
            mask: [B, 1, H, W] ìœ íš¨ í”½ì…€ ë§ˆìŠ¤í¬
        
        Returns:
            loss: scalar gradient loss
        """
        if self.gradient_weight <= 0:
            return torch.tensor(0.0, device=pred_depth.device)
        
        total_loss = 0.0
        
        for scale_idx in range(self.gradient_scales):
            scale_factor = 1.0 / (2 ** scale_idx)
            
            if scale_idx == 0:
                pred_s = pred_depth
                gt_s = gt_depth
                mask_s = mask
            else:
                pred_s = F.interpolate(pred_depth, scale_factor=scale_factor, 
                                       mode='bilinear', align_corners=False)
                gt_s = F.interpolate(gt_depth, scale_factor=scale_factor,
                                     mode='bilinear', align_corners=False)
                mask_s = F.interpolate(mask.float(), scale_factor=scale_factor,
                                       mode='nearest') > 0.5
            
            # Gradient ê³„ì‚°
            grad_pred_x, grad_pred_y = self.gradient_fn(pred_s)
            grad_gt_x, grad_gt_y = self.gradient_fn(gt_s)
            
            # Mask resize (gradient output is H-2, W-2)
            mask_grad = mask_s[:, :, 1:-1, 1:-1]
            
            # L1 loss on gradients
            if mask_grad.sum() > 0:
                loss_x = torch.abs(grad_pred_x - grad_gt_x)[mask_grad].mean()
                loss_y = torch.abs(grad_pred_y - grad_gt_y)[mask_grad].mean()
                total_loss += (loss_x + loss_y)
        
        return total_loss / self.gradient_scales
    
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
        # ê¸°ì¡´ SSI Loss
        ssi_loss = self.compute_ssi_loss_inv(pred_inv_depth, gt_inv_depth, mask)
        
        # ê¸°ì¡´ Silog Loss
        pred_depth = inv2depth(pred_inv_depth)
        gt_depth = inv2depth(gt_inv_depth)
        
        if mask is None:
            mask = (gt_depth > 0)
        
        silog_loss = self.compute_silog_loss(pred_depth, gt_depth, mask)
        
        # ğŸ†• Gradient Loss
        gradient_loss = self.compute_gradient_loss(pred_depth, gt_depth, mask)
        
        # ê²°í•©
        total_loss = (self.ssi_weight * ssi_loss + 
                      self.silog_weight * silog_loss + 
                      self.gradient_weight * gradient_loss)
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        self.add_metric('ssi_component', ssi_loss)
        self.add_metric('silog_component', silog_loss)
        self.add_metric('gradient_component', gradient_loss)  # ğŸ†•
        
        return total_loss
```

### 4.3 supervised_loss.py ìˆ˜ì •

```python
def get_loss_func(supervised_method, **kwargs):
    # ...
    elif supervised_method.endswith('ssi-silog'):
        return SSISilogLoss(
            min_depth=kwargs.get('min_depth', None),
            max_depth=kwargs.get('max_depth', None),
            ssi_weight=kwargs.get('ssi_weight', 0.7),
            silog_weight=kwargs.get('silog_weight', 0.3),
            alpha=kwargs.get('alpha', 0.85),
            silog_ratio=kwargs.get('silog_ratio', 10),
            silog_ratio2=kwargs.get('silog_ratio2', 0.85),
            gradient_weight=kwargs.get('gradient_weight', 0.0),  # ğŸ†•
            gradient_scales=kwargs.get('gradient_scales', 4),     # ğŸ†•
        )
```

### 4.4 YAML Config ì—…ë°ì´íŠ¸

```yaml
model:
    loss:
        supervised_method: 'sparse-ssi-silog'
        ssi_weight: 0.4           # ì¡°ì •
        silog_weight: 0.4         # ì¡°ì •
        gradient_weight: 0.2      # ğŸ†• ì¶”ê°€
        gradient_scales: 4        # ğŸ†• ì¶”ê°€ (1, 2, 4, 8ë°° downsample)
        alpha: 0.85
        silog_ratio2: 0.85
```

---

## 5. Weight ì„¤ì • ê°€ì´ë“œ

### 5.1 ê¶Œì¥ ì´ˆê¸° ì„¤ì •

| ì„¤ì •ëª… | ssi | silog | gradient | ì„¤ëª… |
|--------|-----|-------|----------|------|
| **Conservative** | 0.45 | 0.45 | 0.1 | Gradient ì˜í–¥ ìµœì†Œí™” |
| **Balanced** | 0.4 | 0.4 | 0.2 | ê· í˜• ì¡íŒ ì„¤ì • (ê¶Œì¥) |
| **Edge-Focused** | 0.35 | 0.35 | 0.3 | Edge ë³´ì¡´ ê°•ì¡° |

### 5.2 ì‹¤í—˜ ê³„íš

```
Phase 1: gradient_weight=0.1 (ë³´ìˆ˜ì )
  - ê¸°ì¡´ ì„±ëŠ¥ ìœ ì§€ í™•ì¸
  - Edge ì˜ì—­ ê°œì„  ì—¬ë¶€ í™•ì¸

Phase 2: gradient_weight=0.2 (ê· í˜•)
  - ì „ì²´ ì„±ëŠ¥ ë¹„êµ
  - RMSE, MAE, Î´1 ë³€í™” ë¶„ì„

Phase 3: gradient_weight=0.3 (edge ê°•ì¡°)
  - Edge ì„ ëª…ë„ vs ì „ì²´ ì •í™•ë„ trade-off ë¶„ì„
```

---

## 6. êµ¬í˜„ ìˆœì„œ

### 6.1 ë‹¨ê³„ë³„ êµ¬í˜„

```
Step 1: Gradient2D í´ë˜ìŠ¤ êµ¬í˜„
        - ssi_silog_loss.pyì— ì¶”ê°€
        - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

Step 2: SSISilogLoss í™•ì¥
        - gradient_weight, gradient_scales íŒŒë¼ë¯¸í„° ì¶”ê°€
        - compute_gradient_loss() ë©”ì„œë“œ êµ¬í˜„
        - forward() ìˆ˜ì •

Step 3: supervised_loss.py ìˆ˜ì •
        - get_loss_func()ì— gradient íŒŒë¼ë¯¸í„° ì „ë‹¬

Step 4: YAML Config ì—…ë°ì´íŠ¸
        - gradient_weight, gradient_scales ì¶”ê°€
        - ê¸°ë³¸ê°’ 0.0 (í•˜ìœ„ í˜¸í™˜)

Step 5: í…ŒìŠ¤íŠ¸
        - Dry-run í•™ìŠµ í…ŒìŠ¤íŠ¸
        - Loss ê°’ ë¡œê¹… í™•ì¸
        - ë©”íŠ¸ë¦­ ì¶œë ¥ í™•ì¸
```

### 6.2 íŒŒì¼ ìˆ˜ì • ìˆœì„œ

```
1. packnet_sfm/losses/ssi_silog_loss.py
   - Gradient2D í´ë˜ìŠ¤ ì¶”ê°€
   - SSISilogLoss.__init__() ìˆ˜ì •
   - compute_gradient_loss() ì¶”ê°€
   - forward() ìˆ˜ì •

2. packnet_sfm/losses/supervised_loss.py
   - get_loss_func() ìˆ˜ì • (íŒŒë¼ë¯¸í„° ì „ë‹¬)

3. configs/train_resnet_san_ncdb_distance_dual_head_640x384.yaml
   - gradient_weight, gradient_scales ì¶”ê°€
```

---

## 7. í…ŒìŠ¤íŠ¸ ê³„íš

### 7.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# Gradient2D í…ŒìŠ¤íŠ¸
def test_gradient_2d():
    grad_fn = Gradient2D()
    x = torch.randn(2, 1, 64, 64)
    grad_x, grad_y = grad_fn(x)
    assert grad_x.shape == (2, 1, 62, 62)  # H-2, W-2
    assert grad_y.shape == (2, 1, 62, 62)
```

### 7.2 í†µí•© í…ŒìŠ¤íŠ¸

```bash
# Dry-run í•™ìŠµ (1 epoch)
python scripts/train.py configs/train_resnet_san_ncdb_distance_dual_head_640x384.yaml \
    --max_epochs 1 \
    --checkpoint.filepath checkpoints/test_gradient_loss/
```

### 7.3 ê²€ì¦ í•­ëª©

- [ ] Loss ê°’ì´ NaNì´ ì•„ë‹˜
- [ ] gradient_component ë©”íŠ¸ë¦­ì´ ì •ìƒ ì¶œë ¥
- [ ] gradient_weight=0ì¼ ë•Œ ê¸°ì¡´ê³¼ ë™ì¼í•œ ê²°ê³¼
- [ ] Multi-scale gradientê°€ ì •ìƒ ë™ì‘

---

## 8. ì˜ˆìƒ ì´ìŠˆ ë° í•´ê²°ì±…

### 8.1 ë©”ëª¨ë¦¬ ì´ìŠˆ

**ë¬¸ì œ**: Multi-scale gradient ê³„ì‚°ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì¦ê°€
**í•´ê²°**: 
- gradient_scales=4 ê¸°ë³¸ê°’ (1, 1/2, 1/4, 1/8)
- í•„ìš”ì‹œ gradient_scales=2ë¡œ ê°ì†Œ

### 8.2 ìˆ˜ì¹˜ ì•ˆì •ì„±

**ë¬¸ì œ**: Gradient ê³„ì‚° ì‹œ edgeì—ì„œ í° ê°’
**í•´ê²°**:
- L1 loss ì‚¬ìš© (outlierì— ê°•ê±´)
- Mask ì ìš©ìœ¼ë¡œ invalid ì˜ì—­ ì œì™¸

### 8.3 í•™ìŠµ ë¶ˆì•ˆì •

**ë¬¸ì œ**: Gradient lossê°€ ë„ˆë¬´ ì»¤ì„œ í•™ìŠµ ë¶ˆì•ˆì •
**í•´ê²°**:
- gradient_weight ì‘ê²Œ ì‹œì‘ (0.1)
- Gradient clipping ì‚¬ìš© (clip_grad: 1.0)

---

## 9. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 9.1 ë¡œê¹…í•  ë©”íŠ¸ë¦­

```
loss/total             : ì „ì²´ loss
loss/ssi_component     : SSI loss ê¸°ì—¬ë¶„
loss/silog_component   : Silog loss ê¸°ì—¬ë¶„
loss/gradient_component: Gradient loss ê¸°ì—¬ë¶„ (ğŸ†•)
```

### 9.2 TensorBoard ì‹œê°í™”

```
- Loss curves (total, ssi, silog, gradient)
- Depth map ì‹œê°í™” (edge ì˜ì—­ ë¹„êµ)
- Gradient map ì‹œê°í™” (optional)
```

---

## 10. ìš”ì•½

### 10.1 êµ¬í˜„ ìš”ì•½

| í•­ëª© | ë‚´ìš© |
|------|------|
| **ì ‘ê·¼ë²•** | SSISilogLoss í´ë˜ìŠ¤ í™•ì¥ |
| **ìƒˆ íŒŒë¼ë¯¸í„°** | gradient_weight, gradient_scales |
| **ê¸°ë³¸ê°’** | gradient_weight=0.0 (í•˜ìœ„ í˜¸í™˜) |
| **ê¶Œì¥ê°’** | gradient_weight=0.2, gradient_scales=4 |

### 10.2 ìˆ˜ì • íŒŒì¼

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|------|----------|
| `ssi_silog_loss.py` | Gradient2D ì¶”ê°€, compute_gradient_loss ì¶”ê°€ |
| `supervised_loss.py` | get_loss_func íŒŒë¼ë¯¸í„° ì „ë‹¬ |
| `*.yaml` | gradient_weight, gradient_scales ì¶”ê°€ |

### 10.3 ë‹¤ìŒ ë‹¨ê³„

1. **êµ¬í˜„**: ìœ„ ì„¤ê³„ëŒ€ë¡œ ì½”ë“œ ìˆ˜ì •
2. **í…ŒìŠ¤íŠ¸**: Dry-run ë° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
3. **ì‹¤í—˜**: Baseline vs +Gradient ë¹„êµ
4. **íŠœë‹**: ìµœì  weight ì¡°í•© íƒìƒ‰

---

## 11. PM ê´€ì  ì½”ë“œ ë¦¬ë·° (2024-12-18)

### 11.1 êµ¬í˜„ ê°€ëŠ¥ì„± í‰ê°€: âœ… **í•œ ë²ˆì— êµ¬í˜„ ê°€ëŠ¥**

ì „ì²´ ì½”ë“œë² ì´ìŠ¤ë¥¼ ê²€í† í•œ ê²°ê³¼, ì œì•ˆëœ êµ¬í˜„ ì „ëµì€ **ì •í™•í•˜ê³  ì‹¤í˜„ ê°€ëŠ¥**í•©ë‹ˆë‹¤.

### 11.2 ì½”ë“œ íë¦„ ê²€ì¦

```
YAML Config
    â†“
configs/default_config.py (ê¸°ë³¸ê°’ ì •ì˜)
    â†“
model_wrapper.py:setup_model()
    â†’ model_args = {**config.loss}  # loss configë¥¼ dictë¡œ ë³€í™˜
    â†’ model = SemiSupCompletionModel(**model_args)
    â†“
SemiSupCompletionModel.__init__(**kwargs)
    â†’ SupervisedLoss(min_depth=..., max_depth=..., **kwargs)
    â†“
SupervisedLoss.__init__(**kwargs)
    â†’ get_loss_func(supervised_method, **kwargs)
    â†“
get_loss_func() ë‚´ë¶€
    â†’ SSISilogLoss(
        ssi_weight=kwargs.get('ssi_weight', 0.7),
        silog_weight=kwargs.get('silog_weight', 0.3),
        gradient_weight=kwargs.get('gradient_weight', 0.0),  # ğŸ†• ì¶”ê°€í•  ë¶€ë¶„
        ...
      )
```

**ê²°ë¡ **: YAML â†’ Model â†’ Lossê¹Œì§€ `**kwargs` ì²´ì¸ì´ ì •í™•íˆ ì—°ê²°ë˜ì–´ ìˆì–´, YAMLì— `gradient_weight` ì¶”ê°€ë§Œìœ¼ë¡œ ìë™ ì „ë‹¬ë©ë‹ˆë‹¤.

### 11.3 ìˆ˜ì • íŒŒì¼ ìµœì¢… í™•ì •

| íŒŒì¼ | ìˆ˜ì • ë‚´ìš© | ë‚œì´ë„ | ë¦¬ìŠ¤í¬ |
|------|----------|--------|--------|
| `ssi_silog_loss.py` | Gradient2D í´ë˜ìŠ¤ ì¶”ê°€, compute_gradient_loss ë©”ì„œë“œ ì¶”ê°€, forward() ìˆ˜ì • | ì¤‘ | ë‚®ìŒ |
| `supervised_loss.py` | get_loss_func()ì— gradient_weight, gradient_scales íŒŒë¼ë¯¸í„° ì¶”ê°€ | í•˜ | ë§¤ìš° ë‚®ìŒ |
| `default_config.py` | gradient_weight, gradient_scales ê¸°ë³¸ê°’ ì¶”ê°€ | í•˜ | ë§¤ìš° ë‚®ìŒ |
| `train_*.yaml` | gradient_weight, gradient_scales ì„¤ì • ì¶”ê°€ | í•˜ | ì—†ìŒ |

### 11.4 ë°œê²¬ëœ ê³ ë ¤ì‚¬í•­

#### âœ… í™•ì¸ë¨: import ì¶”ê°€ í•„ìš”
```python
# ssi_silog_loss.py ìƒë‹¨ì— ì¶”ê°€ í•„ìš”
import torch.nn.functional as F  # F.conv2d, F.interpolate ì‚¬ìš©
from typing import Tuple  # Gradient2D ë°˜í™˜ íƒ€ì…
```

#### âœ… í™•ì¸ë¨: register_buffer ì‚¬ìš©
Gradient2D í´ë˜ìŠ¤ì—ì„œ `register_buffer` ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ device ì´ë™ì´ ì²˜ë¦¬ë¨.
`.cuda()` í˜¸ì¶œ ë¶ˆí•„ìš” (G2-MonoDepthì™€ ë‹¤ë¥¸ ì )

#### âœ… í™•ì¸ë¨: í•˜ìœ„ í˜¸í™˜ì„±
- `gradient_weight=0.0` ê¸°ë³¸ê°’ìœ¼ë¡œ ê¸°ì¡´ ë™ì‘ê³¼ 100% ë™ì¼
- ê¸°ì¡´ YAML config ìˆ˜ì • ì—†ì´ë„ ë™ì‘

#### âš ï¸ ì£¼ì˜: Multi-scale ì‹œ mask í¬ê¸°
```python
# gradient outputì€ H-2, W-2ì´ë¯€ë¡œ mask ì¡°ì • í•„ìš”
mask_grad = mask_s[:, :, 1:-1, 1:-1]  # ì „ëµ ë¬¸ì„œì— ì´ë¯¸ í¬í•¨ë¨ âœ“
```

### 11.5 í…ŒìŠ¤íŠ¸ ì „ëµ

#### Phase 1: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (êµ¬í˜„ ì§í›„)
```python
# í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰
python -c "
import torch
from packnet_sfm.losses.ssi_silog_loss import Gradient2D, SSISilogLoss

# Gradient2D í…ŒìŠ¤íŠ¸
grad_fn = Gradient2D()
x = torch.randn(2, 1, 64, 64)
gx, gy = grad_fn(x)
print(f'Gradient2D output: gx={gx.shape}, gy={gy.shape}')
assert gx.shape == (2, 1, 62, 62), 'Gradient2D shape mismatch'

# SSISilogLoss with gradient í…ŒìŠ¤íŠ¸
loss_fn = SSISilogLoss(gradient_weight=0.2, gradient_scales=4)
pred = torch.rand(2, 1, 64, 64) * 0.1 + 0.01
gt = torch.rand(2, 1, 64, 64) * 0.1 + 0.01
mask = torch.ones(2, 1, 64, 64, dtype=torch.bool)
loss = loss_fn(pred, gt, mask)
print(f'SSISilogLoss with gradient: {loss.item():.6f}')
print('âœ… All tests passed!')
"
```

#### Phase 2: í†µí•© í…ŒìŠ¤íŠ¸ (1 epoch)
```bash
python scripts/core/train.py \
    configs/train_resnet_san_ncdb_distance_dual_head_640x384.yaml \
    --arch.max_epochs 1
```

### 11.6 ì˜ˆìƒ ì†Œìš” ì‹œê°„

| ë‹¨ê³„ | ì˜ˆìƒ ì‹œê°„ |
|------|----------|
| ssi_silog_loss.py ìˆ˜ì • | 15ë¶„ |
| supervised_loss.py ìˆ˜ì • | 5ë¶„ |
| default_config.py ìˆ˜ì • | 3ë¶„ |
| YAML config ìˆ˜ì • | 2ë¶„ |
| ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ | 5ë¶„ |
| í†µí•© í…ŒìŠ¤íŠ¸ | 10ë¶„ |
| **ì´ê³„** | **~40ë¶„** |

### 11.7 ìµœì¢… ê²°ë¡ 

```
âœ… êµ¬í˜„ ê°€ëŠ¥ì„±: 100% í™•ì¸
âœ… í•˜ìœ„ í˜¸í™˜ì„±: ë³´ì¥ë¨ (ê¸°ë³¸ê°’ 0.0)
âœ… ì½”ë“œ íë¦„: ê²€ì¦ë¨ (kwargs ì²´ì¸ ì •ìƒ)
âœ… ë¦¬ìŠ¤í¬: ë‚®ìŒ
âœ… ì˜ˆìƒ ì‹œê°„: 40ë¶„ ì´ë‚´

ğŸ‘‰ êµ¬í˜„ ì§„í–‰ ê¶Œì¥
```
