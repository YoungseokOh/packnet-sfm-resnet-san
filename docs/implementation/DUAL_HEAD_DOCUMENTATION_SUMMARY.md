# Dual-Head Implementation Documentation Summary

## üìö Available Documentation

### 1. **DUAL_HEAD_OUTPUT_STRUCTURE.md** (Primary Technical Reference)
- **Î™©Ï†Å**: Complete technical specification of Dual-Head architecture
- **ÎÇ¥Ïö©**:
  - PyTorch checkpoint output format (tuple keys, multi-scale)
  - Integer and Fractional head design
  - Loss function implementation
  - Training pipeline integration
  - Backward compatibility with Single-Head

### 2. **DUAL_HEAD_OUTPUT_SUMMARY.md** (Quick Reference)
- **Î™©Ï†Å**: Quick lookup guide for developers
- **ÎÇ¥Ïö©**:
  - One-page cheat sheet
  - Key formulas and ranges
  - Common usage patterns
  - Troubleshooting guide

### 3. **DUAL_HEAD_ONNX_CONVERSION.md** (Deployment Guide)
- **Î™©Ï†Å**: ONNX conversion and deployment reference
- **ÎÇ¥Ïö©**:
  - ‚≠ê **PyTorch Checkpoint vs ONNX output comparison**
  - Two ONNX models (composed vs separate outputs)
  - Validation results (FP32 accuracy)
  - ‚≠ê **INT8 quantization workflow for NPU**
  - Usage examples and conversion scripts

### 4. **DUAL_HEAD_SAVE_REPORT.md** (Data Archival)
- **Î™©Ï†Å**: NPZ file saving and verification report
- **ÎÇ¥Ïö©**:
  - 91 test samples saved as NPZ files
  - Integer/Fractional/Composed depth archival
  - Reconstruction accuracy verification

---

## üéØ ÌïµÏã¨ ÎÇ¥Ïö© ÏöîÏïΩ

### Checkpoint Output Structure (Training)

```python
# PyTorch checkpoint inference
outputs = depth_net(rgb)

# Output format: Dict with tuple keys
{
    ('integer', 0): Tensor[1, 1, 384, 640],    # Full resolution
    ('fractional', 0): Tensor[1, 1, 384, 640],
    ('integer', 1): Tensor[1, 1, 192, 320],    # Half resolution
    ('fractional', 1): Tensor[1, 1, 192, 320],
    ('integer', 2): Tensor[1, 1, 96, 160],     # Quarter resolution
    ('fractional', 2): Tensor[1, 1, 96, 160],
    ('integer', 3): Tensor[1, 1, 48, 80],      # 1/8 resolution
    ('fractional', 3): Tensor[1, 1, 48, 80],
}

# Depth composition (manual)
integer_sig = outputs[('integer', 0)]
fractional_sig = outputs[('fractional', 0)]
depth = integer_sig * 15.0 + fractional_sig
```

**ÌäπÏßï**:
- ‚úÖ Multi-scale outputs (4 scales)
- ‚úÖ Tuple keys for type safety
- ‚ùå No pre-composed depth
- üéØ Purpose: Multi-scale supervision during training

---

### ONNX Output Structure (Inference)

#### Option 1: Separate Outputs (Recommended for NPU)

```python
# ONNX separate outputs inference
outputs = onnx_session.run(None, {'rgb': image})

integer_sigmoid = outputs[0]      # [1, 1, 384, 640]
fractional_sigmoid = outputs[1]   # [1, 1, 384, 640]
depth_composed = outputs[2]       # [1, 1, 384, 640] - pre-calculated ‚úÖ
```

**ÌäπÏßï**:
- ‚úÖ Single scale (full resolution only)
- ‚úÖ Pre-composed depth provided
- ‚úÖ Separate outputs for quantization analysis
- üéØ Purpose: NPU deployment with INT8 quantization

#### Option 2: Composed Output (Simple Deployment)

```python
# ONNX composed output inference
outputs = onnx_session.run(None, {'rgb': image})

depth = outputs[0]  # [1, 1, 384, 640] - final depth only
```

**ÌäπÏßï**:
- ‚úÖ Single output (simplest)
- ‚úÖ Pre-composed depth
- ‚ùå No access to integer/fractional components
- üéØ Purpose: Simple production deployment

---

## üîß INT8 Quantization Workflow

### Why Use Separate Outputs ONNX?

1. **Per-head error analysis**
   - Integer head: High sensitivity (15√ó amplification)
   - Fractional head: Low sensitivity (1√ó amplification)

2. **Quantization impact tracking**
   ```
   Œî_depth = Œî_integer √ó 15.0 + Œî_fractional
              ‚Üë                 ‚Üë
         Major contributor   Minor contributor
   ```

3. **Independent optimization**
   - Different calibration methods per head
   - Per-head quantization range tuning

### Recommended Workflow

```bash
# Step 1: Convert to separate outputs ONNX
python scripts/convert_dual_head_to_onnx.py \
    --checkpoint checkpoints/.../epoch=28_..._val-loss=0.000.ckpt \
    --separate_outputs \
    --input_shape 384 640 \
    --max_depth 15.0

# Step 2: Save FP32 reference (all 3 outputs per image)
python scripts/save_fp32_references.py \
    --onnx onnx/dual_head_..._separate_zero.onnx \
    --output_dir outputs/fp32_reference

# Step 3: Convert to INT8 using NPU toolkit
your_npu_converter --input onnx/dual_head_..._separate_zero.onnx

# Step 4: Run INT8 inference (save all 3 outputs per image)
your_npu_runner --model model_int8.bin

# Step 5: Compare FP32 vs INT8
python scripts/compare_fp32_int8.py \
    --fp32_dir outputs/fp32_reference \
    --int8_dir outputs/int8_inference
```

### What to Save for Comparison

```python
# For each test image:

# FP32 ONNX
fp32_outputs = onnx_session.run(None, {'rgb': image})
np.savez(f'{sample_id}_fp32.npz',
         integer_sigmoid=fp32_outputs[0],
         fractional_sigmoid=fp32_outputs[1],
         depth_composed=fp32_outputs[2])

# INT8 NPU
int8_outputs = npu_inference(image)
np.savez(f'{sample_id}_int8.npz',
         integer_sigmoid=int8_outputs[0],
         fractional_sigmoid=int8_outputs[1],
         depth_composed=int8_outputs[2])
```

### Analysis Metrics

```python
fp32 = np.load(f'{sample_id}_fp32.npz')
int8 = np.load(f'{sample_id}_int8.npz')

# Per-head errors
int_error = np.abs(fp32['integer_sigmoid'] - int8['integer_sigmoid'])
frac_error = np.abs(fp32['fractional_sigmoid'] - int8['fractional_sigmoid'])
depth_error = np.abs(fp32['depth_composed'] - int8['depth_composed'])

# Error contribution
int_contribution = int_error.mean() * 15.0
frac_contribution = frac_error.mean()

print(f"Integer contribution: {int_contribution/depth_error.mean()*100:.1f}%")
print(f"Fractional contribution: {frac_contribution/depth_error.mean()*100:.1f}%")
```

---

## üìä Validation Results

### FP32 Accuracy (ONNX vs PyTorch)

**Separate outputs model**:
```
Integer sigmoid:    error < 1e-6  ‚úÖ
Fractional sigmoid: error < 1e-5  ‚úÖ
Composed depth:     error < 1e-5  ‚úÖ
Composition check:  error = 0     ‚úÖ
```

**Composed output model**:
```
Composed depth:     error < 1e-5  ‚úÖ
```

### Real KITTI Images Tested

- Image 1 (0000000147.png): ‚úÖ Perfect match
- Image 2 (0000000655.png): ‚úÖ Perfect match
- Random samples (10 tests): ‚úÖ All passed

---

## üöÄ Quick Start Guide

### For Training
```python
from packnet_sfm.networks.depth.ResNetSAN01 import ResNetSAN01

model = ResNetSAN01(use_dual_head=True, version='18A')
outputs = model(rgb)

# Access full resolution outputs
int_sig = outputs[('integer', 0)]
frac_sig = outputs[('fractional', 0)]
depth = int_sig * 15.0 + frac_sig
```

### For Inference (PyTorch)
```python
checkpoint = torch.load('epoch=28_..._val-loss=0.000.ckpt')
# ... load model ...
outputs = model(rgb)
depth = outputs[('integer', 0)] * 15.0 + outputs[('fractional', 0)]
```

### For Inference (ONNX)
```python
import onnxruntime as ort

session = ort.InferenceSession('dual_head_..._separate_zero.onnx')
outputs = session.run(None, {'rgb': image})

integer_sig = outputs[0]
fractional_sig = outputs[1]
depth = outputs[2]  # Pre-calculated ‚úÖ
```

### For INT8 NPU Deployment
```python
# Use separate outputs ONNX
# Save all 3 outputs from both FP32 and INT8
# Compare per-head errors to optimize quantization
```

---

## ‚úÖ Checklist for Documentation Review

- [x] Checkpoint output structure explained (tuple keys, multi-scale)
- [x] ONNX output structure explained (array outputs, single scale)
- [x] Difference between checkpoint and ONNX clearly documented
- [x] INT8 quantization workflow provided
- [x] Separate outputs recommendation for NPU deployment
- [x] Per-head error analysis methodology explained
- [x] Code examples for all use cases provided
- [x] Validation results documented
- [x] Real image testing completed

---

## üìû Î¨∏ÏÑú ÌôúÏö© Í∞ÄÏù¥Îìú

### NPU ÏóÖÏ≤¥Ïóê Ï†ÑÎã¨Ìï† Î¨∏ÏÑú
1. **DUAL_HEAD_ONNX_CONVERSION.md** - Ï£ºÏöî Î¨∏ÏÑú
   - PyTorch vs ONNX Ï∞®Ïù¥Ï†ê
   - INT8 ÏñëÏûêÌôî ÏõåÌÅ¨ÌîåÎ°úÏö∞
   - 3Í∞ú Ï∂úÎ†• Ï†ÄÏû• Î∞©Î≤ï
   - Ïò§Ï∞® Î∂ÑÏÑù Î∞©Î≤ï

2. **DUAL_HEAD_OUTPUT_SUMMARY.md** - Quick reference
   - Îπ†Î•∏ Ï∞∏Ï°∞Ïö©

### ÎÇ¥Î∂Ä Í∞úÎ∞úÌåÄ Ï∞∏Í≥† Î¨∏ÏÑú
1. **DUAL_HEAD_OUTPUT_STRUCTURE.md** - Ï†ÑÏ≤¥ Í∏∞Ïà† ÏÇ¨Ïñë
2. **DUAL_HEAD_SAVE_REPORT.md** - Îç∞Ïù¥ÌÑ∞ ÏïÑÏπ¥Ïù¥Î∏å Î¶¨Ìè¨Ìä∏

---

## üéØ Í≤∞Î°†

‚úÖ **Î™®Îì† ÌïµÏã¨ ÎÇ¥Ïö©Ïù¥ Î¨∏ÏÑúÌôîÎêòÏñ¥ ÏûàÏäµÎãàÎã§**:
- CheckpointÏùò tuple key Íµ¨Ï°∞ÏôÄ multi-scale Ï∂úÎ†•
- ONNXÏùò array Íµ¨Ï°∞ÏôÄ single-scale Ï∂úÎ†•
- INT8 ÏñëÏûêÌôîÎ•º ÏúÑÌïú separate outputs ÏÇ¨Ïö©Î≤ï
- FP32 vs INT8 ÎπÑÍµêÎ•º ÏúÑÌïú 3Í∞ú Ï∂úÎ†• Ï†ÄÏû• Î∞©Î≤ï
- Per-head Ïò§Ï∞® Î∂ÑÏÑù Î∞©Î≤ïÎ°†

NPU ÏóÖÏ≤¥Ïóê **DUAL_HEAD_ONNX_CONVERSION.md**Î•º Ï†ÑÎã¨ÌïòÏãúÎ©¥ Îê©ÎãàÎã§!
