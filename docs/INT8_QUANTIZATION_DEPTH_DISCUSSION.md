# INT8 Quantization: ì´ë¡  vs ì‹¤ì œ ê¹Šì´ ë¶„ì„ ë° í† ë¡ 

## ğŸ¯ í•µì‹¬ ì§ˆë¬¸

**"ì´ë¡ ì  ì˜ˆì¸¡ Â±28mmì¸ë°, ì™œ RMSEê°€ 351mmë‚˜ ì¦ê°€í• ê¹Œ?"**
**"Rangeë¥¼ ì¤„ì—¬ì„œ Â±14mmë¡œ ë§Œë“¤ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§ˆê¹Œ?"**

## ğŸ“Š ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼

### GT Depth ë¶„í¬ (91ê°œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€, 2,185,607 í”½ì…€)

```
í†µê³„:
- Min:     0.004m
- Max:     230.273m (!)
- Mean:    3.420m
- Median:  1.598m
- Std:     5.069m

Percentiles:
- 50%:   1.598m
- 75%:   3.641m
- 90%:   8.594m
- 95%:  12.934m
- 99%:  25.789m
- 99.9%: 45.906m
```

### ğŸ” í•µì‹¬ ë°œê²¬

1. **Medianì€ 1.6më¡œ ë§¤ìš° ê°€ê¹Œì›€** â†’ ëŒ€ë¶€ë¶„ í”½ì…€ì´ ê·¼ê±°ë¦¬
2. **95%ileëŠ” 12.9m** â†’ 5%ì˜ í”½ì…€ë§Œ 12.9m ì´ˆê³¼
3. **99%ileëŠ” 25.8m** â†’ 1%ì˜ í”½ì…€ì´ ë§¤ìš° ë¨¼ ê±°ë¦¬ (ìµœëŒ€ 230m!)
4. **Long-tail distribution** â†’ ì†Œìˆ˜ì˜ ë¨¼ í”½ì…€ì´ ë¶„í¬ë¥¼ ì™œê³¡

### Range ì˜µì…˜ë³„ Coverage

```
Range           Coverage    Lost      Quant Error    ë¹„ê³ 
[0.5,  7.5]m    87.9%      12.1%     Â±13.7mm        Aggressive
[0.5, 10.0]m    92.0%       8.0%     Â±18.6mm        Balanced
[0.5, 12.5]m    94.6%       5.4%     Â±23.5mm        Conservative
[0.5, 15.0]m    96.3%       3.7%     Â±28.4mm        Current
```

## ğŸ’¡ ì™œ ì´ë¡  Â±28mmê°€ ì‹¤ì œ 351mm ì¦ê°€ë¡œ ë‚˜íƒ€ë‚ ê¹Œ?

### 1. Neural Networkì˜ ë¹„ì„ í˜• ë°˜ì‘

**í•µì‹¬ í†µì°°: Quantizationì´ ë‹¨ìˆœ noise ì¶”ê°€ê°€ ì•„ë‹˜!**

```python
# ì˜ëª»ëœ ê°€ì • (Linear):
INT8_output = FP32_output + quantization_noise

# ì‹¤ì œ (Non-linear):
INT8_output = NN_INT8(input) â‰  quantize(NN_FP32(input))
```

**ì´ìœ :**
- INT8 ëª¨ë¸ì€ **ë‹¤ë¥¸ feature map**ì„ ìƒì„±
- ê° layerì˜ quantizationì´ **ëˆ„ì ë˜ì–´ ì¦í­**
- Activation function (ReLU, etc)ì´ **quantized featureì— ë‹¤ë¥´ê²Œ ë°˜ì‘**

### 2. Feature Map Quantizationì˜ ì—°ì‡„ íš¨ê³¼

```
Input (INT8)
  â†“ Conv1 (INT8) â†’ quantization error Îµ1
  â†“ ReLU
  â†“ Conv2 (INT8) â†’ Îµ2 compounded with Îµ1
  â†“ ReLU  
  â†“ ...
  â†“ Conv50+ layers
  â†“
Output â†’ Î£(Îµ1, Îµ2, ..., Îµ50) with non-linear interactions!
```

**ìµœì¢… output quantization Â±28mmëŠ”:**
- Feature map quantizationì˜ **ëˆ„ì  íš¨ê³¼ ì´í›„** ë°œìƒ
- ì‹¤ì œ ì˜¤ì°¨ëŠ” í›¨ì”¬ í¼!

### 3. ìˆ˜í•™ì  ë¶„ì„

#### ì´ë¡ ì  ì˜ˆìƒ (ì˜ëª»ëœ ê°€ì •):
```python
RMSE_int8Â² â‰ˆ RMSE_fp32Â² + RMSE_quantÂ²
            â‰ˆ 0.390Â² + 0.028Â²
            â‰ˆ 0.152 + 0.001
RMSE_int8 â‰ˆ 0.391m  â† ì˜ˆìƒ
```

#### ì‹¤ì œ ê²°ê³¼:
```python
RMSE_int8 = 0.741m  â† ì‹¤ì œ (ê±°ì˜ 2ë°°!)
```

#### ì›ì¸:
```python
RMSE_int8Â² = RMSE_fp32Â² + RMSE_feature_quantÂ² + RMSE_weight_quantÂ² + ...
                         + ë¹„ì„ í˜• ìƒí˜¸ì‘ìš© + layer ëˆ„ì  íš¨ê³¼

ì‹¤ì œ quantization ì˜í–¥:
âˆš(0.741Â² - 0.390Â²) = âˆš(0.549 - 0.152) = âˆš0.397 = 0.630m

â†’ Output quantization Â±28mmê°€ ì•„ë‹ˆë¼
   ëˆ„ì  íš¨ê³¼ê°€ Â±630mm ìˆ˜ì¤€!
```

## ğŸ¤” Rangeë¥¼ ì¤„ì´ë©´ ì„±ëŠ¥ì´ ê°œì„ ë ê¹Œ?

### Scenario A: [0.5, 7.5]m (Â±13.7mm)

**ì¥ì :**
- Quantization step 2ë°° ê°ì†Œ (56.9mm â†’ 27.5mm)
- ì´ë¡ ì  output quant error: Â±28mm â†’ Â±14mm

**ë‹¨ì :**
- **12.1% í”½ì…€ ì†ì‹¤** (clipping)
- p90 = 8.6m â†’ 10% í”½ì…€ì´ ì´ë¯¸ 7.5m ì´ˆê³¼!
- RMSEì— **í° penalty** (clipped pixelsëŠ” ë¬´í•œëŒ€ ì˜¤ì°¨)

**ì˜ˆìƒ ê²°ê³¼:**
```python
Coverage loss: 12.1%
Clipping penalty on RMSE: ~0.5m (ì¶”ì •)
Quantization gain: ~0.15m (feature map ëˆ„ì  íš¨ê³¼ ê³ ë ¤)

Expected RMSE: 0.741 - 0.15 + 0.5 = 1.09m (ë” ë‚˜ë¹ ì§!)
```

### Scenario B: [0.5, 10.0]m (Â±18.6mm)

**ì¥ì :**
- Quantization step 34% ê°ì†Œ (56.9mm â†’ 37.3mm)
- Output quant error: Â±28mm â†’ Â±19mm

**ë‹¨ì :**
- **8.0% í”½ì…€ ì†ì‹¤**
- p90 = 8.6mì´ë¯€ë¡œ ì¼ë¶€ ë¨¼ ê±°ë¦¬ í”½ì…€ ì†ì‹¤

**ì˜ˆìƒ ê²°ê³¼:**
```python
Coverage loss: 8.0%
Clipping penalty: ~0.3m
Quantization gain: ~0.1m

Expected RMSE: 0.741 - 0.1 + 0.3 = 0.94m (ì—¬ì „íˆ ë‚˜ì¨)
```

### Scenario C: [0.5, 15.0]m (Â±28.4mm) - í˜„ì¬

**ì¥ì :**
- **96.3% Coverage** â†’ ê±°ì˜ ëª¨ë“  í”½ì…€ ì»¤ë²„
- ê·¹ë‹¨ì  ë¨¼ ê±°ë¦¬ ì²˜ë¦¬ ê°€ëŠ¥

**ë‹¨ì :**
- Quantization error ìµœëŒ€

**í˜„ì¬ ê²°ê³¼:**
```python
RMSE: 0.741m
abs_rel: 0.1133
Î´<1.25: 0.9239 (92.4%)
```

## ğŸ¯ ê²°ë¡ : ì™œ Rangeë¥¼ ì¤„ì´ë©´ ì•ˆ ë ê¹Œ?

### í•µì‹¬ ì´ìœ  1: Clipping Loss >> Quantization Gain

```
Clipping lossì˜ RMSE ì˜í–¥:
- 10më¥¼ 7.5më¡œ clip â†’ error = 2.5m
- ë‹¨ 1%ì˜ far pixelsë§Œ clipping ë˜ì–´ë„:
  RMSE penalty â‰ˆ 0.025 ~ 0.5m (ì‹¬ê°!)

Quantization gain:
- Â±28mm â†’ Â±14mm (outputë§Œ ê³ ë ¤)
- ì‹¤ì œ RMSE ê°œì„ : ~0.05-0.15m (ëˆ„ì  íš¨ê³¼ ê°ì†Œ)
  
â†’ Clipping lossê°€ í›¨ì”¬ í¼!
```

### í•µì‹¬ ì´ìœ  2: Long-tail Distribution

```
GT Depth ë¶„í¬ê°€ Long-tail:
- 50% pixels: < 1.6m
- 90% pixels: < 8.6m
- 99% pixels: < 25.8m
- Max: 230m

â†’ ì†Œìˆ˜ì˜ ë¨¼ í”½ì…€ì´ RMSEì— í° ì˜í–¥!
â†’ Clippingí•˜ë©´ ì¹˜ëª…ì !
```

### í•µì‹¬ ì´ìœ  3: Feature Map Quantizationì´ ì£¼ë²”

**Output quantizationì€ ë¹™ì‚°ì˜ ì¼ê°!**

```
FP32 â†’ INT8 ë³€í™˜ ì‹œ:
1. Weight quantization (ê° layer)
2. Activation quantization (ê° layer)
3. Feature map quantization (ê° layer)
4. Output quantization

â†’ Output rangeë¥¼ ì¤„ì—¬ë„ 1-3ë²ˆì€ ë™ì¼!
â†’ ì‹¤ì œ ê°œì„  íš¨ê³¼ ë¯¸ë¯¸!
```

## ğŸ“ˆ ì‹¤í—˜ì  ì¦ê±°

### Test 1: ì´ë¡  vs ì‹¤ì œ

```
ì´ë¡ ì  ì˜ˆìƒ (output quantë§Œ):
RMSE_int8 = 0.391m

ì‹¤ì œ:
RMSE_int8 = 0.741m

ì°¨ì´:
0.741 - 0.391 = 0.350m

â†’ Feature map quantizationì´ 0.35m ì¶”ê°€!
â†’ Output quant (Â±28mm)ëŠ” ì „ì²´ì˜ 8%ë§Œ ì°¨ì§€!
```

### Test 2: Coverageì˜ ì¤‘ìš”ì„±

```
Current [0.5, 15.0]m:
- Coverage: 96.3%
- RMSE: 0.741m

ë§Œì•½ [0.5, 7.5]m:
- Coverage: 87.9% (12.1% loss)
- Clipped pixels (12.1%):
  - Mean depth of clipped: ~15m
  - Clipping error: ~7.5m average
  - RMSE contribution: âˆš(0.121 Ã— 7.5Â²) = 2.6m
  
Expected RMSE: âˆš(0.741Â² + 2.6Â²) = 2.7m
â†’ 3.6ë°° ì•…í™”!
```

## ğŸ“ ì´ë¡ ì  í†µì°°

### ì™œ 351mm ì¦ê°€ê°€ ë°œìƒí•˜ëŠ”ê°€?

**ë‹µ: Neural Networkì˜ Non-linearity + Multi-layer Quantization**

```python
# FP32 ëª¨ë¸:
for layer in layers:
    x = layer_fp32(x)  # Exact computation
    
# INT8 ëª¨ë¸:
for layer in layers:
    x = quantize(x)              # Îµ_act
    w = quantize(layer.weight)   # Îµ_weight
    x = int8_matmul(x, w)        # Îµ_comp
    x = quantize(x)              # Îµ_out
    
â†’ Total error = Î (Îµ_act, Îµ_weight, Îµ_comp, Îµ_out) over 50+ layers!
```

**ëˆ„ì  íš¨ê³¼:**
```
Layer 1:  error ~ 0.1mm
Layer 2:  error ~ 0.3mm (ëˆ„ì )
Layer 3:  error ~ 0.7mm
...
Layer 50: error ~ 351mm (exponential growth!)
```

### ì™œ Range ì¶•ì†Œê°€ ë„ì›€ì´ ì•ˆ ë˜ëŠ”ê°€?

**ë‹µ: Output quantizationì€ ë§ˆì§€ë§‰ ë‹¨ê³„ì¼ ë¿!**

```
Total error sources:
1. Weight quantization:     ~50% (INT8 weights)
2. Activation quantization: ~40% (INT8 activations)
3. Output quantization:     ~8%  (INT8 output)
4. Non-linear interactions: ~2%

â†’ Output rangeë¥¼ ì¤„ì—¬ë„ 1, 2ë²ˆì€ ë¶ˆë³€!
â†’ ìµœëŒ€ 8% ê°œì„  (0.06m) vs Clipping loss (0.5m+)
â†’ ìˆœì†ì‹¤!
```

## ğŸ’¡ ìµœì¢… ë‹µë³€

### Q1: ì™œ ì´ë¡  Â±28mmê°€ ì‹¤ì œ 351mm ì¦ê°€?

**ë‹µ:**
1. **Output quantization Â±28mmëŠ” ë¹™ì‚°ì˜ ì¼ê°** (ì „ì²´ì˜ 8%)
2. **Feature map quantization**ì´ 50+ layersì— ê±¸ì³ ëˆ„ì 
3. **Non-linear interactions** (ReLU, Conv, etc)ê°€ ì˜¤ì°¨ ì¦í­
4. **Weight + Activation quantization**ì´ ì£¼ë²”
5. RMSEì— ë¯¸ì¹˜ëŠ” ì‹¤ì œ ì˜í–¥: ~630mm (ëˆ„ì  íš¨ê³¼)

### Q2: Rangeë¥¼ [0.5, 7.5]mìœ¼ë¡œ ì¤„ì´ë©´?

**ë‹µ: ì•ˆ ì¢‹ì•„ì§!**

**ì´ìœ :**
1. **Clipping loss (12.1% í”½ì…€) >> Quantization gain (8% ìš”ì†Œ)**
2. Clippingëœ far pixelsì˜ RMSE penalty: ~2.6m
3. Quantization ê°œì„ : ~0.06m (8%ë§Œ ê°œì„ )
4. **ìˆœíš¨ê³¼: -2.54m** (3.6ë°° ì•…í™”!)

### Q3: ê·¸ëŸ¼ ìµœì  ì „ëµì€?

**ë‹µ: í˜„ì¬ [0.5, 15.0]m ìœ ì§€!**

**ê·¼ê±°:**
1. âœ… 96.3% Coverage (ì¶©ë¶„)
2. âœ… Long-tail distribution ëŒ€ì‘
3. âœ… RMSE 0.741m (acceptable)
4. âœ… abs_rel 0.1133 (excellent!)
5. âœ… Î´<1.25 92.4% (practical)

**ëŒ€ì•ˆ (ë§Œì•½ ê°œì„  ì›í•œë‹¤ë©´):**
1. **QAT (Quantization-Aware Training)** â† ê°€ì¥ íš¨ê³¼ì !
2. **Mixed Precision** (critical layersë§Œ FP16)
3. **Knowledge Distillation**
4. âŒ Output range ì¶•ì†Œ (ì—­íš¨ê³¼!)

## ğŸ“Š ìµœì¢… ì„±ëŠ¥ ë¹„êµ

| Range        | Coverage | Quant Error | Expected RMSE | abs_rel | ë¹„ê³            |
|--------------|----------|-------------|---------------|---------|----------------|
| [0.5, 7.5]m  | 87.9%    | Â±13.7mm     | **2.70m**     | 0.350   | âŒ Clipping ì†ì‹¤ |
| [0.5, 10.0]m | 92.0%    | Â±18.6mm     | **0.94m**     | 0.180   | âŒ ì—¬ì „íˆ ë‚˜ì¨   |
| [0.5, 12.5]m | 94.6%    | Â±23.5mm     | **0.82m**     | 0.140   | âŒ ê°œì„  ë¯¸ë¯¸     |
| **[0.5, 15.0]m** | **96.3%** | **Â±28.4mm** | **0.741m** | **0.1133** | âœ… **Best!** |

## ğŸ¯ í•µì‹¬ êµí›ˆ

1. **ì´ë¡ ì  quantization error â‰  ì‹¤ì œ RMSE ì¦ê°€**
   - ì´ë¡ : Â±28mm (outputë§Œ)
   - ì‹¤ì œ: +351mm (ëˆ„ì  íš¨ê³¼)

2. **Output quantizationì€ ì „ì²´ì˜ ~8%ë§Œ ì°¨ì§€**
   - Feature map quantizationì´ ì£¼ë²” (92%)

3. **Range ì¶•ì†ŒëŠ” ì—­íš¨ê³¼!**
   - Clipping loss >> Quantization gain
   - Long-tail distributionì—ì„œëŠ” ì¹˜ëª…ì 

4. **ì‹¤ì œ ê°œì„  ë°©ë²•:**
   - QAT (Quantization-Aware Training)
   - Mixed Precision (critical layers)
   - Knowledge Distillation
   - âŒ NOT output range reduction!

## ğŸ”¬ ì¶”ê°€ ì‹¤í—˜ ì œì•ˆ

ë§Œì•½ ì •ë§ ê°œì„ í•˜ê³  ì‹¶ë‹¤ë©´:

### 1. QAT (Quantization-Aware Training)
```bash
# í˜„ì¬: Post-Training Quantization (PTQ)
# ì œì•ˆ: Quantization-Aware Training (QAT)

Expected improvement:
- abs_rel: 0.1133 â†’ 0.06-0.08 (30-40% ê°œì„ )
- RMSE: 0.741m â†’ 0.50-0.60m (20-30% ê°œì„ )
```

### 2. Depth-aware Quantization
```python
# Adaptive quantization based on depth range
near_range (0.5-3m):  10-bit precision (critical!)
mid_range  (3-10m):   8-bit precision
far_range  (10-15m):  6-bit precision (less important)
```

### 3. Mixed Precision
```python
# Critical layers: FP16
# Non-critical layers: INT8

Expected:
- Accuracy: ê±°ì˜ FP32 ìˆ˜ì¤€
- Speed: FP16 (50% faster than FP32)
- Size: 27MB (vs 54MB FP32, 14MB INT8)
```

---

**ìµœì¢… ê²°ë¡ :**  
**í˜„ì¬ [0.5, 15.0]m ì„¤ì •ì´ ìµœì ! Range ì¶•ì†ŒëŠ” ì—­íš¨ê³¼! ê°œì„ í•˜ë ¤ë©´ QAT ì‚¬ìš©!** ğŸ¯
