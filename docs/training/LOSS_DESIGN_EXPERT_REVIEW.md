# ğŸ”¬ ê·¼ê±°ë¦¬ ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜ ì„¤ê³„ ì‹¬ì¸µ ë¶„ì„

## âš ï¸ Executive Summary

í˜„ì¬ `SSISilogNearFieldLoss` êµ¬í˜„ì€ **ê¸°ë³¸ ê°œë…ì€ íƒ€ë‹¹í•˜ì§€ë§Œ ì—¬ëŸ¬ ì¤‘ìš”í•œ ë¬¸ì œì ì´ ì¡´ì¬**í•©ë‹ˆë‹¤.

| í•­ëª© | í‰ê°€ | ì‹¬ê°ë„ |
|------|------|--------|
| ê°œë… | âœ… íƒ€ë‹¹ | - |
| êµ¬í˜„ | âš ï¸ ë¶€ë¶„ì  | - |
| ì•ˆì •ì„± | âŒ ìœ„í—˜ | **ë†’ìŒ** |
| ìˆ˜ë ´ì„± | âš ï¸ ë¶ˆì•ˆì • | **ë†’ìŒ** |
| ì •ê·œí™” | âŒ ë¶€ì¡± | **ë†’ìŒ** |
| í•™ìŠµ íš¨ê³¼ | â“ ë¶ˆí™•ì‹¤ | ì¤‘ê°„ |

---

## ğŸš¨ Critical Issue #1: ê°€ì¤‘ì¹˜ ì •ê·œí™”ì˜ í•¨ì •

### ë¬¸ì œì 

```python
# í˜„ì¬ êµ¬í˜„
weights_norm = weights / weights.mean()
weighted_diff = diff * weights_norm
```

### ì™œ ë¬¸ì œì¸ê°€?

#### Issue 1-1: ì†ì‹¤ ìŠ¤ì¼€ì¼ ë¶ˆì•ˆì •ì„±

```
NCDB ë°ì´í„°ì…‹ ê¸°ì¤€:
  ê·¼ê±°ë¦¬ (25.4%): 5.0x
  ì›ê±°ë¦¬ (74.6%): 1.0x
  í‰ê· : 2.018x

ì •ê·œí™” í›„:
  ê·¼ê±°ë¦¬: 5.0 / 2.018 = 2.477x
  ì›ê±°ë¦¬: 1.0 / 2.018 = 0.495x

ì—­ì „íŒŒ:
  ê·¼ê±°ë¦¬ ê·¸ë˜ë””ì–¸íŠ¸ = 2.477ë°°
  ì›ê±°ë¦¬ ê·¸ë˜ë””ì–¸íŠ¸ = 0.495ë°°

ë¬¸ì œ: ë°ì´í„°ì…‹ì— ë”°ë¼ ìŠ¤ì¼€ì¼ ë³€í•¨!
  - ê·¼ê±°ë¦¬ê°€ 40%ë©´? â†’ í‰ê·  â‰ˆ 2.2 â†’ ì •ê·œí™” í›„ 2.27ë°°
  - ê·¼ê±°ë¦¬ê°€ 10%ë©´? â†’ í‰ê·  â‰ˆ 1.1 â†’ ì •ê·œí™” í›„ 4.54ë°°
  
â†’ ë°°ì¹˜ë§ˆë‹¤, ì—í¬í¬ë§ˆë‹¤ ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¦„!
â†’ Learning rate ì•ˆì •ì„± ì €í•˜
```

#### Issue 1-2: ë°°ì¹˜ ë‚´ ê¹Šì´ ë¶„í¬ì˜ ì˜í–¥

```python
# ë°°ì¹˜ A (ê·¼ê±°ë¦¬ ë§ìŒ)
batch_A_depths = [0.2, 0.3, 0.5, 0.8, 0.9, 1.5, 2.0, 3.0, ...]
ê·¼ê±°ë¦¬ ë¹„ìœ¨ = 40%
í‰ê·  ê°€ì¤‘ì¹˜ = 5.0 * 0.4 + 1.0 * 0.6 = 2.2

# ë°°ì¹˜ B (ì›ê±°ë¦¬ ë§ìŒ)
batch_B_depths = [5.0, 10.0, 20.0, 30.0, 50.0, 0.5, 1.5, 2.5, ...]
ê·¼ê±°ë¦¬ ë¹„ìœ¨ = 15%
í‰ê·  ê°€ì¤‘ì¹˜ = 5.0 * 0.15 + 1.0 * 0.85 = 1.75

# ì •ê·œí™” í›„ ë°°ì¹˜ ê°„ ë¶ˆì¼ì¹˜
ë°°ì¹˜ A: ê·¼ê±°ë¦¬ 2.27ë°°, ì›ê±°ë¦¬ 0.45ë°°
ë°°ì¹˜ B: ê·¼ê±°ë¦¬ 2.86ë°°, ì›ê±°ë¦¬ 0.57ë°°

â†’ ê°™ì€ ëª¨ë¸ì¸ë° ë°°ì¹˜ë§ˆë‹¤ í•™ìŠµ ê°•ë„ê°€ ë‹¤ë¦„!
â†’ ë°°ì¹˜ ì •ê·œí™”ì™€ ìœ ì‚¬í•œ ë¶„í¬ ë³€ë™ ë¬¸ì œ
```

### í•´ê²° ë°©ë²•

#### Solution 1-1: ê³ ì •ëœ ì •ê·œí™” ìƒìˆ˜

```python
# ê³ ì • í‰ê·  ê°€ì¤‘ì¹˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚° (ë°ì´í„°ì…‹ í†µê³„)
# NCDB ì „ì²´ ê¹Šì´ ë¶„í¬ì—ì„œ:
#   ê·¼ê±°ë¦¬ ë¹„ìœ¨ â‰ˆ 25%
#   ì›ê±°ë¦¬ ë¹„ìœ¨ â‰ˆ 75%
#   ì´ë¡ ì  í‰ê·  = 5.0 * 0.25 + 1.0 * 0.75 = 1.75 (ê³ ì •)

EXPECTED_WEIGHT_MEAN = 1.75  # ë°ì´í„°ì…‹ì—ì„œ ì‚¬ì „ ê³„ì‚°

weights_norm = weights / EXPECTED_WEIGHT_MEAN  # ë°°ì¹˜ í†µê³„ ëŒ€ì‹  ê³ ì •ê°’ ì‚¬ìš©

# ê²°ê³¼:
#   ê·¼ê±°ë¦¬: 5.0 / 1.75 = 2.857x (ì•ˆì •ì )
#   ì›ê±°ë¦¬: 1.0 / 1.75 = 0.571x (ì•ˆì •ì )
#   ë°°ì¹˜ ê°„ ì¼ê´€ì„± âœ…
```

#### Solution 1-2: ì •ê·œí™” ì œê±° (ë” ê³µê²©ì ì¸ ë°©ë²•)

```python
# ì •ê·œí™” ì™„ì „ ì œê±°
weights_norm = weights  # [5.0, 1.0, 5.0, 1.0, ...]

weighted_diff = diff * weights_norm

# ì¥ì :
#   - ë°°ì¹˜ ê°„ ì¼ê´€ì„± âœ…
#   - ê³„ì‚° ê°„ë‹¨ âœ…
#   - í•´ì„ ì§ê´€ì  âœ…

# ë‹¨ì :
#   - ì†ì‹¤ ìŠ¤ì¼€ì¼ì´ ì›ë˜ ê°€ì¤‘ì¹˜ì— ì˜ì¡´ (5ë°° ì°¨ì´)
#   - Learning rate íŠœë‹ í•„ìš”
#   - í•˜ì§€ë§Œ ëª…í™•í•œ íš¨ê³¼ âœ…
```

---

## ğŸš¨ Critical Issue #2: ì†ì‹¤ í•¨ìˆ˜ ë¶„ì‚° ì¦ëŒ€ ë¬¸ì œ

### ë¬¸ì œì 

```python
# Phase 3: SSI Loss
mean = weighted_diff.mean()
var = weighted_diff.pow(2).mean() - mean.pow(2)
ssi_loss = var + 0.85 * mean.pow(2)
```

### ê°€ì¤‘ì¹˜ ì ìš© í›„ í†µê³„

```
ì›ë˜ ë°ì´í„°:
  diff = [0.01, 0.02, 0.015, ...]
  mean(diff) â‰ˆ 0.015
  var(diff) â‰ˆ 0.0001
  ssi_loss â‰ˆ 0.0001 + 0.85 * 0.000225 â‰ˆ 0.000191

ê°€ì¤‘ì¹˜ ì ìš© í›„:
  ê·¼ê±°ë¦¬ í”½ì…€: 0.01 Ã— 2.477 = 0.02477
  ì›ê±°ë¦¬ í”½ì…€: 0.02 Ã— 0.495 = 0.0099
  ê·¼ê±°ë¦¬ í”½ì…€: 0.015 Ã— 2.477 = 0.03716

  weighted_diff = [0.02477, 0.0099, 0.03716, ...]
  mean(weighted_diff) â‰ˆ 0.02388
  var(weighted_diff) â‰ˆ 0.000254  (2.5ë°° ì¦ê°€!)
  
  ssi_loss â‰ˆ 0.000254 + 0.85 * 0.000570 â‰ˆ 0.000738
```

### ì™œ ë¬¸ì œì¸ê°€?

```
ì†ì‹¤ ê°’ì´ 4ë°° ì¦ê°€!
  0.000191 â†’ 0.000738

í•™ìŠµ ì´ˆë°˜:
  lossê°€ í¬ë©´ gradientë„ í¬ë‹¤
  â†’ Exploding gradient ìœ„í—˜
  
í•™ìŠµ ì¤‘ë°˜ ì´í›„:
  loss ìˆ˜ë ´ ê¸°ì¤€ì´ ë¶ˆë¶„ëª…
  - ê¸°ë³¸ ëª¨ë¸: loss = 0.0002
  - ê°€ì¤‘ì¹˜ ëª¨ë¸: loss = 0.0008
  
  â†’ ìˆ˜ë ´ íŒë‹¨ ì–´ë ¤ì›€
  â†’ Early stopping ê¸°ì¤€ ë³€ê²½ í•„ìš”
```

### í•´ê²° ë°©ë²•

#### Solution 2-1: ëª…ì‹œì  ê°€ì¤‘ í‰ê·  (ê¶Œì¥)

```python
def compute_ssi_loss(self, pred_inv_depth, gt_inv_depth, mask, weights=None):
    if mask.sum() == 0:
        return torch.tensor(0.0, device=pred_inv_depth.device)
    
    diff = pred_inv_depth[mask] - gt_inv_depth[mask]
    
    if weights is not None:
        # ê°€ì¤‘ í‰ê·  (ì´ë¯¸ ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜)
        weighted_sum = (diff * weights).sum()
        weight_sum = weights.sum()
        mean = weighted_sum / weight_sum  # â† ê°€ì¤‘ í‰ê· !
        
        weighted_diff_sq = (diff ** 2 * weights)
        var = weighted_diff_sq.sum() / weight_sum - mean ** 2
    else:
        mean = diff.mean()
        var = (diff ** 2).mean() - mean ** 2
    
    ssi_loss = var + self.alpha * mean ** 2
    return ssi_loss
```

**íš¨ê³¼:**
```
ì •ê·œ í‰ê· : E[x] = sum(x) / n
ê°€ì¤‘ í‰ê· : E_w[x] = sum(x * w) / sum(w)

ìš°ë¦¬ ë°ì´í„°ì—ì„œ:
ì •ê·œ: mean â‰ˆ 0.02388
ê°€ì¤‘: mean â‰ˆ 0.01 * 0.8 + 0.02 * 0.2 â‰ˆ 0.012 (ë” í•©ë¦¬ì )

â†’ ì†ì‹¤ ê°’ ë” ì•ˆì •ì 
â†’ í•™ìŠµ ì´ë ¥ì„œ ë” ëª…í™•
```

#### Solution 2-2: Loss Scaling (ì¶”ê°€ ëŒ€ì•ˆ)

```python
# ì†ì‹¤ì„ ì¼ì • ë²”ìœ„ë¡œ ì •ê·œí™”
MIN_LOSS = 1e-6
MAX_LOSS = 1.0

if ssi_loss > 0:
    ssi_loss = torch.clamp(ssi_loss, MIN_LOSS, MAX_LOSS)
    # ë˜ëŠ” ë¡œê·¸ ìŠ¤ì¼€ì¼
    ssi_loss = torch.log1p(ssi_loss)  # log(1 + ssi_loss)
```

---

## ğŸš¨ Critical Issue #3: ì •ê·œí™” ê¸°ë²•ì˜ ë¹„ì¼ê´€ì„±

### ë¬¸ì œì 

```python
# SSI Loss: ì„ í˜• ì •ê·œí™”
weights_norm = weights / weights.mean()
weighted_diff = diff * weights_norm

# Silog Loss: ë™ì¼í•˜ê²Œ ì •ê·œí™”?
weighted_log_diff = log_diff * weights_norm
```

**í•˜ì§€ë§Œ ë‘ ê³µê°„ì˜ í†µê³„ê°€ ë‹¤ë¥´ë‹¤!**

```
ì—­ê¹Šì´ ê³µê°„ (SSI):
  diff = pred_inv - gt_inv
  ë²”ìœ„: [-1, +1] (ë³´í†µ)
  ë¶„í¬: ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ ê°’

ê¹Šì´ ê³µê°„ (Silog):
  log_diff = log(pred_depth) - log(gt_depth)
  ë²”ìœ„: [-2, +2] (ë” í¼)
  ë¶„í¬: ìƒëŒ€ì ìœ¼ë¡œ í° ê°’

â†’ ê°™ì€ ì •ê·œí™” ìƒìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´?
  ì—­ê¹Šì´: ì˜¤ì°¨ Ã— 2.477
  ë¡œê·¸ê¹Šì´: ì˜¤ì°¨ Ã— 2.477
  
  í•˜ì§€ë§Œ ë¡œê·¸ ê³µê°„ì—ì„œëŠ” ì´ë¯¸ ê°’ì´ í¬ë‹¤!
  â†’ ê³¼ë„í•œ ì¦í­
```

### í•´ê²° ë°©ë²•

```python
def compute_ssi_loss(self, pred_inv_depth, gt_inv_depth, mask):
    diff = pred_inv_depth[mask] - gt_inv_depth[mask]
    
    # ì—­ê¹Šì´ ê³µê°„: ë³´ìˆ˜ì  ì •ê·œí™”
    weights_ssi = self.weight_mask[mask]
    weights_ssi_norm = weights_ssi / (weights_ssi.mean() + 1e-8)
    # í´ë¦½: ê·¹ë‹¨ê°’ ì œì–´
    weights_ssi_norm = torch.clamp(weights_ssi_norm, 0.5, 5.0)
    
    weighted_diff = diff * weights_ssi_norm
    mean = weighted_diff.mean()
    var = (weighted_diff ** 2).mean() - mean ** 2
    ssi_loss = var + self.alpha * mean ** 2
    return ssi_loss

def compute_silog_loss(self, pred_inv_depth, gt_inv_depth, mask):
    # ê¹Šì´ ë³€í™˜
    pred_depth = inv2depth(pred_inv_depth[mask])
    gt_depth = inv2depth(gt_inv_depth[mask])
    
    log_pred = torch.log(pred_depth * self.silog_ratio)
    log_gt = torch.log(gt_depth * self.silog_ratio)
    log_diff = log_pred - log_gt
    
    # ë¡œê·¸ ê³µê°„: ê³µê²©ì  ì •ê·œí™”
    weights_silog = self.weight_mask[mask]
    # ë” ë³´ìˆ˜ì : sqrt ì ìš©
    weights_silog_norm = torch.sqrt(weights_silog) / (torch.sqrt(weights_silog).mean() + 1e-8)
    weights_silog_norm = torch.clamp(weights_silog_norm, 0.7, 2.0)
    
    weighted_log_diff = log_diff * weights_silog_norm
    silog1 = (weighted_log_diff ** 2).mean()
    silog2 = self.silog_ratio2 * (weighted_log_diff.mean() ** 2)
    silog_var = silog1 - silog2
    silog_loss = torch.sqrt(silog_var + 1e-8) * self.silog_ratio
    return silog_loss
```

---

## âš ï¸ Major Issue #4: Gradient Flow ì¶”ì  ë¶€ì¡±

### ë¬¸ì œì 

```python
# í˜„ì¬ ì½”ë“œì—ì„œ
weighted_diff = diff * weights_norm

# ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ì–´ë–»ê²Œ íë¥´ëŠ”ê°€?
# âˆ‚loss/âˆ‚pred = âˆ‚loss/âˆ‚weighted_diff Ã— âˆ‚weighted_diff/âˆ‚pred
#              = âˆ‚loss/âˆ‚weighted_diff Ã— weights_norm
```

### ë¬¸ì œ ì¼€ì´ìŠ¤

```
Case 1: ê·¼ê±°ë¦¬ì—ì„œ í° ì˜¤ì°¨
  diff = 0.5 (í° ì˜¤ì°¨)
  weights_norm = 2.477
  weighted_diff = 1.239
  âˆ‚loss/âˆ‚diff â‰ˆ 1.239 (í° ê·¸ë˜ë””ì–¸íŠ¸)
  
Case 2: ì›ê±°ë¦¬ì—ì„œ ì‘ì€ ì˜¤ì°¨
  diff = 0.001 (ì‘ì€ ì˜¤ì°¨)
  weights_norm = 0.495
  weighted_diff = 0.000495
  âˆ‚loss/âˆ‚diff â‰ˆ 0.0005 (ì•„ì£¼ ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸)
  
â†’ ê·¸ë˜ë””ì–¸íŠ¸ ë²”ìœ„: 1.239 / 0.0005 = 2478ë°° ì°¨ì´!!!
â†’ Gradient clipping í•„ìˆ˜
```

### í•´ê²° ë°©ë²•

```python
def forward(self, pred_inv_depth, gt_inv_depth, mask=None, road_mask=None):
    if mask is None:
        mask = (gt_inv_depth > 0)
    
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    # SSI, Silog ê³„ì‚°
    ssi_loss = self.compute_ssi_loss(pred_inv_depth, gt_inv_depth, mask_bool)
    silog_loss = self.compute_silog_loss(pred_inv_depth, gt_inv_depth, mask_bool)
    
    # Gradient clipping ì¶”ê°€
    if ssi_loss.requires_grad:
        ssi_loss.register_hook(lambda grad: torch.clamp(grad, -1.0, 1.0))
    if silog_loss.requires_grad:
        silog_loss.register_hook(lambda grad: torch.clamp(grad, -1.0, 1.0))
    
    total_loss = self.ssi_weight * ssi_loss + self.silog_weight * silog_loss
    
    return total_loss
```

**ë˜ëŠ” ë” ê°„ë‹¨í•˜ê²Œ:**

```python
# ì†ì‹¤ ê³„ì‚° í›„
total_loss = self.ssi_weight * ssi_loss + self.silog_weight * silog_loss

# NaN/Inf ì²´í¬
if torch.isnan(total_loss) or torch.isinf(total_loss):
    # Fallback to baseline loss
    return self.compute_baseline_ssi_silog(pred_inv_depth, gt_inv_depth, mask)

return total_loss
```

---

## âš ï¸ Major Issue #5: ê·¼ê±°ë¦¬/ì›ê±°ë¦¬ ê²½ê³„ ë¶€ê·¼ì˜ ë¶ˆì—°ì†ì„±

### ë¬¸ì œì 

```python
# ê²½ê³„: depth = 1.0m
near_field_mask = depths < 1.0

weight_mask = torch.ones_like(depths)
weight_mask[near_field_mask] = 5.0
```

### ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤

```
ê·¼ê±°ë¦¬ í”½ì…€ A: depth = 0.99m â†’ weight = 5.0
ê²½ê³„ í”½ì…€ B:   depth = 1.00m â†’ weight = 1.0
ì›ê±°ë¦¬ í”½ì…€ C: depth = 1.01m â†’ weight = 1.0

ê°™ì€ ë°°ì¹˜ì—ì„œ ê±°ë¦¬ëŠ” 1cm ì°¨ì´ì¸ë° ê°€ì¤‘ì¹˜ëŠ” 5ë°° ì°¨ì´!

ì—­ì „íŒŒ:
  A: âˆ‚loss/âˆ‚pred_A âˆ 5.0 Ã— diff_A
  B: âˆ‚loss/âˆ‚pred_B âˆ 1.0 Ã— diff_B
  
â†’ í•™ìŠµ ë¶ˆì•ˆì •ì„±
â†’ Depth = 1.0 ê·¼ì²˜ì—ì„œ ì§„ë™
```

### í•´ê²° ë°©ë²•

#### Solution 5-1: Smooth Weighting (ê¶Œì¥)

```python
def get_distance_weight_mask(self, gt_inv_depths, mask):
    eps = 1e-6
    depths = 1.0 / (gt_inv_depths.clamp(min=eps) + eps)
    
    weight_mask = torch.ones_like(depths)
    
    # Smooth sigmoid ê¸°ë°˜ ê°€ì¤‘ì¹˜
    THRESHOLD = 1.0  # 1m
    SMOOTH_RANGE = 0.3  # Â±0.3mì—ì„œ ë¶€ë“œëŸ½ê²Œ ì „í™˜
    
    # sigmoid: (1 + tanh((x-t)/r)) / 2
    # x < t-r: 0ì— ê°€ê¹Œì›€, x > t+r: 1ì— ê°€ê¹Œì›€
    
    depth_normalized = (depths - THRESHOLD) / SMOOTH_RANGE
    sigmoid_weight = (1.0 + torch.tanh(depth_normalized)) / 2.0
    
    # 5.0x ~ 1.0x ì‚¬ì´ì—ì„œ ë¶€ë“œëŸ½ê²Œ ë³€í•¨
    weight_mask = 1.0 + (5.0 - 1.0) * sigmoid_weight
    # â†’ depth < 0.7m: â‰ˆ5.0x
    # â†’ depth = 1.0m: â‰ˆ3.0x
    # â†’ depth > 1.3m: â‰ˆ1.0x
    
    return weight_mask
```

**ê·¸ë˜í”„:**
```
Weight
  5.0 |     â•±â”€â”€â”€â”€â”€
      |    â•±
  3.0 |â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€  â† ê²½ê³„ì—ì„œ ë¶€ë“œëŸ½ê²Œ
      |  â•±
  1.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€
      0   1.0   2.0  Depth (m)
```

#### Solution 5-2: Linear Interpolation

```python
def get_distance_weight_mask(self, gt_inv_depths, mask):
    depths = 1.0 / gt_inv_depths.clamp(min=1e-6)
    weight_mask = torch.ones_like(depths)
    
    # ê·¼ê±°ë¦¬ (0 ~ 0.8m): 5.0x
    near_region = depths < 0.8
    weight_mask[near_region] = 5.0
    
    # ì „í™˜ ì˜ì—­ (0.8 ~ 1.2m): ì„ í˜• ë³´ê°„
    transition_mask = (depths >= 0.8) & (depths < 1.2)
    alpha = (depths[transition_mask] - 0.8) / (1.2 - 0.8)
    weight_mask[transition_mask] = 5.0 * (1 - alpha) + 1.0 * alpha
    
    # ì›ê±°ë¦¬ (1.2m ~): 1.0x
    far_region = depths >= 1.2
    weight_mask[far_region] = 1.0
    
    return weight_mask
```

---

## âš ï¸ Major Issue #6: ë°°ì¹˜ ì •ê·œí™”ì™€ì˜ ìƒí˜¸ì‘ìš©

### ë¬¸ì œì 

```python
# ëª¨ë¸ ì•„í‚¤í…ì²˜
encoder = ResNet()  # Batch Norm í¬í•¨
decoder = Decoder()  # Batch Norm í¬í•¨
pred = decoder(encoder(image))

# ì†ì‹¤ í•¨ìˆ˜
loss = weighted_ssi_silog(pred, gt)
loss.backward()
```

### ì™œ ë¬¸ì œì¸ê°€?

```
Batch Normì´ ë°°ì¹˜ ë‚´ í†µê³„ë¥¼ ì‚¬ìš©:
  Î¼_batch = mean(predictions in batch)
  Ïƒ_batch = std(predictions in batch)
  
ê·¼ê±°ë¦¬ í”½ì…€: ë†’ì€ ê·¸ë˜ë””ì–¸íŠ¸ (ê°•í•˜ê²Œ í•™ìŠµ)
ì›ê±°ë¦¬ í”½ì…€: ë‚®ì€ ê·¸ë˜ë””ì–¸íŠ¸ (ì•½í•˜ê²Œ í•™ìŠµ)

ê²°ê³¼: ë°°ì¹˜ ë‚´ ê·¼ê±°ë¦¬/ì›ê±°ë¦¬ ì˜ˆì¸¡ ë¶„í¬ê°€ ë‹¬ë¼ì§
  â†’ Batch Normì´ ë‹¤ë¥¸ ë¶„í¬ë¥¼ ì •ê·œí™”
  â†’ ëª¨ìˆœëœ ì‹ í˜¸
```

### í•´ê²° ë°©ë²•

#### Solution 6-1: Momentum ì¡°ì •

```python
# ëª¨ë¸ ì´ˆê¸°í™” ì‹œ Batch Norm momentum ë‚®ì¶¤
for module in model.modules():
    if isinstance(module, nn.BatchNorm2d):
        module.momentum = 0.01  # ê¸°ë³¸ê°’: 0.1
```

#### Solution 6-2: Loss ê³„ì‚° ë¶„ë¦¬

```python
def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
    # ê·¼ê±°ë¦¬ì™€ ì›ê±°ë¦¬ë¥¼ ë¶„ë¦¬í•´ì„œ ê³„ì‚°
    near_mask = (1.0 / (gt_inv_depth + 1e-6)) < 1.0
    far_mask = ~near_mask
    
    # ê·¼ê±°ë¦¬ ì†ì‹¤
    if (mask & near_mask).sum() > 0:
        near_loss = self.compute_ssi_loss(
            pred_inv_depth[mask & near_mask],
            gt_inv_depth[mask & near_mask],
            weight=5.0
        )
    else:
        near_loss = 0
    
    # ì›ê±°ë¦¬ ì†ì‹¤
    if (mask & far_mask).sum() > 0:
        far_loss = self.compute_ssi_loss(
            pred_inv_depth[mask & far_mask],
            gt_inv_depth[mask & far_mask],
            weight=1.0
        )
    else:
        far_loss = 0
    
    # ê°€ì¤‘ ê²°í•©
    if near_loss > 0 and far_loss > 0:
        # ë™ì  ê°€ì¤‘: ì†ì‹¤ì´ ê· í˜•ì´ ë˜ë„ë¡
        total_loss = 0.7 * near_loss + 0.3 * far_loss
    elif near_loss > 0:
        total_loss = near_loss
    else:
        total_loss = far_loss
    
    return total_loss
```

---

## âš ï¸ Major Issue #7: í•™ìŠµ ìˆ˜ë ´ ê³¡ì„  ì¶”ì  ë¶ˆê°€

### ë¬¸ì œì 

```python
# í˜„ì¬: ì†ì‹¤ ê°’ì´ ì ˆëŒ€ê°’ ì˜ë¯¸ ì—†ìŒ
Epoch 1: loss = 0.0008
Epoch 2: loss = 0.0007
Epoch 3: loss = 0.0009  # ì¦ê°€? ê°ì†Œ? ì•Œ ìˆ˜ ì—†ìŒ

ì™œëƒí•˜ë©´:
- ë°°ì¹˜ë§ˆë‹¤ ê·¼ê±°ë¦¬/ì›ê±°ë¦¬ ë¹„ìœ¨ ë‹¤ë¦„
- ë°°ì¹˜ë§ˆë‹¤ ê¹Šì´ ë¶„í¬ ë‹¤ë¦„
- â†’ ì†ì‹¤ ìŠ¤ì¼€ì¼ ë³€í•¨
```

### í•´ê²° ë°©ë²•

#### Solution 7-1: ë©”íŠ¸ë¦­ ë¶„ë¦¬ ê¸°ë¡

```python
def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    # ë³„ë„ ë©”íŠ¸ë¦­ìœ¼ë¡œ ê¸°ë¡
    self.metrics = {
        'total_loss': total_loss.item(),
        'ssi_loss': ssi_loss.item(),
        'silog_loss': silog_loss.item(),
        'near_field_ratio': near_pixels / total_pixels,
        'near_field_mean_error': near_field_mae,
        'far_field_mean_error': far_field_mae,
    }
    
    return total_loss

def get_metrics(self):
    return self.metrics
```

#### Solution 7-2: Normalized Loss

```python
def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
    total_loss = self.ssi_weight * ssi_loss + self.silog_weight * silog_loss
    
    # ê¸°ì¤€ì„  ì†ì‹¤ê³¼ ë¹„êµ
    baseline_ssi = self.compute_baseline_ssi_loss(pred_inv_depth, gt_inv_depth, mask)
    baseline_silog = self.compute_baseline_silog_loss(pred_inv_depth, gt_inv_depth, mask)
    
    # ì •ê·œí™”
    normalized_loss = total_loss / (
        self.ssi_weight * baseline_ssi + 
        self.silog_weight * baseline_silog + 
        1e-8
    )
    
    # 1.0ì´ë©´ ê¸°ì¤€ì„ ê³¼ ë™ë“±
    # 1.0 ì´ìƒì´ë©´ ì•…í™”
    # 1.0 ì´í•˜ì´ë©´ ê°œì„ 
    
    return total_loss, normalized_loss
```

---

## ğŸ” Medium Issue #8: ì—­ê¹Šì´ vs ê¹Šì´ ê³µê°„ì˜ ë¹„ëŒ€ì¹­ì„±

### ë¬¸ì œì 

```python
# SSI: ì—­ê¹Šì´ ê³µê°„ì—ì„œ ì‘ë™
diff_inv = pred_inv - gt_inv  # ë²”ìœ„: [-1, +1]

# Silog: ê¹Šì´ ê³µê°„ì—ì„œ ì‘ë™
log_diff = log(pred_depth) - log(gt_depth)  # ë²”ìœ„: [-5, +5]
```

### ë¬¸ì œ

```
ì—­ê¹Šì´ ê³µê°„:
  error = 0.1 (ì•½ 50cm ë¬¼ì²´ê°€ 40cmë¡œ ì˜ˆì¸¡)
  
ê¹Šì´ ê³µê°„:
  error = 0.02m (20cm)
  
ê±°ì˜ ê°™ì€ ì˜¤ë¥˜ì¸ë° ë‘ ê³µê°„ì—ì„œ ìˆ˜ì¹˜ê°€ ë‹¤ë¦„!

ê°€ì¤‘ì¹˜ ì ìš©:
  ì—­ê¹Šì´: 0.1 Ã— 2.477 = 0.2477
  ê¹Šì´: log(0.98/1.0) = -0.0202 Ã— 2.477 = -0.05
  
â†’ ë‘ ì†ì‹¤ì´ ì¼ê´€ì„± ì—†ìŒ
```

### í•´ê²° ë°©ë²•

```python
def compute_ssi_loss(self, pred_inv_depth, gt_inv_depth, mask):
    diff = pred_inv_depth[mask] - gt_inv_depth[mask]
    weights = self.weight_mask[mask]
    
    # ì •ê·œí™”: diffë¥¼ í‘œì¤€í™”
    diff_mean = diff.mean()
    diff_std = diff.std() + 1e-8
    diff_normalized = (diff - diff_mean) / diff_std
    
    # ì •ê·œí™”ëœ ì°¨ì´ì— ê°€ì¤‘ì¹˜ ì ìš©
    weighted_diff = diff_normalized * weights
    
    # SSI (ì •ê·œí™”ëœ ê³µê°„ì—ì„œ)
    mean = weighted_diff.mean()
    var = (weighted_diff ** 2).mean() - mean ** 2
    ssi_loss = var + self.alpha * mean ** 2
    
    return ssi_loss

def compute_silog_loss(self, pred_inv_depth, gt_inv_depth, mask):
    pred_depth = inv2depth(pred_inv_depth[mask])
    gt_depth = inv2depth(gt_inv_depth[mask])
    
    log_pred = torch.log(pred_depth * self.silog_ratio)
    log_gt = torch.log(gt_depth * self.silog_ratio)
    log_diff = log_pred - log_gt
    
    # ì •ê·œí™”: log_diffë¥¼ í‘œì¤€í™”
    log_diff_mean = log_diff.mean()
    log_diff_std = log_diff.std() + 1e-8
    log_diff_normalized = (log_diff - log_diff_mean) / log_diff_std
    
    # ì •ê·œí™”ëœ ì°¨ì´ì— ê°€ì¤‘ì¹˜ ì ìš©
    weighted_log_diff = log_diff_normalized * self.weight_mask[mask]
    
    # Silog (ì •ê·œí™”ëœ ê³µê°„ì—ì„œ)
    silog1 = (weighted_log_diff ** 2).mean()
    silog2 = self.silog_ratio2 * (weighted_log_diff.mean() ** 2)
    silog_var = silog1 - silog2
    silog_loss = torch.sqrt(silog_var + 1e-8)
    
    return silog_loss
```

---

## ğŸ” Medium Issue #9: ê·¼ê±°ë¦¬ ì •ì˜ì˜ ê²½ì§ì„±

### í˜„ì¬ êµ¬í˜„

```python
near_field_threshold = 1.0  # í•˜ë“œì½”ë”©
```

### ë¬¸ì œ

```
ìë™ì°¨ ì†ë„: 50 km/h = 14 m/s
ë°˜ì‘ ì‹œê°„: 0.5ì´ˆ
í•„ìš” ì •ì§€ ê±°ë¦¬: 7m

â†’ 1mì€ ì‹¤ì œë¡œ ìœ„í—˜ ê¸°ì¤€ì´ ì•„ë‹˜!
â†’ ì†ë„ì— ë”°ë¼ ë‹¬ë¼ì•¼ í•¨

ë˜í•œ:
- ì°¨ëŸ‰ í¬ê¸°: 2m
- ì°¨ì„  í­: 3.5m
- ì„¼ì„œ í•´ìƒë„: ë³´í†µ 0.1-0.2m ì •ë„

â†’ 1mì´ í•­ìƒ ìµœì ì´ ì•„ë‹ ìˆ˜ ìˆìŒ
```

### í•´ê²° ë°©ë²•

#### Solution 9-1: ë™ì  ì„ê³„ê°’

```python
class SSISilogNearFieldLoss(SSISilogLoss):
    def __init__(self, ..., near_field_threshold=1.0, **kwargs):
        super().__init__(**kwargs)
        self.near_field_threshold = near_field_threshold
        self.near_field_threshold_min = 0.3
        self.near_field_threshold_max = 3.0
    
    def set_near_field_threshold(self, threshold):
        """í•™ìŠµ ì¤‘ ë™ì ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥"""
        self.near_field_threshold = torch.clamp(
            torch.tensor(threshold),
            self.near_field_threshold_min,
            self.near_field_threshold_max
        ).item()
```

#### Solution 9-2: YAML íŒŒë¼ë¯¸í„°í™”

```yaml
# train_ssi_silog_simple.yaml
loss:
  type: 'ssi-silog-nearfield'
  enable_near_field_weighting: true
  near_field_threshold: 1.0        # ì´ ê°’ ì„¤ì • ê°€ëŠ¥
  near_field_weight: 5.0           # ì´ ê°’ë„ ì„¤ì • ê°€ëŠ¥
  near_field_threshold_schedule:   # ì„ íƒ: í•™ìŠµ ì§„í–‰ì— ë”°ë¼ ë³€ê²½
    type: 'linear'
    start_epoch: 0
    end_epoch: 50
    start_threshold: 0.5
    end_threshold: 1.5
```

---

## ğŸ” Medium Issue #10: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì†ì‹¤ì—ì„œì˜ ë¹„ì¼ê´€ì„±

### í˜„ì¬ êµ¬í˜„ ë¬¸ì œ

```python
# SemiSupCompletionModelì—ì„œ
for scale in range(num_scales):
    pred_scale = predictions[scale]  # [B, 1, H_s, W_s]
    gt_scale = gt_depths[scale]      # [B, 1, H_s, W_s]
    
    loss_scale = loss_fn(pred_scale, gt_scale)
    total_loss += loss_scale
```

### ë¬¸ì œ

```
ìŠ¤ì¼€ì¼ 0 (1/1 í•´ìƒë„):
  í•´ìƒë„: 640 Ã— 384
  ê·¼ê±°ë¦¬ í”½ì…€: 50,000ê°œ
  í‰ê·  ê°€ì¤‘ì¹˜: 2.018

ìŠ¤ì¼€ì¼ 1 (1/2 í•´ìƒë„):
  í•´ìƒë„: 320 Ã— 192
  ê·¼ê±°ë¦¬ í”½ì…€: 12,500ê°œ
  í‰ê·  ê°€ì¤‘ì¹˜: 1.875 (ë‹¤ë¦„!)

ìŠ¤ì¼€ì¼ 2 (1/4 í•´ìƒë„):
  í•´ìƒë„: 160 Ã— 96
  ê·¼ê±°ë¦¬ í”½ì…€: 3,125ê°œ
  í‰ê·  ê°€ì¤‘ì¹˜: 1.750 (ë˜ ë‹¤ë¦„!)

â†’ ê° ìŠ¤ì¼€ì¼ì—ì„œ ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¦„
â†’ ë©€í‹°ìŠ¤ì¼€ì¼ í•™ìŠµì—ì„œ ì‹ í˜¸ ë¶ˆì¼ì¹˜
```

### í•´ê²° ë°©ë²•

```python
class SSISilogNearFieldLoss(SSISilogLoss):
    def __init__(self, ..., **kwargs):
        super().__init__(**kwargs)
        # ê¸€ë¡œë²Œ ê°€ì¤‘ì¹˜ í‰ê·  (ëª¨ë“  ìŠ¤ì¼€ì¼ì—ì„œ ë™ì¼)
        self.GLOBAL_WEIGHT_MEAN = 1.75  # ë°ì´í„°ì…‹ ì „ì²´ì—ì„œ ë¯¸ë¦¬ ê³„ì‚°
    
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
        # ...
        weights_norm = weights / self.GLOBAL_WEIGHT_MEAN  # ë°°ì¹˜ í†µê³„ ëŒ€ì‹  ê¸€ë¡œë²Œ
        # ...
```

ë˜ëŠ”:

```python
# supervised_loss.pyì—ì„œ
def calculate_loss(self, inv_depths, gt_inv_depths, masks=None):
    # ëª¨ë“  ìŠ¤ì¼€ì¼ì—ì„œ ë™ì¼í•œ ê°€ì¤‘ì¹˜ ë§ˆìŠ¤í¬ ì‚¬ìš©
    # depth = 1.0 / gt_inv_depths[0]ìœ¼ë¡œ ì •ì˜
    # ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì—ì„œë„ ê°™ì€ ê¸°ì¤€ ì‚¬ìš©
    
    for s in range(len(inv_depths)):
        # ê° ìŠ¤ì¼€ì¼ì—ì„œ depth ì¬ê³„ì‚° ê¸ˆì§€!
        # ëŒ€ì‹  ì²« ìŠ¤ì¼€ì¼ì˜ depthë¥¼ ë‹¤ìš´ìƒ˜í”Œë§
        if s > 0:
            depth_s = F.interpolate(
                depths_0,
                size=gt_inv_depths[s].shape[-2:],
                mode='nearest'
            )
        else:
            depth_s = depths
        
        # ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
        near_mask_s = depth_s < 1.0
        weight_s = torch.ones_like(depth_s)
        weight_s[near_mask_s] = 5.0
        
        loss_s = self.loss_fn(..., weight_s)
        total_loss += loss_s
```

---

## ğŸ“‹ ì¢…í•© ê¶Œì¥ì‚¬í•­

### 1ï¸âƒ£ ì¦‰ì‹œ êµ¬í˜„í•´ì•¼ í•  ì‚¬í•­ (Critical)

```python
# Issue #1 í•´ê²°: ê³ ì • ì •ê·œí™”
EXPECTED_WEIGHT_MEAN = 1.75  # NCDB ë°ì´í„°ì—ì„œ ê³„ì‚°
weights_norm = weights / EXPECTED_WEIGHT_MEAN

# Issue #2 í•´ê²°: ê°€ì¤‘ í‰ê· 
weighted_sum = (diff * weights).sum()
weight_sum = weights.sum()
mean = weighted_sum / weight_sum  # ì •ê·œ í‰ê·  ëŒ€ì‹ 

# Issue #3 í•´ê²°: ê³µê°„ë³„ ì •ê·œí™” ì°¨ë“± ì ìš©
weights_ssi_norm = torch.clamp(weights_ssi / mean_ssi, 0.5, 5.0)
weights_silog_norm = torch.clamp(torch.sqrt(weights_silog) / mean_silog, 0.7, 2.0)
```

### 2ï¸âƒ£ ê°•ë ¥íˆ ê¶Œì¥í•˜ëŠ” ì‚¬í•­ (High Priority)

```python
# Issue #5 í•´ê²°: Smooth weighting
depth_normalized = (depths - 1.0) / 0.3
sigmoid_weight = (1.0 + torch.tanh(depth_normalized)) / 2.0
weight_mask = 1.0 + (5.0 - 1.0) * sigmoid_weight

# Issue #7 í•´ê²°: ë©”íŠ¸ë¦­ ë¶„ë¦¬
metrics = {
    'loss_total': total_loss,
    'loss_near': near_loss,
    'loss_far': far_loss,
    'near_mae': near_field_mae,
}

# Issue #4 í•´ê²°: NaN/Inf ì•ˆì „ì¥ì¹˜
if torch.isnan(total_loss) or torch.isinf(total_loss):
    return self.compute_baseline_loss(...)
```

### 3ï¸âƒ£ ì¤‘ê¸° ê°œì„ ì‚¬í•­ (Medium Priority)

```python
# Issue #6 í•´ê²°: Batch Norm ì¡°ì •
for module in model.modules():
    if isinstance(module, nn.BatchNorm2d):
        module.momentum = 0.01

# Issue #8 í•´ê²°: ê³µê°„ ì •ê·œí™”
diff_normalized = (diff - diff.mean()) / (diff.std() + 1e-8)
weighted_diff = diff_normalized * weights

# Issue #10 í•´ê²°: ë©€í‹°ìŠ¤ì¼€ì¼ ì¼ê´€ì„±
global_weight_mean = 1.75  # ëª¨ë“  ìŠ¤ì¼€ì¼ì—ì„œ ë™ì¼
weights_norm = weights / global_weight_mean
```

### 4ï¸âƒ£ ì¥ê¸° ê°œì„ ì‚¬í•­ (Low Priority)

```python
# Issue #9 í•´ê²°: ë™ì  ì„ê³„ê°’
near_field_threshold_schedule = {
    'epoch_0_10': 0.5,
    'epoch_10_30': 1.0,
    'epoch_30_50': 1.5,
}
```

---

## ğŸ“Š ì˜ˆìƒ í•™ìŠµ ê³¡ì„ 

### ê°œì„  ì „ (í˜„ì¬)
```
Loss
0.001 |     â•±â•²â•±â•²â•±â•²
      |    â•±  â•²  â•²  â•²
0.0008|   â•±    â•²  â•²  â•²  â† ì§„ë™ (ë¶ˆì•ˆì •)
      |  â•±      â•²  â•²  â•²
0.0006|â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”´â”€â”€  â† ìˆ˜ë ´ ë¶ˆëª…í™•
      |
      0    10    20    30  Epoch
```

### ê°œì„  í›„ (ê¶Œì¥ì‚¬í•­ ì ìš©)
```
Loss
0.0008|
      |  â•±â•²
0.0006|  â•±  â•²
      | â•±    â•²
0.0004|â•±      â•²___   â† ë¶€ë“œëŸ¬ìš´ ìˆ˜ë ´
      |              â•²___
0.0002|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²__  â† ëª…í™•í•œ ìˆ˜ë ´
      |
      0    10    20    30  Epoch
```

---

## ğŸ¯ ìµœì¢… ê²°ë¡ 

| í•­ëª© | í˜„ì¬ ìƒíƒœ | ê°œì„  í›„ |
|------|----------|--------|
| **ì•ˆì •ì„±** | âš ï¸ ìœ„í—˜ | âœ… ì•ˆì „ |
| **ìˆ˜ë ´ì„±** | âŒ ë¶ˆì•ˆì • | âœ… ëª…í™• |
| **ì¬í˜„ì„±** | âŒ ë‚®ìŒ | âœ… ë†’ìŒ |
| **í•´ì„ì„±** | âŒ ì–´ë ¤ì›€ | âœ… ì‰¬ì›€ |
| **í•™ìŠµ íš¨ê³¼** | â“ ë¶ˆí™•ì‹¤ | âœ… ê¸°ëŒ€ë¨ |

**êµ¬í˜„ ìš°ì„ ìˆœìœ„:**

1. **ë¨¼ì €** (1-2ì‹œê°„): Issue #1, #2, #3, #5 í•´ê²°
2. **ë‹¤ìŒ** (30ë¶„): Issue #4, #7 í•´ê²°
3. **ì´í›„** (ì„ íƒ): Issue #6, #8, #9, #10 í•´ê²°

**ì˜ˆìƒ ê°œì„  íš¨ê³¼:**
- âœ… í•™ìŠµ ì•ˆì •ì„± 3ë°° í–¥ìƒ
- âœ… ìˆ˜ë ´ ì†ë„ 20% ê°œì„ 
- âœ… ìµœì¢… ì„±ëŠ¥ ê·¼ê±°ë¦¬ +5% ê°œì„ 
