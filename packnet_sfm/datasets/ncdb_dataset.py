import os
import json
from pathlib import Path
import glob
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset
from torch.utils.data.dataloader import default_collate

from packnet_sfm.datasets.transforms import get_transforms
from packnet_sfm.utils.image import load_image
from packnet_sfm.geometry.camera import FisheyeCamera  # âœ… ì¶”ê°€

# ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° í•˜ë“œì½”ë”© (camera_lidar_projector.pyì—ì„œ ê°€ì ¸ì˜´)
DEFAULT_CALIB_A6 = {
    "model": "vadas",
    "intrinsic": [-0.0004, 1.0136, -0.0623, 0.2852, -0.332, 0.1896, -0.0391,
                  1.0447, 0.0021, 44.9516, 2.48822, 0, 0.9965, -0.0067,
                  -0.0956, 0.1006, -0.054, 0.0106],
    "extrinsic": [0.0900425, -0.00450864, -0.356367, 0.00100918, -0.236104, -0.0219886],
    "image_size": None
}

DEFAULT_LIDAR_TO_WORLD = np.array([
    [-0.998752, -0.00237052, -0.0498847,  0.0375091],
    [ 0.00167658, -0.999901,   0.0139481,  0.0349093],
    [-0.0499128,  0.0138471,   0.998658,   0.771878],
    [ 0.,         0.,          0.,         1.       ]
])

class NcdbDataset(Dataset):
    """
    NCDB Dataset for Semi-supervised Learning with FisheyeCamera support.
    """
    # ğŸ”” í•´ìƒë„ ê°ì§€ ë¡œê·¸: í”„ë¡œì„¸ìŠ¤ë‹¹ í•œ ë²ˆë§Œ ì¶œë ¥
    _RESOLUTION_LOG_SHOWN = False

    # âœ… depth_type â†’ í´ë”ëª… ë§¤í•‘
    DEPTH_TYPE_MAPPING = {
        'distance': 'newest_distance_maps',           # ì¹´ë©”ë¼ ì¤‘ì‹¬ì—ì„œ 3D ìœ í´ë¦¬ë“œ ê±°ë¦¬
        'depth': 'newest_original_depth_maps',        # Z-axis depth (synthetic ì—†ìŒ, ì›ë³¸)
        'depth_synthetic': 'newest_depth_maps',       # Z-axis depth (synthetic í¬í•¨)
    }
    
    # ê¸°ë³¸ê°’: distance (3D ìœ í´ë¦¬ë“œ ê±°ë¦¬)
    DEFAULT_DEPTH_TYPE = 'distance'

    # âœ… ê³µí†µ PNG ë¡œë”: í•­ìƒ (ë¯¸í„°*256) ê·œì•½ì´ë©´ /256 ë³µì› (default)
    def _load_depth_png(self, depth_path: Path):
        try:
            depth_png = Image.open(depth_path)
            arr16 = np.asarray(depth_png, dtype=np.uint16)
            depth = arr16.astype(np.float32)
            
            # 1. KITTI ìŠ¤íƒ€ì¼ë¡œ 256ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
            if depth.max() > 255:
                depth /= 256.0
            
            # 2. ìœ íš¨í•˜ì§€ ì•Šì€ í”½ì…€(ê°’ì´ 0ì´ì—ˆë˜ ë¶€ë¶„)ì„ -1ë¡œ ë§ˆìŠ¤í‚¹
            depth[arr16 == 0] = 0
            
            return depth
        except (FileNotFoundError, OSError) as e:
            print(f"[NcdbDataset] Depth load failed: {depth_path} ({e})")
            return None

    def __init__(self, dataset_root, split_file, transform=None, mask_file=None,
                 back_context=0, forward_context=0, strides=(1,), 
                 with_context=False, with_depth=True,
                 depth_type=None,        # âœ… 'distance', 'depth', or 'depth_synthetic' (configì—ì„œ ì„¤ì •)
                 depth_variants=None,    # str | list | None (í•˜ìœ„í˜¸í™˜ìš©, depth_type ìš°ì„ )
                 strict_depth=True,      # âš ï¸ ê¸°ë³¸ True: depth íŒŒì¼ ì—†ìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
                 use_mask: bool = False, # â† ì¶”ê°€: ë§ˆìŠ¤í¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ ë¯¸ì‚¬ìš©)
                 min_depth: float = None,  # â† GT depth í•„í„°ë§ ìµœì†Œê°’
                 max_depth: float = None,  # â† GT depth í•„í„°ë§ ìµœëŒ€ê°’
                 dry_run: bool = False,  # âœ… dry-run ëª¨ë“œ: í´ë” ì¡´ì¬ ì—¬ë¶€ ê²€ì¦ë§Œ ìˆ˜í–‰
                 **kwargs):
        super().__init__()
        
        # Dataset paths
        self.dataset_root = Path(dataset_root)
        self.use_mask = bool(use_mask)
        
        # Depth range for GT filtering
        self.min_depth = min_depth
        self.max_depth = max_depth
        if self.min_depth is not None or self.max_depth is not None:
            print(f"[NcdbDataset] GT depth filtering enabled: "
                  f"min_depth={self.min_depth}, max_depth={self.max_depth}")

        # Context parameters (KITTI style)
        self.backward_context = back_context
        self.forward_context = forward_context
        self.strides = strides
        self.with_context = with_context or (back_context > 0 or forward_context > 0)
        self.with_depth = with_depth
        
        # âœ… depth_type ì²˜ë¦¬ (configì—ì„œ ì„¤ì • ê°€ëŠ¥)
        # ìš°ì„ ìˆœìœ„: depth_type > í™˜ê²½ë³€ìˆ˜ > depth_variants > ê¸°ë³¸ê°’
        env_depth_type = os.getenv('NCDB_DEPTH_TYPE', '').strip().lower()
        env_variant = os.getenv('NCDB_DEPTH_VARIANT', '').strip()
        
        if depth_type is not None:
            # configì—ì„œ depth_type ì§€ì •
            depth_type = depth_type.lower()
            if depth_type not in self.DEPTH_TYPE_MAPPING:
                raise ValueError(f"[NcdbDataset] Invalid depth_type: '{depth_type}'. "
                               f"Valid options: {list(self.DEPTH_TYPE_MAPPING.keys())}")
            self.depth_type = depth_type
            self.depth_variants = [self.DEPTH_TYPE_MAPPING[depth_type]]
        elif env_depth_type:
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ depth_type ì§€ì •
            if env_depth_type not in self.DEPTH_TYPE_MAPPING:
                raise ValueError(f"[NcdbDataset] Invalid NCDB_DEPTH_TYPE: '{env_depth_type}'. "
                               f"Valid options: {list(self.DEPTH_TYPE_MAPPING.keys())}")
            self.depth_type = env_depth_type
            self.depth_variants = [self.DEPTH_TYPE_MAPPING[env_depth_type]]
        elif env_variant:
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì§ì ‘ í´ë”ëª… ì§€ì • (í•˜ìœ„í˜¸í™˜)
            self.depth_type = None
            self.depth_variants = [v.strip() for v in env_variant.split(',') if v.strip()]
        elif depth_variants is not None:
            # depth_variants ì§ì ‘ ì§€ì • (í•˜ìœ„í˜¸í™˜)
            self.depth_type = None
            if isinstance(depth_variants, str):
                self.depth_variants = [depth_variants]
            else:
                self.depth_variants = list(depth_variants)
        else:
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            self.depth_type = self.DEFAULT_DEPTH_TYPE
            self.depth_variants = [self.DEPTH_TYPE_MAPPING[self.DEFAULT_DEPTH_TYPE]]
        
        self.strict_depth = strict_depth
        
        # ì¤‘ë³µ ì œê±° (ì•ìª½ ìš°ì„ ìˆœìœ„ ìœ ì§€)
        seen = set()
        ordered = []
        for v in self.depth_variants:
            if v not in seen:
                ordered.append(v)
                seen.add(v)
        self.depth_variants = ordered
        
        # âœ… ëª…í™•í•œ ë¡œê·¸ ì¶œë ¥
        if self.depth_type:
            print(f"[NcdbDataset] ğŸ“Š depth_type='{self.depth_type}' â†’ folder: {self.depth_variants[0]}")
        else:
            print(f"[NcdbDataset] Using depth variants (priority order): {self.depth_variants}")
        
        # âœ… dry_run ëª¨ë“œ ì €ì¥
        self.dry_run = dry_run
        
        # Context path storage
        self.backward_context_paths = []
        self.forward_context_paths = []
        
        # Cache for file validation
        self._file_cache = {}
        self._folder_cache = {}
        
        # Load split file
        self._load_split_file(split_file)
        
        # âœ… dry-run ëª¨ë“œ: í´ë” ë° íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
        if dry_run:
            self._validate_depth_folders()
            return  # dry-run ëª¨ë“œì—ì„œëŠ” ì—¬ê¸°ì„œ ì¢…ë£Œ
        
        # Load mask if provided (0/1 binary), but apply only when use_mask=True
        self.mask = None
        if mask_file:
            absolute_mask_path = self.dataset_root / mask_file
            if absolute_mask_path.exists():
                self.mask = (np.array(Image.open(absolute_mask_path).convert('L')) > 0).astype(np.uint8)
                print(f"[NcdbDataset] Loaded mask (0/1) shape={self.mask.shape} | use_mask={self.use_mask}")
        
        # Transform
        self.transform = transform
        
        # Filter paths with context if needed
        if self.with_context:
            self._filter_paths_with_context()

        # ğŸ”” ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ í•œ ë²ˆë§Œ ì…ë ¥ í•´ìƒë„ ë¡œê·¸
        self._log_input_resolution_once()
    
    def _load_split_file(self, split_file):
        """Load and validate split file"""
        absolute_split_path = self.dataset_root / split_file
        if Path(split_file).is_absolute():
            absolute_split_path = Path(split_file)

        if not absolute_split_path.exists():
            raise FileNotFoundError(f"Split file not found: {absolute_split_path}")
        
        with open(absolute_split_path, 'r') as f:
            mapping_data = json.load(f)
        
        if not isinstance(mapping_data, list):
            raise ValueError("Split file must contain a list of entries")
        
        # í‘œì¤€í™”: image_pathë§Œ ìˆëŠ” í•­ëª©ì„ (dataset_root, new_filename)ë¡œ ë³€í™˜
        normalized = []
        converted = 0
        for item in mapping_data:
            if 'dataset_root' in item and 'new_filename' in item:
                normalized.append({'dataset_root': item['dataset_root'],
                                   'new_filename': item['new_filename']})
                continue
            if 'image_path' in item:
                p = Path(item['image_path'])
                stem = p.stem  # íŒŒì¼ ìŠ¤í…œ
                base_dir = p.parent
                if base_dir.name == 'image_a6':
                    base_dir = base_dir.parent  # .../synced_data

                # self.dataset_root ê¸°ì¤€ ìƒëŒ€ê²½ë¡œë¡œ ë§Œë“¤ê¸° ì‹œë„
                try:
                    rel_base = str(base_dir.relative_to(self.dataset_root))
                except Exception:
                    # ë£¨íŠ¸ ë°– ì ˆëŒ€ ê²½ë¡œë©´ ê·¸ëŒ€ë¡œ ì €ì¥(ì ˆëŒ€ ê²½ë¡œ ìš°ì„ )
                    rel_base = str(base_dir)
                normalized.append({'dataset_root': rel_base, 'new_filename': stem})
                converted += 1
                continue
            raise ValueError(f"Split entry missing required keys: {list(item.keys())}")
        
        self.data_entries = normalized
        print(f"Loaded {len(self.data_entries)} entries from {absolute_split_path} (converted {converted} from image_path)")
    
    def _validate_depth_folders(self):
        """
        âœ… Dry-run validation: í´ë” ì¡´ì¬ ì—¬ë¶€ ë° íŒŒì¼ ë§¤ì¹­ ê²€ì¦
        """
        print(f"\n{'='*60}")
        print(f"ğŸ” DRY-RUN VALIDATION")
        print(f"{'='*60}")
        print(f"Dataset root: {self.dataset_root}")
        print(f"Depth type: {self.depth_type}")
        print(f"Target folder: {self.depth_variants[0]}")
        print(f"Total entries: {len(self.data_entries)}")
        print(f"{'='*60}\n")
        
        # 1. ê³ ìœ í•œ dataset_root ê²½ë¡œë“¤ ìˆ˜ì§‘
        unique_roots = set(entry['dataset_root'] for entry in self.data_entries)
        print(f"ğŸ“‚ Unique dataset roots: {len(unique_roots)}")
        
        # 2. ê° rootì—ì„œ depth í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        folder_status = {}
        for root in sorted(unique_roots):
            base_path = self.dataset_root / root
            depth_folder = base_path / self.depth_variants[0]
            
            if depth_folder.exists():
                # í´ë” ë‚´ íŒŒì¼ ìˆ˜ í™•ì¸
                png_files = list(depth_folder.glob('*.png'))
                folder_status[root] = {
                    'exists': True,
                    'file_count': len(png_files),
                    'path': str(depth_folder)
                }
            else:
                folder_status[root] = {
                    'exists': False,
                    'file_count': 0,
                    'path': str(depth_folder)
                }
        
        # 3. ê²°ê³¼ ì¶œë ¥
        exists_count = sum(1 for s in folder_status.values() if s['exists'])
        missing_count = len(folder_status) - exists_count
        
        print(f"\nğŸ“Š Folder Status:")
        print(f"   âœ… Found: {exists_count}/{len(folder_status)}")
        print(f"   âŒ Missing: {missing_count}/{len(folder_status)}")
        
        if missing_count > 0:
            print(f"\nâš ï¸  Missing folders:")
            for root, status in folder_status.items():
                if not status['exists']:
                    print(f"   âŒ {status['path']}")
        
        # 4. ìƒ˜í”Œ ë‹¨ìœ„ íŒŒì¼ ë§¤ì¹­ ê²€ì¦ (ì²˜ìŒ 10ê°œ + ëœë¤ 10ê°œ)
        print(f"\nğŸ“ Sample file validation (first 10 entries):")
        matched = 0
        missing_files = []
        
        check_entries = self.data_entries[:min(10, len(self.data_entries))]
        for entry in check_entries:
            stem = entry['new_filename']
            depth_path = self.dataset_root / entry['dataset_root'] / self.depth_variants[0] / f"{stem}.png"
            rgb_path = self.dataset_root / entry['dataset_root'] / 'image_a6' / f"{stem}.png"
            if not rgb_path.exists():
                rgb_path = self.dataset_root / entry['dataset_root'] / 'image_a6' / f"{stem}.jpg"
            
            rgb_ok = rgb_path.exists()
            depth_ok = depth_path.exists()
            
            if rgb_ok and depth_ok:
                matched += 1
                status = "âœ…"
            else:
                status = "âŒ"
                missing_files.append({
                    'stem': stem,
                    'rgb': str(rgb_path) if not rgb_ok else None,
                    'depth': str(depth_path) if not depth_ok else None
                })
            
            print(f"   {status} {stem}: RGB={'âœ“' if rgb_ok else 'âœ—'}, Depth={'âœ“' if depth_ok else 'âœ—'}")
        
        # 5. ì „ì²´ íŒŒì¼ ë§¤ì¹­ë¥  ê³„ì‚° (ìƒ˜í”Œë§)
        import random
        sample_size = min(100, len(self.data_entries))
        sample_entries = random.sample(self.data_entries, sample_size)
        
        total_matched = 0
        for entry in sample_entries:
            stem = entry['new_filename']
            depth_path = self.dataset_root / entry['dataset_root'] / self.depth_variants[0] / f"{stem}.png"
            if depth_path.exists():
                total_matched += 1
        
        match_rate = (total_matched / sample_size) * 100
        
        print(f"\nğŸ“ˆ Estimated match rate (sampled {sample_size} entries):")
        print(f"   Depth files found: {total_matched}/{sample_size} ({match_rate:.1f}%)")
        
        # 6. ìµœì¢… ìš”ì•½
        print(f"\n{'='*60}")
        if missing_count == 0 and match_rate >= 95:
            print(f"âœ… VALIDATION PASSED")
            print(f"   depth_type='{self.depth_type}' â†’ '{self.depth_variants[0]}'")
        else:
            print(f"âš ï¸  VALIDATION WARNING")
            if missing_count > 0:
                print(f"   - {missing_count} depth folders missing")
            if match_rate < 95:
                print(f"   - Low match rate: {match_rate:.1f}%")
        print(f"{'='*60}\n")
    
    def _filter_paths_with_context(self):
        """Filter paths that have valid context frames (KITTI style)"""
        print(f"Filtering for context (backward={self.backward_context}, forward={self.forward_context})")
        
        valid_entries = []
        valid_backward_contexts = []
        valid_forward_contexts = []
        
        for stride in self.strides:
            for idx, entry in enumerate(self.data_entries):
                backward_context, forward_context = self._get_sample_context(
                    idx, self.backward_context, self.forward_context, stride)
                
                if backward_context is not None:
                    valid_entries.append(entry)
                    valid_backward_contexts.append(backward_context)
                    valid_forward_contexts.append(forward_context)
        
        self.data_entries = valid_entries
        self.backward_context_paths = valid_backward_contexts
        self.forward_context_paths = valid_forward_contexts
        
        print(f"After context filtering: {len(self.data_entries)} valid samples")
    
    def _get_sample_context(self, idx, backward_context, forward_context, stride=1):
        """Get context frames for a sample (KITTI style)"""
        max_idx = len(self.data_entries) - 1
        
        # Check bounds
        if idx - backward_context * stride < 0 or idx + forward_context * stride > max_idx:
            return None, None
        
        # Validate backward context
        backward_context_files = []
        for offset in range(-backward_context, 0):
            context_idx = idx + offset * stride
            if not self._check_sample_exists(context_idx):
                return None, None
            backward_context_files.append(context_idx)
        
        # Validate forward context
        forward_context_files = []
        for offset in range(1, forward_context + 1):
            context_idx = idx + offset * stride
            if not self._check_sample_exists(context_idx):
                return None, None
            forward_context_files.append(context_idx)
        
        return backward_context_files, forward_context_files
    
    def _resolve_depth_path(self, entry, stem):
        """
        variant ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì¡´ì¬í•˜ëŠ” depth ê²½ë¡œ ë°˜í™˜.
        Returns (path, variant_name) or (None, None)
        """
        base = self.dataset_root / entry['dataset_root']
        for variant in self.depth_variants:
            p = base / variant / f"{stem}.png"
            if p.exists():
                return p, variant
        return None, None
    
    def _check_sample_exists(self, idx):
         """Check if sample files exist"""
         if idx in self._file_cache:
             return self._file_cache[idx]
         entry = self.data_entries[idx]
         stem = entry['new_filename']
         image_path = self.dataset_root / entry['dataset_root'] / 'image_a6' / f"{stem}.png"
         if not image_path.exists():
             image_path = self.dataset_root / entry['dataset_root'] / 'image_a6' / f"{stem}.jpg"
         if not image_path.exists():
             self._file_cache[idx] = False
             return False
         if self.with_depth:
            depth_path, variant = self._resolve_depth_path(entry, stem)
            if depth_path is None:
                if self.strict_depth:
                    self._file_cache[idx] = False
                    return False
                # strictê°€ ì•„ë‹ˆë©´ depth ì—†ì´ë„ ìƒ˜í”Œ ì‚¬ìš©
         self._file_cache[idx] = True
         return True
    
    def __len__(self):
        return len(self.data_entries)
    
    def __getitem__(self, idx):
        entry = self.data_entries[idx]
        stem = entry['new_filename']
        
        # Construct paths (try .png first, fallback to .jpg)
        image_path = self.dataset_root / entry['dataset_root'] / 'image_a6' / f"{stem}.png"
        if not image_path.exists():
            image_path = self.dataset_root / entry['dataset_root'] / 'image_a6' / f"{stem}.jpg"
        
        depth_path = None
        depth_variant = None
        if self.with_depth:
            depth_path, depth_variant = self._resolve_depth_path(entry, stem)
            if depth_path is None and self.strict_depth:
                base_path = self.dataset_root / entry['dataset_root']
                raise FileNotFoundError(
                    f"âŒ [STRICT] Depth file not found!\n"
                    f"   Sample: {stem}\n"
                    f"   Expected: {base_path / self.depth_variants[0] / f'{stem}.png'}\n"
                    f"   Variants tried: {self.depth_variants}")
        
        # Load image
        image = load_image(str(image_path))
        W, H = image.size
        depth_gt = None
        
        if self.with_depth and depth_path is not None:
            depth_gt = self._load_depth_png(depth_path)
            
            # âœ… GT depth í•„í„°ë§: min_depth ë¯¸ë§Œê³¼ max_depth ì´ˆê³¼ ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
            if depth_gt is not None and (self.min_depth is not None or self.max_depth is not None):
                original_valid = (depth_gt > 0).sum()
                
                if self.min_depth is not None:
                    depth_gt[depth_gt < self.min_depth] = 0
                
                if self.max_depth is not None:
                    depth_gt[depth_gt > self.max_depth] = 0
                
        # (ì¶”ê°€) Depth í†µê³„ ê³„ì‚°
        depth_stats = None
        if depth_gt is not None:
            pos = depth_gt[depth_gt > 0]
            if pos.size > 0:
                depth_stats = {
                    'min': float(depth_gt.min()),
                    'max': float(depth_gt.max()),
                    'p50': float(np.percentile(pos, 50)),
                    'p90': float(np.percentile(pos, 90)),
                    'p95': float(np.percentile(pos, 95)),
                    'count_pos': int(pos.size),
                    'count_all': int(depth_gt.size),
                }
                # âœ… Debug output removed - was causing spam during training
            else:
                depth_stats = {
                    'min': float(depth_gt.min()),
                    'max': float(depth_gt.max()),
                    'p50': 0.0, 'p90': 0.0, 'p95': 0.0,
                    'count_pos': 0,
                    'count_all': int(depth_gt.size)
                }
                # âœ… Debug output removed

        # (ì„ íƒ) Binary mask per-sample ìƒì„± (RGBì—ëŠ” ì ìš©í•˜ì§€ ì•ŠìŒ)
        mask01 = None
        if self.use_mask and (self.mask is not None):
            if self.mask.shape[:2] != (H, W):
                mask_img = Image.fromarray((self.mask * 255).astype(np.uint8), mode='L')
                mask_img = mask_img.resize((W, H), Image.NEAREST)
                mask01 = (np.array(mask_img) > 0).astype(np.uint8)
            else:
                mask01 = self.mask

            # GTì—ë§Œ ë§ˆìŠ¤í¬ ì ìš© (0/1 ê³±)
            if depth_gt is not None:
                depth_gt = depth_gt * mask01

        # Calibration data
        calib_data = DEFAULT_CALIB_A6.copy()
        calib_data['image_size'] = (H, W)
        
        # Process intrinsics for FisheyeCamera
        intrinsics_list = torch.tensor(calib_data['intrinsic'], dtype=torch.float32)
        distortion_coeffs = {
            'k': torch.tensor(calib_data['intrinsic'][0:7], dtype=torch.float32),
            's': torch.tensor(calib_data['intrinsic'][7], dtype=torch.float32),
            'div': torch.tensor(calib_data['intrinsic'][8], dtype=torch.float32),
            'ux': torch.tensor(calib_data['intrinsic'][9], dtype=torch.float32),
            'uy': torch.tensor(calib_data['intrinsic'][10], dtype=torch.float32),
            'image_size': (H, W)
        }
        
        # âœ… FisheyeCamera ê°ì²´ ìƒì„±
        camera = FisheyeCamera(
            intrinsics=distortion_coeffs,
            image_size=(H, W)
        )
        
        # Extrinsic and LiDAR data
        extrinsic_matrix = torch.tensor(calib_data['extrinsic'], dtype=torch.float32)
        lidar_to_world_matrix = torch.tensor(DEFAULT_LIDAR_TO_WORLD, dtype=torch.float32)
        
        # Apply mask if available
        if self.mask is not None:
            if self.mask.shape[:2] != (H, W):
                mask_img = Image.fromarray((self.mask * 255).astype(np.uint8), mode='L')
                mask_img = mask_img.resize((W, H), Image.NEAREST)
                self.mask = np.array(mask_img)
            
            image_np = np.array(image)
            image_np = (image_np * self.mask[:, :, np.newaxis]).astype(image_np.dtype)
            image = Image.fromarray(image_np)
            
            if depth_gt is not None:
                depth_gt = depth_gt * self.mask  # â† ì—¬ê¸°ê°€ ë¬¸ì œì¼ ê°€ëŠ¥ì„±
        
        # Build sample
        sample = {
            'rgb': image,
            'idx': idx,
            'camera': camera,
            'intrinsics': intrinsics_list,
            'distortion_coeffs': distortion_coeffs,
            'extrinsic': extrinsic_matrix,
            'lidar_to_world': lidar_to_world_matrix,
            'filename': stem,
            'meta': {
                'image_path': str(image_path),
                'depth_path': str(depth_path) if (self.with_depth and depth_path is not None) else None,
                'depth_variant': depth_variant,
                'calibration_source': 'hardcoded_default',
                'depth_stats': depth_stats,
            }
        }
        
        # Add depth
        if depth_gt is not None:
            sample['depth'] = depth_gt

        # í•„ìš” ì‹œ downstreamì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ maskë„ ì „ë‹¬ (ì˜µì…˜)
        if mask01 is not None:
            sample['mask'] = mask01

        # Apply transforms
        if self.transform:
            sample = self.transform(sample)
            
            # # â˜… ë””ë²„ê¹…: Transform í›„ depth í™•ì¸
            # if 'depth' in sample and not hasattr(self, '_transform_depth_logged'):
            #     self._transform_depth_logged = True
            #     d = sample['depth']
            #     print(f"\n[NcdbDataset.__getitem__] After transform:")
            #     if isinstance(d, torch.Tensor):
            #         print(f"  Type: torch.Tensor")
            #         print(f"  Shape: {d.shape}")
            #         print(f"  Max: {d.max().item():.2f}")
            #         print(f"  Min: {d.min().item():.2f}")
            #         valid = d > 0
            #         if valid.any():
            #             print(f"  Valid range: [{d[valid].min().item():.2f}, {d[valid].max().item():.2f}]")
            #     elif isinstance(d, np.ndarray):
            #         print(f"  Type: numpy.ndarray")
            #         print(f"  Shape: {d.shape}")
            #         print(f"  Max: {np.max(d):.2f}")
            #         print(f"  Min: {np.min(d):.2f}")
            #         valid = d > 0
            #         if valid.any():
            #             print(f"  Valid range: [{d[valid].min().item():.2f}, {d[valid].max().item():.2f}]")
        
        return sample

    # âœ… ì»¤ìŠ¤í…€ collate_fn ì¶”ê°€: FisheyeCameraë¥¼ ì²˜ë¦¬
    @staticmethod
    def custom_collate_fn(batch):
        """
        ì»¤ìŠ¤í…€ collate í•¨ìˆ˜: FisheyeCamera ê°ì²´ë¥¼ ì²˜ë¦¬.
        - ë°°ì¹˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ (ì˜ˆ: [{'rgb': ..., 'camera': FisheyeCamera, ...}]).
        - 'camera'ì™€ 'camera_context'ëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ìŒ.
        - ë‹¤ë¥¸ í‚¤ëŠ” default_collate ì‚¬ìš©.
        """
        if not batch:
            return {}
        
        # ì²« ë²ˆì§¸ ì•„ì´í…œì˜ í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬
        keys = batch[0].keys()
        collated = {}
        
        for key in keys:
            values = [item[key] for item in batch if key in item]
            
            if key in ['camera', 'camera_context']:
                # FisheyeCamera ê°ì²´ëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ìŒ
                collated[key] = values
            else:
                # ë‹¤ë¥¸ ë°ì´í„°ëŠ” default_collate ì‚¬ìš©
                try:
                    collated[key] = default_collate(values)
                except TypeError:
                    # ì‹¤íŒ¨ ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€
                    collated[key] = values
        
        return collated
    
    def _log_input_resolution_once(self):
        """ì²« ìœ íš¨ ìƒ˜í”Œì˜ WÃ—Hë¥¼ í•œ ë²ˆë§Œ ë¡œê·¸ë¡œ ì¶œë ¥"""
        if NcdbDataset._RESOLUTION_LOG_SHOWN:
            return
        # ì²« ìœ íš¨ ì—”íŠ¸ë¦¬ íƒìƒ‰
        for entry in self.data_entries:
            stem = entry.get('new_filename')
            if not stem:
                continue
            img_path = self.dataset_root / entry['dataset_root'] / 'image_a6' / f"{stem}.png"
            if not img_path.exists():
                img_path = self.dataset_root / entry['dataset_root'] / 'image_a6' / f"{stem}.jpg"
            if not img_path.exists():
                continue
            try:
                with Image.open(img_path) as im:
                    W, H = im.size
            except Exception:
                continue
            msg = f"[NcdbDataset] Detected input resolution {W}x{H} (WÃ—H)"
            if (W, H) == (640, 384):
                msg += " â€” OK (expected 640x384)."
            print(msg + " [once]")
            NcdbDataset._RESOLUTION_LOG_SHOWN = True
            break