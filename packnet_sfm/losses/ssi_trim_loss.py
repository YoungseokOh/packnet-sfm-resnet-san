import torch
import torch.nn as nn

class SSITrimLoss(nn.Module):
    """
    Scale- and Shift-Invariant Trimmed L1-Loss (MiDaS `L_ssitrim`)

    Args
    ----
    trim : float, optional
        Fraction (0â€’1) of highest-error pixels to discard. 0 â†’ no trimming.
    epsilon : float, optional
        Small number to stabilise pseudo-inverse when var(pred) â‰ˆ 0.
    """
    def __init__(self, trim: float = 0.2, epsilon: float = 1e-6):
        super().__init__()
        assert 0.0 <= trim < 1.0, "trim must be in [0,1)"
        self.trim = trim
        self.eps = epsilon

    @staticmethod
    def _solve_scale_shift(pred, gt, mask, eps):
        """Closed-form Î±, Î² solving â€–Î± d + Î² â€“ zâ€–Â² (least-squares)."""
        # flatten valid pixels
        d = pred[mask]                # (N,)
        z = gt[mask]
        if d.numel() == 0:            # empty mask â†’ return neutral Î±,Î²
            return torch.tensor(1.0, device=pred.device), torch.tensor(0.0, device=pred.device)
        
        # ğŸ†• ë” ì•ˆì •ì ì¸ ìŠ¤ì¼€ì¼-ì‹œí”„íŠ¸ ê³„ì‚°
        n = d.numel()
        if n < 10:  # ë„ˆë¬´ ì ì€ í”½ì…€ì´ë©´ neutral ë°˜í™˜
            return torch.tensor(1.0, device=pred.device), torch.tensor(0.0, device=pred.device)
        
        # ğŸ†• Robust statisticsë¡œ outlierì— ëœ ë¯¼ê°í•˜ê²Œ
        mean_d, mean_z = d.mean(), z.mean()
        
        # ë¶„ì‚° ê³„ì‚°ì„ ë” ì•ˆì •ì ìœ¼ë¡œ
        var_d = torch.var(d, unbiased=False) + eps
        cov_dz = torch.mean((d - mean_d) * (z - mean_z))
        
        # ğŸ†• ë” ì•ˆì •ì ì¸ alpha ê³„ì‚°
        alpha = cov_dz / var_d
        beta = mean_z - alpha * mean_d
        
        # ğŸ†• ë¹„ì •ìƒì ì¸ ìŠ¤ì¼€ì¼ ë°©ì§€
        alpha = torch.clamp(alpha, 0.1, 10.0)  # ìŠ¤ì¼€ì¼ ì œí•œ
        
        return alpha, beta

    def forward(self, pred, gt, mask=None):
        """
        pred, gt : (B, 1, H, W) -or- (B, H, W)
        mask     : bool/int same shape, True/1 = valid GT.  None â†’ all valid.
        """
        if pred.dim() == 4:     # squeeze channel dim if present
            pred, gt = pred.squeeze(1), gt.squeeze(1)
            if mask is not None and mask.dim() == 4:
                mask = mask.squeeze(1)

        B = pred.shape[0]
        total = 0.0
        
        for b in range(B):
            mb = torch.ones_like(gt[b], dtype=torch.bool) if mask is None else mask[b] > 0
            
            # ğŸ†• ìœ íš¨í•œ í”½ì…€ì´ ì¶©ë¶„í•œì§€ í™•ì¸
            if mb.sum() < 100:  # ìµœì†Œ 100ê°œ í”½ì…€ í•„ìš”
                total += torch.tensor(0.0, device=pred.device, dtype=pred.dtype)
                continue
            
            Î±, Î² = self._solve_scale_shift(pred[b], gt[b], mb, self.eps)
            
            # ğŸ†• aligned prediction ê³„ì‚°
            aligned_pred = Î± * pred[b] + Î²
            
            # aligned residuals
            res = torch.abs(aligned_pred - gt[b])
            res = res[mb]                     # apply mask

            if self.trim > 0 and res.numel() > 0:
                k = int((1.0 - self.trim) * res.numel())
                if k > 0:
                    # ì •ë ¬ì„ ì‚¬ìš©í•´ì„œ ìƒìœ„ trim% ì œê±°
                    res_sorted, _ = torch.sort(res)
                    res = res_sorted[:k]  # ì‘ì€ kê°œë§Œ ì„ íƒ (ìƒìœ„ trim% ì œê±°)
                elif k == 0:
                    res = torch.tensor([], device=res.device, dtype=res.dtype)
            
            # ë¹ˆ í…ì„œ ì²˜ë¦¬
            if res.numel() > 0:
                total += res.mean()
            else:
                total += torch.tensor(0.0, device=pred.device, dtype=pred.dtype)
        
        return total / B