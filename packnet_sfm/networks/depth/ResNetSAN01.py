import os
import torch
import torch.nn as nn
from torchvision.utils import save_image  # ‚Üê Ï∂îÍ∞Ä
import json  # ‚Üê Ï∂îÍ∞Ä

from packnet_sfm.networks.layers.resnet.resnet_encoder import ResnetEncoder
from packnet_sfm.networks.layers.resnet.depth_decoder import DepthDecoder
from packnet_sfm.networks.layers.enhanced_minkowski_encoder import EnhancedMinkowskiEncoder
from functools import partial


class ResNetSAN01(nn.Module):
    """
    üÜï Enhanced ResNet-based SAN network with improved LiDAR feature extraction
    
    Parameters
    ----------
    dropout : float
        Dropout value to use
    version : str
        Version string (format: {num_layers}{variant}, e.g., '18A', '34A', '50B')
    use_film : bool
        Whether to use Depth-aware FiLM modulation
    film_scales : list of int
        Which scales to apply FiLM (default: [0] - first scale only)
    kwargs : dict
        Extra parameters
    """
    def __init__(self, dropout=None, version=None, use_film=False, film_scales=[0],
                 use_enhanced_lidar=False,
                 min_depth=0.5, max_depth=80.0,
                 depth_output_mode='sigmoid',  # 'sigmoid' (default) or 'direct'
                 **kwargs):
        super().__init__()
        
        # ÏïàÏ†Ñ Î≥¥Ï†ï
        if max_depth <= 0: max_depth = 80.0
        if min_depth <= 0: min_depth = 0.5
        if max_depth <= min_depth: max_depth = min_depth + 1.0
        self.min_depth = float(min_depth)
        self.max_depth = float(max_depth)
        self.depth_output_mode = depth_output_mode
        
        # üÜï Í∏∞Ï°¥ ÌååÎùºÎØ∏ÌÑ∞Îßå ÏÇ¨Ïö©
        use_enhanced_lidar = kwargs.get('use_enhanced_lidar', False)  # Í∏∞Î≥∏Í∞í FalseÎ°ú Î≥ÄÍ≤Ω
        
        # Parse version string
        if version:
            num_layers = int(version[:2])
            self.variant = version[2:] if len(version) > 2 else 'A'
        else:
            num_layers = 18
            self.variant = 'A'
        
        print(f"üèóÔ∏è Initializing ResNetSAN01 with ResNet-{num_layers} (variant {self.variant})")
        print(f"üéØ Depth range: [{self.min_depth}, {self.max_depth}]m")
        print(f"üéØ Depth output mode: {self.depth_output_mode}")
        if self.depth_output_mode == 'direct':
            print(f"   ‚Üí Direct Linear Depth (INT8 friendly: ¬±{(self.max_depth - self.min_depth) / 255 / 2 * 1000:.1f}mm)")
        else:
            print(f"   ‚Üí Sigmoid ‚Üí Bounded Inverse (legacy)")
        
        # ResNet encoder
        self.encoder = ResnetEncoder(num_layers=num_layers, pretrained=True)
        
        # Standard depth decoder
        self.decoder = DepthDecoder(num_ch_enc=self.encoder.num_ch_enc)
        
        # ÏÑ§Ï†ï
        self.use_film = use_film
        self.film_scales = film_scales
        self.use_enhanced_lidar = use_enhanced_lidar
        
        # FiLM configuration
        rgb_channels_per_scale = None
        if use_film:
            rgb_channels_per_scale = []
            for i in range(len(self.encoder.num_ch_enc)):
                if i in film_scales:
                    rgb_channels_per_scale.append(self.encoder.num_ch_enc[i])
                else:
                    rgb_channels_per_scale.append(0)

        # üîß Minkowski encoder ÏÑ†ÌÉù (Ï°∞Í±¥Î∂Ä)
        # use_film=FalseÏù¥Î©¥ Minkowski encoder Î∂àÌïÑÏöî (Ï∂îÎ°† Ï†ÑÏö©)
        self.mconvs = None
        if use_film:
            if use_enhanced_lidar:
                print("üîß Using EnhancedMinkowskiEncoder")
                from packnet_sfm.networks.layers.enhanced_minkowski_encoder import EnhancedMinkowskiEncoder
                self.mconvs = EnhancedMinkowskiEncoder(
                    self.encoder.num_ch_enc,
                    rgb_channels=rgb_channels_per_scale,
                    with_uncertainty=False
                )
                
                # Feature refinement layers (EnhancedÏö©)
                self.feature_refinement = nn.ModuleList([
                    nn.Sequential(
                        nn.Conv2d(ch, ch, 3, padding=1, bias=False),
                        nn.BatchNorm2d(ch),
                        nn.ReLU(inplace=True),
                        nn.Conv2d(ch, ch, 3, padding=1, bias=False)
                    ) for ch in self.encoder.num_ch_enc
                ])
            else:
                print("üîß Using standard MinkowskiEncoder")
                from packnet_sfm.networks.layers.minkowski_encoder import MinkowskiEncoder
                self.mconvs = MinkowskiEncoder(
                    self.encoder.num_ch_enc,
                    rgb_channels=rgb_channels_per_scale,
                    with_uncertainty=False
                )
        else:
            print("üîß Minkowski encoder disabled (inference-only mode)")

        
        # Learnable fusion weights
        self.weight = torch.nn.parameter.Parameter(
            torch.ones(5) * 0.5, requires_grad=True
        )
        self.bias = torch.nn.parameter.Parameter(
            torch.zeros(5), requires_grad=True
        )
        
        print(f"üéØ FiLM enabled: {use_film}")
        if use_film:
            print(f"   FiLM scales: {film_scales}")
            print(f"   RGB channels per scale: {rgb_channels_per_scale}")
        
        self.init_weights()
        
        self._disp_stats_done = False  # ‚úÖ DISP_STATS_ONCE Ï†úÏñ¥ ÌîåÎûòÍ∑∏

    def init_weights(self):
        """Initialize only newly created layers; keep pretrained encoder intact."""
        # Skip encoder (pretrained)
        for name, m in self.named_modules():
            if name.startswith('encoder'):
                continue
            if isinstance(m, (nn.Conv2d, nn.Conv3d)):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    m.bias.data.zero_()

    def _maybe_log_disp_stats(self, outputs):
        """
        ENV:
          DISP_STATS_ONCE=1  -> Ìïú Î≤àÎßå disparity ÌÜµÍ≥Ñ Ï∂úÎ†•
          DISP_STATS_EVERY=1 -> Îß§ step Ï∂úÎ†•
          DISP_STATS_DIR     -> Ï†ÄÏû• Ìè¥Îçî (Í∏∞Î≥∏: disp_stats)
        """
        every = os.environ.get("DISP_STATS_EVERY", "0") == "1"
        once  = os.environ.get("DISP_STATS_ONCE", "0") == "1"
        if not (every or once):
            return
        if once and self._disp_stats_done:
            return

        # scale 0 Í∏∞Ï§Ä (ÏóÜÏúºÎ©¥ Î¶¨ÌÑ¥)
        key = ("disp", 0)
        if key not in outputs:
            return
        disp = outputs[key].detach()
        v = disp[disp.isfinite()]
        if v.numel() == 0:
            print("[DISP_STATS] no finite values")
            return
        q = torch.quantile(v, torch.tensor([0.0,0.01,0.05,0.5,0.95,0.99,1.0], device=v.device))
        stats = {
            "min": float(q[0]), "p1": float(q[1]), "p5": float(q[2]),
            "median": float(q[3]), "p95": float(q[4]), "p99": float(q[5]),
            "max": float(q[6]),
            "mean": float(v.mean()), "std": float(v.std()),
            "sat>0.99": float((disp > 0.99).float().mean()),
            "sat<0.01": float((disp < 0.01).float().mean()),
        }
        print(f"[DISP_STATS] scale0:", " ".join(f"{k}={stats[k]:.4g}" for k in stats))

        # ===== Ï†ÄÏû• (JSON + PNG Ìïú Ïû•) =====
        try:
            if not hasattr(self, "_disp_stats_idx"):
                self._disp_stats_idx = 0
            out_dir = os.environ.get("DISP_STATS_DIR", "disp_stats")
            os.makedirs(out_dir, exist_ok=True)
            json_path = os.path.join(out_dir, f"disp_stats_{self._disp_stats_idx:05d}.json")
            with open(json_path, "w") as f:
                json.dump(stats, f, indent=2)
            # Ï≤´ Î∞∞Ïπò Ï≤´ ÏÉòÌîå Ï†ÄÏû• (0~1 Í∞í Í∞ÄÏ†ï)
            png_path = os.path.join(out_dir, f"disp_{self._disp_stats_idx:05d}.png")
            save_image(disp[0:1], png_path)
            # Ïù∏Îç±Ïä§ Ï¶ùÍ∞Ä (EVERY Î™®Îìú ÎåÄÎπÑ)
            self._disp_stats_idx += 1
        except Exception as e:
            print("[DISP_STATS][SAVE_ERROR]", e)
        # ===============================

        if once:
            self._disp_stats_done = True
            # Ïù¥ÌõÑ Ï†ÄÏû• Î∞òÎ≥µ Î∞©ÏßÄ (EVERYÍ∞Ä ÏïÑÎãàÎ©¥ Ïù∏Îç±Ïä§ Í≥†Ï†ï)

    def run_network(self, rgb, input_depth=None):
        """
        üÜï Enhanced network execution with improved LiDAR processing
        """
        # Encode RGB features
        skip_features = self.encoder(rgb)
        
        # Enhanced sparse depth processing
        if input_depth is not None:
            self.mconvs.prep(input_depth)
            
            fused_features = []
            for i, feat in enumerate(skip_features):
                # üÜï Enhanced FiLM application
                if self.use_film and i in self.film_scales:
                    result = self.mconvs(feat)
                    
                    if isinstance(result, tuple) and len(result) == 3:
                        sparse_feat, gamma, beta = result
                        
                        # üÜï Improved FiLM with feature refinement
                        if self.use_enhanced_lidar and str(i) in self.feature_refinement:
                            attention_map = self.feature_refinement[str(i)](feat)
                            refined_feat = feat * attention_map
                        else:
                            refined_feat = feat
                        
                        # Enhanced FiLM application
                        modulated_feat = gamma * refined_feat + beta
                        
                        # üÜï Adaptive fusion based on feature importance
                        fusion_weight = torch.sigmoid(self.weight[i])
                        fused_feat = (fusion_weight * modulated_feat + 
                                     (1 - fusion_weight) * sparse_feat + 
                                     self.bias[i].view(1, 1, 1, 1))
                    else:
                        sparse_feat = result
                        fusion_weight = torch.sigmoid(self.weight[i])
                        fused_feat = (fusion_weight * feat + 
                                     (1 - fusion_weight) * sparse_feat + 
                                     self.bias[i].view(1, 1, 1, 1))
                else:
                    # Standard fusion
                    sparse_feat = self.mconvs(feat)
                    fusion_weight = torch.sigmoid(self.weight[i])
                    fused_feat = (fusion_weight * feat + 
                                 (1 - fusion_weight) * sparse_feat + 
                                 self.bias[i].view(1, 1, 1, 1))
                
                fused_features.append(fused_feat)
            
            skip_features = fused_features
        
        # Decode to get outputs
        outputs = self.decoder(skip_features)  # ("disp", i) is sigmoid output [0, 1]

        # üÜï Convert to depth based on depth_output_mode
        if self.depth_output_mode == 'direct':
            # Direct Linear Depth Output
            depth_outputs = []
            for i in range(4):
                sigmoid = outputs[("disp", i)]
                # Linear transformation: depth = min + (max - min) * sigmoid
                depth = self.min_depth + (self.max_depth - self.min_depth) * sigmoid
                depth_outputs.append(depth)
            
            if not hasattr(self, "_direct_mode_logged"):
                print(f"\n[ResNetSAN01] Direct Depth Output mode")
                print(f"   Range: [{self.min_depth}, {self.max_depth}]m")
                print(f"   INT8 quantization error: ¬±{(self.max_depth - self.min_depth) / 255 / 2 * 1000:.1f}mm (uniform)")
                self._direct_mode_logged = True
        else:
            # Sigmoid Output (legacy, for Bounded Inverse transformation)
            depth_outputs = []
            for i in range(4):
                sigmoid = outputs[("disp", i)]
                # Bounded Inverse: inv = inv_min + (inv_max - inv_min) * sigmoid
                inv_min = 1.0 / self.max_depth
                inv_max = 1.0 / self.min_depth
                inv_depth = inv_min + (inv_max - inv_min) * sigmoid
                depth = 1.0 / (inv_depth + 1e-8)
                depth_outputs.append(depth)
            
            if not hasattr(self, "_sigmoid_mode_logged"):
                print(f"\n[ResNetSAN01] Sigmoid ‚Üí Bounded Inverse mode (legacy)")
                print(f"   Range: [{self.min_depth}, {self.max_depth}]m")
                print(f"   Warning: INT8 error @ {self.max_depth}m: ~{(self.max_depth - self.min_depth) * 434 / 255 * 1000:.0f}mm")
                self._sigmoid_mode_logged = True

        if self.training:
            if hasattr(self, "_maybe_log_disp_stats"):
                self._maybe_log_disp_stats(outputs)
            # Training: return 4 scales of depth outputs
            return depth_outputs, skip_features
        else:
            # Inference: return 1 scale of depth output
            return [depth_outputs[0]], skip_features

    def forward(self, rgb, input_depth=None, **kwargs):
        """
        üÜï Enhanced forward pass with improved LiDAR integration
        
        Returns:
            dict with 'inv_depths' key (name kept for backward compatibility)
            - If depth_output_mode='direct': contains direct depth values
            - If depth_output_mode='sigmoid': contains bounded inverse depth values
        """
        if not self.training:
            depths, _ = self.run_network(rgb, input_depth)
            return {'inv_depths': depths}  # Keep key name for compatibility

        output = {}
        
        # RGB-only forward pass
        depths_rgb, skip_feat_rgb = self.run_network(rgb)
        output['inv_depths'] = depths_rgb  # Keep key name for compatibility
        
        if input_depth is None:
            return output
        
        # RGB+D forward pass with enhanced processing
        depths_rgbd, skip_feat_rgbd = self.run_network(rgb, input_depth)
        output['inv_depths_rgbd'] = depths_rgbd  # Keep key name for compatibility
        
        # üÜï Enhanced consistency loss with feature-level weighting
        feature_weights = torch.softmax(torch.abs(self.weight), dim=0)
        weighted_loss = sum([
            weight * ((feat_rgbd.detach() - feat_rgb) ** 2).mean()
            for weight, feat_rgbd, feat_rgb in zip(feature_weights, skip_feat_rgbd, skip_feat_rgb)
        ]) / len(skip_feat_rgbd)
        
        output['depth_loss'] = weighted_loss
        
        return output