# Scale-Adaptive Loss í†µí•© ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ëª©ì°¨
1. [Parameter Flow ì™„ì „ ë¶„ì„](#1-parameter-flow-ì™„ì „-ë¶„ì„)
2. [Training Flow ìƒì„¸ ë¶„ì„](#2-training-flow-ìƒì„¸-ë¶„ì„)
3. [ê¸°ì¡´ Loss Functions íŒ¨í„´ ë¶„ì„](#3-ê¸°ì¡´-loss-functions-íŒ¨í„´-ë¶„ì„)
4. [ë¹ ì§„ ë¶€ë¶„ ë° í•„ìš”í•œ ìˆ˜ì •ì‚¬í•­](#4-ë¹ ì§„-ë¶€ë¶„-ë°-í•„ìš”í•œ-ìˆ˜ì •ì‚¬í•­)
5. [ì™„ì „í•œ í†µí•© ì²´í¬ë¦¬ìŠ¤íŠ¸](#5-ì™„ì „í•œ-í†µí•©-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## 1. Parameter Flow ì™„ì „ ë¶„ì„

### 1.1 ì „ì²´ íŒŒë¼ë¯¸í„° íë¦„ë„

```
YAML Config (train_*.yaml)
    â†“
    model:
      params:
        min_depth: 0.0
        max_depth: 80.0
      loss:
        supervised_method: 'sparse-scale-adaptive'
        supervised_num_scales: 4
        lambda_sg: 0.5
        use_inv_depth: false
        alpha_schedule: 'linear'
        # ... other params
    â†“
ModelWrapper.__init__(config)
    â†“
SemiSupCompletionModel.__init__(
    min_depth=config.model.params.min_depth,  # âœ… ëª…ì‹œì  ì „ë‹¬
    max_depth=config.model.params.max_depth,  # âœ… ëª…ì‹œì  ì „ë‹¬
    **config.model.loss  # supervised_method, lambda_sg ë“±
)
    â†“
SupervisedLoss.__init__(
    supervised_method='sparse-scale-adaptive',
    min_depth=min_depth,  # âœ… ë¶€ëª¨ì—ì„œ ë°›ìŒ
    max_depth=max_depth,  # âœ… ë¶€ëª¨ì—ì„œ ë°›ìŒ
    **kwargs  # lambda_sg, use_inv_depth, alpha_schedule ë“±
)
    â†“
get_loss_func(
    supervised_method='sparse-scale-adaptive',
    min_depth=min_depth,  # âœ… kwargsë¡œ ì „ë‹¬
    max_depth=max_depth,  # âœ… kwargsë¡œ ì „ë‹¬
    lambda_sg=lambda_sg,
    use_inv_depth=use_inv_depth,
    alpha_schedule=alpha_schedule,
    # ... other params
)
    â†“
ScaleAdaptiveLoss.__init__(
    min_depth=kwargs.get('min_depth', 0.1),
    max_depth=kwargs.get('max_depth', 100.0),
    lambda_sg=kwargs.get('lambda_sg', 0.5),
    use_inv_depth=kwargs.get('use_inv_depth', False),
    alpha_schedule=kwargs.get('alpha_schedule', 'linear'),
    # ... other params
)
```

### 1.2 Runtime Parameter Flow (Training Step)

```python
# ModelWrapper.training_step (model_wrapper.py:266)
def training_step(self, batch, batch_idx):
    # Progress ê³„ì‚°
    progress = self.current_epoch / self.config.arch.max_epochs
    
    # Model forward (progress ì „ë‹¬)
    model_output = self.model(batch, progress=progress)
    
    return model_output['loss']

# SemiSupCompletionModel.forward (SemiSupCompletionModel.py:~200)
def forward(self, batch, return_logs=False, progress=0.0):
    # ... depth prediction ...
    
    # Supervised loss ê³„ì‚° (progress ì „ë‹¬)
    sup_output = self.supervised_loss(
        pred_inv_depths,
        gt_inv_depths,
        return_logs=return_logs,
        progress=progress,  # âœ… Progress ì „ë‹¬
        masks=masks  # âœ… Mask ì „ë‹¬
    )
    
    return {'loss': loss, ...}

# SupervisedLoss.forward (supervised_loss.py:277)
def forward(self, inv_depths, gt_inv_depth, return_logs=False, progress=0.0, masks=None):
    # âœ… Progress ì €ì¥ (loss functionì—ì„œ ì‚¬ìš©)
    self._progress = progress
    
    # Multi-scale GT depth ìƒì„±
    gt_inv_depths = match_scales(gt_inv_depth, inv_depths, self.n, ...)
    
    # Loss ê³„ì‚° (ë§ˆìŠ¤í¬ í¬í•¨)
    loss = self.calculate_loss(inv_depths, gt_inv_depths, masks=masks)
    
    return {'loss': loss, 'metrics': self.metrics}

# SupervisedLoss.calculate_loss (supervised_loss.py:149)
def calculate_loss(self, inv_depths, gt_inv_depths, masks=None):
    for i in range(num_scales):
        # Sparse ë§ˆìŠ¤í¬ ìƒì„±
        valid_mask = (gt_inv_depths[i] > 0.).detach()
        
        # ì¶”ê°€ ë§ˆìŠ¤í¬ ê²°í•©
        if masks is not None and i < len(masks):
            current_mask = valid_mask & masks[i]
        else:
            current_mask = valid_mask
        
        # Loss function signature ê²€ì‚¬
        loss_kwargs = {}
        if hasattr(self.loss_func, 'forward'):
            sig = inspect.signature(self.loss_func.forward)
            params = sig.parameters
            if 'mask' in params:
                loss_kwargs['mask'] = current_mask
            if 'progress' in params:
                loss_kwargs['progress'] = self._progress  # âœ… Progress ì „ë‹¬
        
        # Loss ê³„ì‚°
        loss_i = self.loss_func(pred_filled, gt_filled, **loss_kwargs)
        
    return total_loss / num_scales
```

---

## 2. Training Flow ìƒì„¸ ë¶„ì„

### 2.1 ModelWrapper í•µì‹¬ ë©”ì„œë“œ

```python
# packnet_sfm/models/model_wrapper.py

class ModelWrapper(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # Model ì´ˆê¸°í™” (config ì „ì²´ ì „ë‹¬)
        self.model = setup_model(config.model)
        
    @property
    def progress(self):
        """Training progress (0.0 ~ 1.0)"""
        return self.current_epoch / self.config.arch.max_epochs
    
    def training_step(self, batch, batch_idx):
        """
        í•µì‹¬ training step
        - progressë¥¼ model.forward()ì— ì „ë‹¬
        - modelì´ loss ê³„ì‚° ë° ë°˜í™˜
        """
        # âœ… Progress ì „ë‹¬
        model_output = self.model(batch, progress=self.progress)
        
        # Loss ë°˜í™˜
        return model_output['loss']
    
    def configure_optimizers(self):
        """Optimizerì™€ scheduler ì„¤ì •"""
        # Depth network optimizer
        depth_params = self.model.depth_net.parameters()
        self.optimizer = setup_optimizer(depth_params, self.config.model.optimizer)
        
        # Scheduler
        self.scheduler = setup_scheduler(self.optimizer, self.config.model.scheduler)
        
        return self.optimizer
```

### 2.2 Progress ì „ë‹¬ ë©”ì»¤ë‹ˆì¦˜

| Level | Component | Progress ì „ë‹¬ ì—¬ë¶€ | ë¹„ê³  |
|-------|-----------|-------------------|------|
| 1 | `ModelWrapper.training_step()` | **ìƒì„±** | `self.current_epoch / max_epochs` |
| 2 | `SemiSupModel.forward()` | âœ… **ì „ë‹¬** | `progress=progress` parameter |
| 3 | `SupervisedLoss.forward()` | âœ… **ì €ì¥** | `self._progress = progress` |
| 4 | `SupervisedLoss.calculate_loss()` | âœ… **í™œìš©** | `loss_kwargs['progress'] = self._progress` |
| 5 | `ScaleAdaptiveLoss.forward()` | âœ… **ìˆ˜ì‹ ** | `forward(..., progress=0.0)` parameter |

**âœ… ProgressëŠ” ì™„ì „íˆ ì „ë‹¬ë˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤!**

### 2.3 Mask ì „ë‹¬ ë©”ì»¤ë‹ˆì¦˜

```python
# 1ë‹¨ê³„: Batchì—ì„œ mask ì¶”ì¶œ (SemiSupCompletionModel)
masks = batch.get('mask', None)  # Optional binary mask

# 2ë‹¨ê³„: Multi-scale masks ìƒì„±
if masks is not None:
    masks_list = [
        F.interpolate(masks, size=inv_depths[i].shape[-2:], mode='nearest')
        for i in range(len(inv_depths))
    ]

# 3ë‹¨ê³„: SupervisedLossì— ì „ë‹¬
sup_output = self.supervised_loss(
    inv_depths, gt_inv_depths,
    masks=masks_list  # âœ… Multi-scale masks
)

# 4ë‹¨ê³„: calculate_lossì—ì„œ valid_maskì™€ ê²°í•©
valid_mask = (gt_inv_depths[i] > 0.).detach()  # Sparse GT mask
if masks is not None and i < len(masks):
    current_mask = valid_mask & masks[i]  # âœ… ê²°í•©
else:
    current_mask = valid_mask

# 5ë‹¨ê³„: Loss functionì— ì „ë‹¬ (signature ê²€ì‚¬ í›„)
if 'mask' in sig.parameters:
    loss_kwargs['mask'] = current_mask
```

**âœ… Maskë„ ì™„ì „íˆ ì „ë‹¬ë˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤!**

---

## 3. ê¸°ì¡´ Loss Functions íŒ¨í„´ ë¶„ì„

### 3.1 SSISilogLoss íŒ¨í„´ (ssi_silog_loss.py)

```python
class SSISilogLoss(LossBase):
    def __init__(self, alpha=0.85, silog_ratio=10, silog_ratio2=0.85,
                 ssi_weight=0.7, silog_weight=0.3,
                 min_depth: Optional[float] = None,
                 max_depth: Optional[float] = None):
        super().__init__()
        # âœ… min/max_depthë¥¼ __init__ì—ì„œ ë°›ìŒ
        self.min_depth = min_depth
        self.max_depth = max_depth
        
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None, progress=None):
        """
        âœ… íŒ¨í„´:
        - mask parameter ìˆ˜ì‹  (optional)
        - progress parameter ìˆ˜ì‹  (optional, ì‚¬ìš© ì•ˆí•¨)
        """
        # SSI component (inverse depth)
        ssi_loss = self.compute_ssi_loss(pred_inv_depth, gt_inv_depth, mask)
        
        # Silog component (depth, inv2depth ë³€í™˜)
        silog_loss = self.compute_silog_loss(pred_inv_depth, gt_inv_depth, mask)
        
        # Combine
        total_loss = self.ssi_weight * ssi_loss + self.silog_weight * silog_loss
        
        # âœ… Metrics ì €ì¥ (LossBase ìƒì†)
        self.add_metric('ssi_component', ssi_loss)
        self.add_metric('silog_component', silog_loss)
        
        return total_loss
```

**í•µì‹¬ íŒ¨í„´:**
1. `mask` parameterëŠ” optional (ê¸°ë³¸ê°’ None)
2. `progress` parameterëŠ” optional (ì‚¬ìš© ì—¬ë¶€ëŠ” lossë§ˆë‹¤ ë‹¤ë¦„)
3. `LossBase` ìƒì†ìœ¼ë¡œ `add_metric()` ì‚¬ìš©
4. `min_depth`, `max_depth`ëŠ” `__init__`ì—ì„œ ë°›ìŒ

### 3.2 EnhancedSSILoss íŒ¨í„´ (ssi_loss_enhanced.py)

```python
class EnhancedSSILoss(LossBase):
    def __init__(self, alpha=0.85, l1_weight=0.2, ssi_weight=0.8,
                 adaptive_weighting=True):
        super().__init__()
        self.adaptive_weighting = adaptive_weighting
        
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None, progress=None):
        """
        âœ… Progressë¥¼ adaptive weightingì— í™œìš©
        """
        # Adaptive weights ê³„ì‚°
        ssi_weight, l1_weight = self.get_adaptive_weights(progress)
        
        # Loss ê³„ì‚°
        ssi_loss = self.compute_ssi_loss(pred_inv_depth, gt_inv_depth, mask)
        l1_loss = self.compute_l1_loss(pred_inv_depth, gt_inv_depth, mask)
        
        # Combine with adaptive weights
        total_loss = ssi_weight * ssi_loss + l1_weight * l1_loss
        
        # âœ… Adaptive weights ê¸°ë¡
        self.add_metric('dynamic_ssi_weight', ssi_weight)
        self.add_metric('dynamic_l1_weight', l1_weight)
        
        return total_loss
    
    def get_adaptive_weights(self, progress=None):
        """
        âœ… Progress ê¸°ë°˜ adaptive weighting
        Early: SSI ìœ„ì£¼ (0.9)
        Later: Balanced (0.8/0.2)
        """
        if not self.adaptive_weighting or progress is None:
            return self.ssi_weight, self.l1_weight
        
        progress = max(0.0, min(1.0, progress))
        ssi_weight = self.ssi_weight + (1.0 - progress) * 0.1
        l1_weight = self.l1_weight + progress * 0.1
        
        # Normalize
        total = ssi_weight + l1_weight
        return ssi_weight / total, l1_weight / total
```

**í•µì‹¬ íŒ¨í„´:**
1. `progress`ë¥¼ **ì ê·¹ í™œìš©** (adaptive weighting)
2. `mask`ëŠ” ëª¨ë“  sub-lossì— ì „ë‹¬
3. Adaptive weightsë¥¼ metricsë¡œ ê¸°ë¡

### 3.3 SSILoss íŒ¨í„´ (ssi_loss.py)

```python
class SSILoss(LossBase):
    def __init__(self, alpha=0.85):
        super().__init__()
        self.alpha = alpha
        
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
        """
        âœ… ê°€ì¥ ë‹¨ìˆœí•œ íŒ¨í„´
        - progress ì•ˆë°›ìŒ
        - maskë§Œ optional
        """
        if mask is None:
            mask = torch.ones_like(pred_inv_depth, dtype=torch.bool)
        
        if mask.sum() == 0:
            return torch.tensor(0.0, device=pred_inv_depth.device)
        
        diff = (pred_inv_depth[mask] - gt_inv_depth[mask])
        diff2 = diff ** 2
        mean = diff.mean()
        var = diff2.mean() - mean ** 2
        
        return var + self.alpha * mean ** 2
```

**í•µì‹¬ íŒ¨í„´:**
1. ê°€ì¥ ê¸°ë³¸ì ì¸ loss - `mask`ë§Œ ë°›ìŒ
2. `progress` ì—†ìŒ (ì •ì  loss)
3. Mask ê¸°ë³¸ê°’ ì²˜ë¦¬

---

## 4. ë¹ ì§„ ë¶€ë¶„ ë° í•„ìš”í•œ ìˆ˜ì •ì‚¬í•­

### 4.1 âŒ í˜„ì¬ ë¬¸ì„œì— ë¹ ì§„ ë¶€ë¶„

#### 4.1.1 `get_loss_func()` ì—…ë°ì´íŠ¸ ëˆ„ë½

**ë¬¸ì œì :**
```python
# supervised_loss.pyì˜ get_loss_func()ì— Scale-Adaptive ì¼€ì´ìŠ¤ ì¶”ê°€ í•„ìš”
def get_loss_func(supervised_method, **kwargs):
    # ...
    elif supervised_method.endswith('ssi-silog'):
        return SSISilogLoss(
            min_depth=kwargs.get('min_depth', None),
            max_depth=kwargs.get('max_depth', None),
        )
    # âŒ ì´ ë¶€ë¶„ì´ ë¬¸ì„œì— ì—†ìŒ!
    elif supervised_method.endswith('scale-adaptive'):
        return ScaleAdaptiveLoss(
            min_depth=kwargs.get('min_depth', 0.1),
            max_depth=kwargs.get('max_depth', 100.0),
            lambda_sg=kwargs.get('lambda_sg', 0.5),
            use_inv_depth=kwargs.get('use_inv_depth', False),
            alpha_schedule=kwargs.get('alpha_schedule', 'linear'),
            scale_schedule=kwargs.get('scale_schedule', 'linear'),
            num_scales=kwargs.get('num_scales', 4),
        )
    # ...
```

#### 4.1.2 Import ë¬¸ ëˆ„ë½

**ë¬¸ì œì :**
```python
# supervised_loss.py ìƒë‹¨ì— import ì¶”ê°€ í•„ìš”
from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss
```

#### 4.1.3 Progress Parameter í™œìš© ì˜ˆì‹œ ë¶€ì¡±

**í˜„ì¬ ë¬¸ì„œ:**
- `progress` parameterë¥¼ ë°›ëŠ”ë‹¤ê³ ë§Œ ëª…ì‹œ
- **ì‹¤ì œ í™œìš© ë°©ë²• ì˜ˆì‹œ ë¶€ì¡±**

**í•„ìš”í•œ ë‚´ìš©:**
```python
def forward(self, pred, gt, mask=None, progress=0.0):
    """
    progress í™œìš© ì˜ˆì‹œ:
    - alpha_t = self.get_alpha(progress)  # Adaptive alpha
    - scales = self.get_active_scales(progress)  # Progressive scaling
    """
```

#### 4.1.4 Mask ì²˜ë¦¬ ìƒì„¸ ì„¤ëª… ë¶€ì¡±

**í˜„ì¬ ë¬¸ì„œ:**
- `mask` parameterë¥¼ ë°›ëŠ”ë‹¤ê³ ë§Œ ëª…ì‹œ
- **Sparse GT maskì™€ì˜ ê²°í•© ë°©ë²• ë¯¸ì„¤ëª…**

**í•„ìš”í•œ ë‚´ìš©:**
```python
def forward(self, pred, gt, mask=None, progress=0.0):
    """
    Mask ì²˜ë¦¬:
    1. SupervisedLoss.calculate_loss()ì—ì„œ:
       - valid_mask = (gt > 0.).detach()  # Sparse GT
       - current_mask = valid_mask & mask  # ê²°í•©
    
    2. Loss function ë‚´ë¶€ì—ì„œ:
       - mask ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì´ë¯¸ ê²°í•©ë¨)
       - pred[mask], gt[mask]ë¡œ í•„í„°ë§
    """
```

### 4.2 âœ… í•„ìš”í•œ ìˆ˜ì •ì‚¬í•­

#### ìˆ˜ì • 1: `supervised_loss.py` - `get_loss_func()` ì—…ë°ì´íŠ¸

```python
# packnet_sfm/losses/supervised_loss.py

# ìƒë‹¨ì— import ì¶”ê°€
from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss

def get_loss_func(supervised_method, **kwargs):
    """Determines the supervised loss to be used, given the supervised method."""
    print(f"ğŸ” Loading loss function for: {supervised_method}")
    
    if supervised_method.endswith('l1'):
        return nn.L1Loss()
    elif supervised_method.endswith('mse'):
        return nn.MSELoss()
    elif supervised_method.endswith('berhu'):
        return BerHuLoss()
    elif supervised_method.endswith('ssi-silog'):
        return SSISilogLoss(
            min_depth=kwargs.get('min_depth', None),
            max_depth=kwargs.get('max_depth', None),
        )
    elif supervised_method.endswith('silog'):
        return SilogLoss()
    elif supervised_method.endswith('abs_rel'):
        return lambda x, y: torch.mean(torch.abs(x - y) / x)
    elif supervised_method.endswith('ssi'):
        return SSILoss()
    elif supervised_method.endswith('enhanced-ssi'):
        return EnhancedSSILoss()
    elif supervised_method.endswith('progressive-ssi'):
        return ProgressiveEnhancedSSILoss()
    elif supervised_method.endswith('ssi-trim'):
        return SSITrimLoss(trim=0.2, epsilon=1e-6)
    # âœ… ì¶”ê°€: Scale-Adaptive Loss
    elif supervised_method.endswith('scale-adaptive'):
        return ScaleAdaptiveLoss(
            min_depth=kwargs.get('min_depth', 0.1),
            max_depth=kwargs.get('max_depth', 100.0),
            lambda_sg=kwargs.get('lambda_sg', 0.5),
            use_inv_depth=kwargs.get('use_inv_depth', False),
            alpha_schedule=kwargs.get('alpha_schedule', 'linear'),
            scale_schedule=kwargs.get('scale_schedule', 'linear'),
            num_scales=kwargs.get('num_scales', 4),
        )
    else:
        raise ValueError('Unknown supervised loss {}'.format(supervised_method))
```

#### ìˆ˜ì • 2: `scale_adaptive_loss.py` - Forward Signature ëª…í™•í™”

```python
class ScaleAdaptiveLoss(LossBase):
    def __init__(self, ...):
        # ... (ê¸°ì¡´ __init__)
        
    def forward(self, pred, gt, mask=None, progress=0.0):
        """
        Scale-Adaptive Loss forward pass
        
        Parameters
        ----------
        pred : torch.Tensor [B,1,H,W]
            Predicted depth or inverse depth (depends on use_inv_depth)
        gt : torch.Tensor [B,1,H,W]
            Ground-truth depth or inverse depth (depends on use_inv_depth)
        mask : torch.Tensor [B,1,H,W], optional
            Combined binary mask (already includes sparse GT mask)
            - SupervisedLoss.calculate_loss()ì—ì„œ ì´ë¯¸ ê²°í•©ë¨
            - valid_mask (gt > 0) & custom_mask
        progress : float, optional
            Training progress [0.0, 1.0]
            - Used for adaptive alpha and scale weighting
            
        Returns
        -------
        loss : torch.Tensor [1]
            Total scale-adaptive loss
            
        Notes
        -----
        1. MaskëŠ” ì´ë¯¸ SupervisedLossì—ì„œ ê²°í•©ëœ ìƒíƒœë¡œ ë“¤ì–´ì˜µë‹ˆë‹¤:
           - Sparse GT mask (gt > 0)ì™€ custom maskê°€ AND ì—°ì‚°ë¨
        
        2. ProgressëŠ” ModelWrapperì—ì„œ ê³„ì‚°ë˜ì–´ ì „ë‹¬ë©ë‹ˆë‹¤:
           - progress = current_epoch / max_epochs
        
        3. use_inv_depth ì²˜ë¦¬:
           - True: pred/gtë¥¼ inverse depthë¡œ ê°„ì£¼, ì§ì ‘ ì‚¬ìš©
           - False: pred/gtë¥¼ inverse depthë¡œ ê°„ì£¼, depthë¡œ ë³€í™˜
        """
        # 1. Mask ê¸°ë³¸ê°’ ì²˜ë¦¬
        if mask is None:
            mask = torch.ones_like(pred, dtype=torch.bool)
        
        if mask.sum() == 0:
            return torch.tensor(0.0, device=pred.device, requires_grad=True)
        
        # 2. Depth ë³€í™˜ (use_inv_depthì— ë”°ë¼)
        if self.use_inv_depth:
            # Inverse depthë¡œ ì§ì ‘ ì‚¬ìš©
            pred_for_loss = pred
            gt_for_loss = gt
        else:
            # Depthë¡œ ë³€í™˜
            pred_for_loss = inv2depth(pred)
            gt_for_loss = inv2depth(gt)
            # Clamp
            pred_for_loss = torch.clamp(pred_for_loss, self.min_depth, self.max_depth)
            gt_for_loss = torch.clamp(gt_for_loss, self.min_depth, self.max_depth)
        
        # 3. Adaptive parameters ê³„ì‚°
        alpha_t = self.get_alpha(progress)
        scale_weights = self.get_scale_weights(progress)
        
        # 4. Multi-scale loss ê³„ì‚°
        total_loss = 0.0
        for scale in range(self.num_scales):
            # Downsampling for multi-scale
            h, w = pred_for_loss.shape[-2:]
            factor = 2 ** scale
            
            if scale > 0:
                pred_s = F.interpolate(pred_for_loss, size=(h//factor, w//factor), mode='bilinear')
                gt_s = F.interpolate(gt_for_loss, size=(h//factor, w//factor), mode='nearest')
                mask_s = F.interpolate(mask.float(), size=(h//factor, w//factor), mode='nearest') > 0.5
            else:
                pred_s, gt_s, mask_s = pred_for_loss, gt_for_loss, mask
            
            # MAD normalization
            pred_n, gt_n = self.mad_normalize(pred_s, gt_s, mask_s)
            
            # L1 component
            l1_loss = torch.abs(pred_n - gt_n)[mask_s].mean()
            
            # Gradient component
            grad_loss = self.compute_gradient_loss(pred_n, gt_n, mask_s)
            
            # Combine with scale weight
            scale_loss = (1.0 - self.lambda_sg) * l1_loss + self.lambda_sg * grad_loss
            total_loss += scale_weights[scale] * scale_loss
            
            # âœ… Per-scale metrics
            self.add_metric(f'scale{scale}_loss', scale_loss)
            self.add_metric(f'scale{scale}_l1', l1_loss)
            self.add_metric(f'scale{scale}_grad', grad_loss)
        
        # âœ… Global metrics
        self.add_metric('total_loss', total_loss)
        self.add_metric('alpha_t', alpha_t)
        self.add_metric('num_valid_pixels', mask.sum())
        
        return total_loss
```

#### ìˆ˜ì • 3: Documentation ì—…ë°ì´íŠ¸

**SCALE_ADAPTIVE_LOSS_IMPLEMENTATION.mdì— ì¶”ê°€í•  ì„¹ì…˜:**

```markdown
## 7. Integration Details

### 7.1 Parameter Flow from YAML to Loss

1. **YAML Configuration**
   ```yaml
   model:
     params:
       min_depth: 0.0
       max_depth: 80.0
     loss:
       supervised_method: 'sparse-scale-adaptive'
       supervised_num_scales: 4
       lambda_sg: 0.5
       use_inv_depth: false
       alpha_schedule: 'linear'
   ```

2. **Model Initialization**
   ```python
   # SemiSupCompletionModel.__init__()
   self._supervised_loss = SupervisedLoss(
       min_depth=min_depth,  # From config.model.params
       max_depth=max_depth,
       **kwargs  # From config.model.loss
   )
   ```

3. **Loss Function Creation**
   ```python
   # SupervisedLoss.__init__() â†’ get_loss_func()
   self.loss_func = get_loss_func(
       supervised_method='sparse-scale-adaptive',
       min_depth=min_depth,
       max_depth=max_depth,
       lambda_sg=kwargs.get('lambda_sg', 0.5),
       use_inv_depth=kwargs.get('use_inv_depth', False),
       # ... other params
   )
   ```

### 7.2 Runtime Data Flow

1. **Training Step**
   ```python
   # ModelWrapper.training_step()
   progress = self.current_epoch / self.max_epochs
   model_output = self.model(batch, progress=progress)
   ```

2. **Model Forward**
   ```python
   # SemiSupModel.forward()
   sup_output = self.supervised_loss(
       pred_inv_depths,
       gt_inv_depths,
       progress=progress,  # âœ… Passed
       masks=masks  # âœ… Optional
   )
   ```

3. **Supervised Loss**
   ```python
   # SupervisedLoss.forward()
   self._progress = progress  # Store for loss function
   
   # SupervisedLoss.calculate_loss()
   loss_kwargs = {}
   if 'mask' in sig.parameters:
       loss_kwargs['mask'] = current_mask  # Combined mask
   if 'progress' in sig.parameters:
       loss_kwargs['progress'] = self._progress  # âœ… Passed
   
   loss = self.loss_func(pred, gt, **loss_kwargs)
   ```

4. **Scale-Adaptive Loss**
   ```python
   # ScaleAdaptiveLoss.forward(pred, gt, mask, progress)
   alpha_t = self.get_alpha(progress)  # Use progress
   # ... compute loss ...
   ```

### 7.3 Mask Handling

**Important:** The `mask` parameter in `ScaleAdaptiveLoss.forward()` is **already combined**:

```python
# In SupervisedLoss.calculate_loss():
valid_mask = (gt_inv_depths[i] > 0.).detach()  # Sparse GT mask
if masks is not None:
    current_mask = valid_mask & masks[i]  # âœ… Combined!
else:
    current_mask = valid_mask

# Then passed to loss function:
loss = self.loss_func(pred, gt, mask=current_mask)  # âœ… Already combined
```

**You should NOT re-combine masks in your loss function!**

### 7.4 Required Code Changes

#### Change 1: Update `supervised_loss.py`

```python
# Add import at top
from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss

# Add case in get_loss_func()
elif supervised_method.endswith('scale-adaptive'):
    return ScaleAdaptiveLoss(
        min_depth=kwargs.get('min_depth', 0.1),
        max_depth=kwargs.get('max_depth', 100.0),
        lambda_sg=kwargs.get('lambda_sg', 0.5),
        use_inv_depth=kwargs.get('use_inv_depth', False),
        alpha_schedule=kwargs.get('alpha_schedule', 'linear'),
        scale_schedule=kwargs.get('scale_schedule', 'linear'),
        num_scales=kwargs.get('num_scales', 4),
    )
```

#### Change 2: Ensure `scale_adaptive_loss.py` follows signature

```python
def forward(self, pred, gt, mask=None, progress=0.0):
    # âœ… Correct signature
    # mask: Already combined (sparse GT & custom)
    # progress: From ModelWrapper
```
```

---

## 5. ì™„ì „í•œ í†µí•© ì²´í¬ë¦¬ìŠ¤íŠ¸

### 5.1 ì½”ë“œ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **1. íŒŒì¼ ìƒì„±**
  - [ ] `packnet_sfm/losses/scale_adaptive_loss.py` ìƒì„±
  
- [ ] **2. ScaleAdaptiveLoss í´ë˜ìŠ¤ êµ¬í˜„**
  - [ ] `LossBase` ìƒì†
  - [ ] `__init__()`: ëª¨ë“  íŒŒë¼ë¯¸í„° ìˆ˜ì‹  (min/max_depth, lambda_sg, use_inv_depth, schedules ë“±)
  - [ ] `forward(pred, gt, mask=None, progress=0.0)`: ì •í™•í•œ signature
  - [ ] `mad_normalize()`: MAD normalization êµ¬í˜„
  - [ ] `compute_gradient_loss()`: Sobel gradient loss êµ¬í˜„
  - [ ] `get_alpha()`: Progress ê¸°ë°˜ alpha ìŠ¤ì¼€ì¤„ë§
  - [ ] `get_scale_weights()`: Progress ê¸°ë°˜ scale weighting
  
- [ ] **3. supervised_loss.py ìˆ˜ì •**
  - [ ] Import ì¶”ê°€: `from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss`
  - [ ] `get_loss_func()`ì— `'scale-adaptive'` ì¼€ì´ìŠ¤ ì¶”ê°€
  - [ ] ëª¨ë“  kwargs ì „ë‹¬ í™•ì¸
  
- [ ] **4. í…ŒìŠ¤íŠ¸ YAML ì‘ì„±**
  - [ ] `configs/train_*_scale_adaptive.yaml` ìƒì„±
  - [ ] ëª¨ë“  íŒŒë¼ë¯¸í„° ëª…ì‹œ (lambda_sg, use_inv_depth, schedules ë“±)

### 5.2 Parameter Flow ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] **YAML â†’ Model**
  - [x] `min_depth`, `max_depth`: `config.model.params` â†’ `SemiSupModel.__init__()`
  - [x] `supervised_method`: `config.model.loss` â†’ `SupervisedLoss.__init__()`
  - [x] Custom params: `config.model.loss` â†’ `**kwargs`

- [x] **Model â†’ SupervisedLoss**
  - [x] `min_depth`, `max_depth`: ëª…ì‹œì  ì „ë‹¬
  - [x] `**kwargs`: loss-specific íŒŒë¼ë¯¸í„° ì „ë‹¬

- [x] **SupervisedLoss â†’ get_loss_func()**
  - [x] `supervised_method`: ì²« ë²ˆì§¸ ì¸ì
  - [x] `**kwargs`: ëª¨ë“  íŒŒë¼ë¯¸í„° ì „ë‹¬

- [x] **get_loss_func() â†’ ScaleAdaptiveLoss**
  - [x] `kwargs.get('min_depth', default)`: min_depth ì „ë‹¬
  - [x] `kwargs.get('max_depth', default)`: max_depth ì „ë‹¬
  - [x] `kwargs.get('lambda_sg', 0.5)`: lambda_sg ì „ë‹¬
  - [x] `kwargs.get('use_inv_depth', False)`: use_inv_depth ì „ë‹¬
  - [x] ê¸°íƒ€ ëª¨ë“  íŒŒë¼ë¯¸í„° ì „ë‹¬

- [x] **Runtime: Progress ì „ë‹¬**
  - [x] `ModelWrapper.progress` property ì¡´ì¬
  - [x] `model.forward(batch, progress=self.progress)` í˜¸ì¶œ
  - [x] `SupervisedLoss.forward(..., progress=progress)` ì „ë‹¬
  - [x] `self._progress = progress` ì €ì¥
  - [x] `loss_kwargs['progress'] = self._progress` ì „ë‹¬
  - [x] `ScaleAdaptiveLoss.forward(..., progress=0.0)` ìˆ˜ì‹ 

- [x] **Runtime: Mask ì „ë‹¬**
  - [x] `batch.get('mask', None)` ì¶”ì¶œ
  - [x] Multi-scale masks ìƒì„± (í•„ìš”ì‹œ)
  - [x] `SupervisedLoss.forward(..., masks=masks)` ì „ë‹¬
  - [x] `valid_mask & masks[i]` ê²°í•©
  - [x] `loss_kwargs['mask'] = current_mask` ì „ë‹¬
  - [x] `ScaleAdaptiveLoss.forward(..., mask=None)` ìˆ˜ì‹ 

### 5.3 ê¸°ëŠ¥ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ê¸°ë³¸ ê¸°ëŠ¥**
  - [ ] Loss ê°’ì´ ì •ìƒ ê³„ì‚°ë¨ (NaN/Inf ì—†ìŒ)
  - [ ] Gradientê°€ ì •ìƒ ì „íŒŒë¨ (backward ì„±ê³µ)
  - [ ] Multi-scale loss ê³„ì‚° í™•ì¸
  - [ ] MAD normalization ì‘ë™ í™•ì¸
  - [ ] Gradient loss ê³„ì‚° í™•ì¸

- [ ] **Adaptive ê¸°ëŠ¥**
  - [ ] Progress ê¸°ë°˜ alpha ë³€í™” í™•ì¸
  - [ ] Progress ê¸°ë°˜ scale weighting ë³€í™” í™•ì¸
  - [ ] Metrics ì •ìƒ ê¸°ë¡ í™•ì¸ (TensorBoard/WandB)

- [ ] **use_inv_depth ì˜µì…˜**
  - [ ] `use_inv_depth=True`: Inverse depth ì§ì ‘ ì‚¬ìš©
  - [ ] `use_inv_depth=False`: Depth ë³€í™˜ í›„ ì‚¬ìš©
  - [ ] ì„±ëŠ¥ ì°¨ì´ ì¸¡ì •

- [ ] **Mask ì²˜ë¦¬**
  - [ ] Sparse GT mask ì ìš© í™•ì¸ (gt > 0)
  - [ ] Custom mask ê²°í•© í™•ì¸ (ì œê³µì‹œ)
  - [ ] Empty mask ì²˜ë¦¬ í™•ì¸ (return 0.0)

### 5.4 ì„±ëŠ¥ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **í•™ìŠµ ì•ˆì •ì„±**
  - [ ] Lossê°€ ë°œì‚°í•˜ì§€ ì•ŠìŒ
  - [ ] Gradient exploding/vanishing ì—†ìŒ
  - [ ] í•™ìŠµ ê³¡ì„ ì´ ë§¤ë„ëŸ¬ì›€

- [ ] **ì •í™•ë„**
  - [ ] Abs Rel ê°œì„  í™•ì¸
  - [ ] RMSE ê°œì„  í™•ì¸
  - [ ] Î´ < 1.25 ê°œì„  í™•ì¸

- [ ] **ì†ë„**
  - [ ] `use_inv_depth=True`: 15% ë¹ ë¦„ í™•ì¸
  - [ ] Memory ì‚¬ìš©ëŸ‰ 9% ê°ì†Œ í™•ì¸

### 5.5 ë¬¸ì„œí™” ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] **ì´ë¡  ë¬¸ì„œ**
  - [x] SCALE_ADAPTIVE_LOSS.md (í•œêµ­ì–´)

- [x] **êµ¬í˜„ ê°€ì´ë“œ**
  - [x] SCALE_ADAPTIVE_LOSS_IMPLEMENTATION.md (ì˜ì–´)
  - [x] Integration details ì¶”ê°€ í•„ìš” âœ…

- [x] **Quick Start**
  - [x] SCALE_ADAPTIVE_LOSS_QUICK_START.md

- [x] **use_inv_depth ì„¤ëª…**
  - [x] SCALE_ADAPTIVE_LOSS_USE_INV_DEPTH_UPDATE.md

- [x] **í†µí•© ë¶„ì„** (ë³¸ ë¬¸ì„œ)
  - [x] SCALE_ADAPTIVE_LOSS_INTEGRATION_ANALYSIS.md

- [ ] **README ì—…ë°ì´íŠ¸**
  - [ ] ë©”ì¸ README.mdì— Scale-Adaptive Loss ì¶”ê°€
  - [ ] ë§í¬ ì¶”ê°€

---

## 6. ë‹¤ìŒ ë‹¨ê³„ (Action Items)

### ìš°ì„ ìˆœìœ„ 1: ì½”ë“œ êµ¬í˜„

1. **`scale_adaptive_loss.py` êµ¬í˜„**
   ```bash
   # íŒŒì¼ ìƒì„± ìœ„ì¹˜
   packnet_sfm/losses/scale_adaptive_loss.py
   ```
   - ëª¨ë“  ë©”ì„œë“œ êµ¬í˜„ (forward, MAD, gradient, schedules)
   - Signature ì •í™•íˆ ë§ì¶”ê¸°: `forward(pred, gt, mask=None, progress=0.0)`

2. **`supervised_loss.py` ìˆ˜ì •**
   - Import ì¶”ê°€
   - `get_loss_func()`ì— ì¼€ì´ìŠ¤ ì¶”ê°€

3. **YAML config ì‘ì„±**
   ```bash
   configs/train_resnet_san_kitti_scale_adaptive.yaml
   ```

### ìš°ì„ ìˆœìœ„ 2: í…ŒìŠ¤íŠ¸

1. **Unit í…ŒìŠ¤íŠ¸**
   - Loss ê³„ì‚° ì •í™•ì„±
   - Gradient ì „íŒŒ í™•ì¸
   - Edge case ì²˜ë¦¬ (empty mask, progress=0/1 ë“±)

2. **Integration í…ŒìŠ¤íŠ¸**
   - ì‹¤ì œ í•™ìŠµ 1 epoch
   - Metrics ê¸°ë¡ í™•ì¸
   - TensorBoard ì‹œê°í™” í™•ì¸

### ìš°ì„ ìˆœìœ„ 3: ë¬¸ì„œ ì—…ë°ì´íŠ¸

1. **IMPLEMENTATION.md ì—…ë°ì´íŠ¸**
   - Section 7 ì¶”ê°€ (ìœ„ì— ì‘ì„±í•œ ë‚´ìš©)

2. **README.md ì—…ë°ì´íŠ¸**
   - Scale-Adaptive Loss ì†Œê°œ
   - ë¬¸ì„œ ë§í¬ ì¶”ê°€

---

## 7. ìš”ì•½

### âœ… ì˜ ë˜ì–´ìˆëŠ” ë¶€ë¶„

1. **Parameter Flow**: YAML â†’ Model â†’ SupervisedLoss â†’ get_loss_func() â†’ ScaleAdaptiveLoss
   - `min_depth`, `max_depth` ì „ë‹¬ ì™„ë²½
   - `**kwargs`ë¡œ custom params ì „ë‹¬ ì™„ë²½

2. **Progress ì „ë‹¬**: ModelWrapper â†’ Model â†’ SupervisedLoss â†’ Loss Function
   - ëª¨ë“  ë‹¨ê³„ì—ì„œ `progress` parameter ì „ë‹¬
   - EnhancedSSILoss ë“±ì—ì„œ ì´ë¯¸ í™œìš© ì¤‘

3. **Mask ì²˜ë¦¬**: Batch â†’ Model â†’ SupervisedLoss (ê²°í•©) â†’ Loss Function
   - Sparse GT mask (gt > 0)ì™€ custom mask ìë™ ê²°í•©
   - Loss functionì€ ê²°í•©ëœ maskë§Œ ë°›ìŒ

### âŒ ë¹ ì§„ ë¶€ë¶„

1. **`supervised_loss.py` ìˆ˜ì • í•„ìš”**
   - Import ì¶”ê°€
   - `get_loss_func()`ì— `'scale-adaptive'` ì¼€ì´ìŠ¤ ì¶”ê°€

2. **`scale_adaptive_loss.py` êµ¬í˜„ í•„ìš”**
   - ì „ì²´ í´ë˜ìŠ¤ êµ¬í˜„
   - Correct signature: `forward(pred, gt, mask=None, progress=0.0)`

3. **ë¬¸ì„œ ì—…ë°ì´íŠ¸**
   - IMPLEMENTATION.mdì— Integration Details ì¶”ê°€
   - Mask ì²˜ë¦¬ ì£¼ì˜ì‚¬í•­ ëª…ì‹œ

### ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

1. **Loss Function SignatureëŠ” ë°˜ë“œì‹œ:**
   ```python
   def forward(self, pred, gt, mask=None, progress=0.0):
   ```

2. **MaskëŠ” ì´ë¯¸ ê²°í•©ëœ ìƒíƒœë¡œ ë“¤ì–´ì˜´:**
   - `valid_mask (gt > 0) & custom_mask`
   - Loss ë‚´ë¶€ì—ì„œ ì¬ê²°í•© ë¶ˆí•„ìš”

3. **ProgressëŠ” ì„ íƒì  í™œìš©:**
   - Adaptive ê¸°ëŠ¥ í•„ìš”ì‹œ: `get_alpha(progress)`, `get_scale_weights(progress)`
   - ë¶ˆí•„ìš”ì‹œ: ë¬´ì‹œí•´ë„ ë¨ (ê¸°ë³¸ê°’ 0.0)

4. **Metrics ê¸°ë¡ í•„ìˆ˜:**
   - `self.add_metric('metric_name', value)`
   - TensorBoard/WandB ìë™ ê¸°ë¡ë¨

---

**ì´ì œ ì‹¤ì œ `scale_adaptive_loss.py` êµ¬í˜„ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤!** ğŸš€
