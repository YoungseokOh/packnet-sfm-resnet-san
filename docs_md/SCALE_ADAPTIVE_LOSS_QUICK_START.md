# Scale-Adaptive Loss ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸš€ 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

ì´ ë¬¸ì„œëŠ” Scale-Adaptive Lossë¥¼ **ìµœëŒ€í•œ ë¹ ë¥´ê²Œ** í”„ë¡œì íŠ¸ì— ì¶”ê°€í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## âš¡ Step 1: íŒŒì¼ ìƒì„± (2ë¶„)

### 1.1 Loss í´ë˜ìŠ¤ íŒŒì¼ ìƒì„±

```bash
cd /workspace/packnet-sfm
```

`packnet_sfm/losses/scale_adaptive_loss.py` íŒŒì¼ì„ ìƒì„±í•˜ê³  ì•„ë˜ ì½”ë“œë¥¼ ë³µì‚¬:

<details>
<summary>ğŸ“„ ì „ì²´ ì½”ë“œ ë³´ê¸° (í´ë¦­)</summary>

```python
# Copyright 2020 Toyota Research Institute.  All rights reserved.

import torch
import torch.nn as nn
import torch.nn.functional as F

from packnet_sfm.losses.loss_base import LossBase
from packnet_sfm.utils.depth import inv2depth


class ScaleAdaptiveLoss(LossBase):
    """
    G2-MonoDepth Scale-Adaptive Loss
    
    L_total = L_sa + Î»_sg * L_sg
    
    Parameters
    ----------
    lambda_sg : float
        Gradient loss weight (default: 0.5)
    epsilon : float
        Numerical stability constant (default: 1e-8)
    num_scales : int
        Multi-scale levels (default: 4)
    use_absolute : bool
        Use absolute term for valid pixels (default: True)
    use_inv_depth : bool
        If True, compute on inverse depth directly (faster, like SSI)
        If False, convert to depth first (more accurate for gradients)
        Default: False (convert to depth)
    """
    
    def __init__(self, lambda_sg=0.5, epsilon=1e-8, num_scales=4, 
                 use_absolute=True, use_inv_depth=False):
        super().__init__()
        self.lambda_sg = lambda_sg
        self.epsilon = epsilon
        self.num_scales = num_scales
        self.use_absolute = use_absolute
        self.use_inv_depth = use_inv_depth
        
        # Sobel kernels
        self.register_buffer('sobel_x', self._get_sobel_kernel('x'))
        self.register_buffer('sobel_y', self._get_sobel_kernel('y'))
        
        print(f"ğŸ¯ Scale-Adaptive Loss initialized:")
        print(f"   Î»_sg: {lambda_sg}, num_scales: {num_scales}")
        print(f"   use_absolute: {use_absolute}, use_inv_depth: {use_inv_depth}")
    
    def _get_sobel_kernel(self, direction):
        """Create Sobel kernel"""
        if direction == 'x':
            kernel = torch.tensor([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]])
        else:
            kernel = torch.tensor([[-1., -2., -1.], [0., 0., 0.], [1., 2., 1.]])
        return kernel.unsqueeze(0).unsqueeze(0)
    
    def normalize_depth(self, depth):
        """Normalize using MAD"""
        mean = torch.mean(depth, dim=[2, 3], keepdim=True)
        mad = torch.mean(torch.abs(depth - mean), dim=[2, 3], keepdim=True)
        normalized = (depth - mean) / (mad + self.epsilon)
        return normalized, mean, mad
    
    def scale_adaptive_loss(self, pred_depth, gt_depth, valid_mask=None):
        """Compute L_sa = L_relative + L_absolute"""
        # Relative term
        pred_norm, _, _ = self.normalize_depth(pred_depth)
        gt_norm, _, _ = self.normalize_depth(gt_depth)
        relative_loss = torch.mean(torch.abs(pred_norm - gt_norm))
        
        # Absolute term
        absolute_loss = 0.0
        if self.use_absolute and valid_mask is not None:
            valid_pred = pred_depth * valid_mask
            valid_gt = gt_depth * valid_mask
            num_valid = torch.clamp(torch.sum(valid_mask, dim=[1,2,3], keepdim=True), min=1.0)
            absolute_error = torch.abs(valid_pred - valid_gt) * valid_mask
            absolute_loss = torch.sum(absolute_error) / (torch.sum(num_valid) + self.epsilon)
        
        self.add_metric('scale_adaptive/relative', relative_loss)
        if self.use_absolute:
            self.add_metric('scale_adaptive/absolute', absolute_loss)
        
        return relative_loss + absolute_loss
    
    def apply_sobel(self, x, kernel):
        """Apply Sobel operator"""
        x_padded = F.pad(x, (1, 1, 1, 1), mode='replicate')
        return F.conv2d(x_padded, kernel, padding=0)
    
    def scale_invariant_gradient_loss(self, pred_depth, gt_depth):
        """Compute multi-scale gradient loss"""
        pred_norm, _, _ = self.normalize_depth(pred_depth)
        gt_norm, _, _ = self.normalize_depth(gt_depth)
        residual = pred_norm - gt_norm
        
        total_loss = 0.0
        for k in range(1, self.num_scales + 1):
            if k > 1:
                residual_k = F.interpolate(residual, scale_factor=1.0/(2**(k-1)), 
                                          mode='bilinear', align_corners=False)
            else:
                residual_k = residual
            
            grad_x = self.apply_sobel(residual_k, self.sobel_x)
            grad_y = self.apply_sobel(residual_k, self.sobel_y)
            loss_k = torch.mean(torch.abs(grad_x) + torch.abs(grad_y))
            total_loss += loss_k
            self.add_metric(f'gradient/scale_{k}', loss_k)
        
        return total_loss / self.num_scales
    
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
        """Main forward pass"""
        # Convert to depth or use inv_depth directly
        if self.use_inv_depth:
            # Work directly on inverse depth (like SSI)
            pred_data = pred_inv_depth
            gt_data = gt_inv_depth
        else:
            # Convert to depth (like original G2-MonoDepth)
            pred_data = inv2depth(pred_inv_depth)
            gt_data = inv2depth(gt_inv_depth)
        
        # Compute losses
        loss_sa = self.scale_adaptive_loss(pred_data, gt_data, mask)
        loss_sg = self.scale_invariant_gradient_loss(pred_data, gt_data)
        
        total_loss = loss_sa + self.lambda_sg * loss_sg
        
        self.add_metric('total_loss', total_loss)
        self.add_metric('loss_sa', loss_sa)
        self.add_metric('loss_sg', loss_sg)
        
        return total_loss
```

</details>

**í•œ ì¤„ë¡œ ë³µì‚¬:**
```bash
# ìœ„ ì½”ë“œ ì „ì²´ë¥¼ ë³µì‚¬í•˜ì—¬ íŒŒì¼ì— ë¶™ì—¬ë„£ê¸°
vi packnet_sfm/losses/scale_adaptive_loss.py
```

---

## âš¡ Step 2: í”„ë¡œì íŠ¸ í†µí•© (1ë¶„)

### 2.1 supervised_loss.py ìˆ˜ì •

`packnet_sfm/losses/supervised_loss.py` íŒŒì¼ ì—´ê¸°:

**1) Import ì¶”ê°€ (íŒŒì¼ ìƒë‹¨):**

```python
from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss
```

**2) get_loss_func() í•¨ìˆ˜ì— ì¶”ê°€ (ì•½ 79-110ë¼ì¸ ë¶€ê·¼):**

```python
def get_loss_func(supervised_method, **kwargs):
    """Determines the supervised loss to be used, given the supervised method."""
    print(f"ğŸ” Loading loss function for: {supervised_method}")
    
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    elif supervised_method.endswith('scale-adaptive'):  # â† ì´ ë¶€ë¶„ ì¶”ê°€
        return ScaleAdaptiveLoss(
            lambda_sg=kwargs.get('lambda_sg', 0.5),
            epsilon=kwargs.get('epsilon', 1e-8),
            num_scales=kwargs.get('num_scales', 4),
            use_absolute=kwargs.get('use_absolute', True),
        )
    
    else:
        raise ValueError('Unknown supervised loss {}'.format(supervised_method))
```

**ë¹ ë¥¸ ìˆ˜ì • ëª…ë ¹:**

```bash
# Import ì¶”ê°€
sed -i '13a from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss' \
    packnet_sfm/losses/supervised_loss.py

# ë˜ëŠ” ì§ì ‘ í¸ì§‘
vi packnet_sfm/losses/supervised_loss.py
# 13ë²ˆì§¸ ì¤„ ë‹¤ìŒì— import ì¶”ê°€
# get_loss_func() í•¨ìˆ˜ì— elif ë¸”ë¡ ì¶”ê°€
```

---

## âš¡ Step 3: í…ŒìŠ¤íŠ¸ (2ë¶„)

### 3.1 ë¹ ë¥¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

Python ì¸í„°í”„ë¦¬í„°ì—ì„œ í…ŒìŠ¤íŠ¸:

```python
import torch
from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss

# 1. ì´ˆê¸°í™”
loss_fn = ScaleAdaptiveLoss(lambda_sg=0.5)
print("âœ… Loss ì´ˆê¸°í™” ì„±ê³µ")

# 2. ë”ë¯¸ ë°ì´í„°
pred = torch.rand(2, 1, 192, 640) * 0.1 + 0.01
gt = torch.rand(2, 1, 192, 640) * 0.1 + 0.01

# 3. Forward pass
loss = loss_fn(pred, gt)
print(f"âœ… Loss ê³„ì‚° ì„±ê³µ: {loss.item():.4f}")

# 4. Metrics í™•ì¸
print(f"âœ… Metrics: {list(loss_fn.metrics.keys())}")
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ğŸ¯ Scale-Adaptive Loss initialized:
   Î»_sg: 0.5, num_scales: 4, use_absolute: True
âœ… Loss ì´ˆê¸°í™” ì„±ê³µ
âœ… Loss ê³„ì‚° ì„±ê³µ: 0.8234
âœ… Metrics: ['scale_adaptive/relative', 'gradient/scale_1', 'gradient/scale_2', 
             'gradient/scale_3', 'gradient/scale_4', 'total_loss', 'loss_sa', 'loss_sg']
```

### 3.2 í†µí•© í…ŒìŠ¤íŠ¸ (ì„ íƒ)

ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸:

```bash
# 5 ì—í­ë§Œ í…ŒìŠ¤íŠ¸
python scripts/train.py \
    configs/train_resnet_san_kitti.yaml \
    --supervised-method sparse-scale-adaptive \
    --lambda-sg 0.5 \
    --max-epochs 5 \
    --gpus 0
```

---

## ğŸ“ YAML Config ì˜ˆì‹œ

### ê¸°ë³¸ ì„¤ì • (KITTI)

`configs/train_scale_adaptive.yaml` ìƒì„±:

```yaml
name: 'test_scale_adaptive'

model:
    name: 'SemiSupModel'
    params:
        supervised_method: 'sparse-scale-adaptive'
        supervised_loss_weight: 1.0
        lambda_sg: 0.5
        num_scales: 4
        use_absolute: true
        
    depth_net:
        name: 'ResNetSAN01'
        version: '18pt'
        use_film: true
        film_scales: [0]

datasets:
    train:
        batch_size: 4
        path: ['/data/kitti_raw']
        split: ['eigen_zhou']

trainer:
    max_epochs: 20
    gpus: [0]
```

---

## ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¹ ë¥¸ ê°€ì´ë“œ

### lambda_sg (Gradient ê°€ì¤‘ì¹˜)

| ê°’ | ì¶”ì²œ ì‚¬ìš©ì²˜ | íš¨ê³¼ |
|----|------------|------|
| `0.3` | í¬ì†Œ LiDAR | ë¶€ë“œëŸ¬ì›€ ìš°ì„  |
| `0.5` | **ê¸°ë³¸ê°’** | ê· í˜•ì¡íŒ ì„¤ì • |
| `0.7` | ë°€ì§‘ ê¹Šì´ | ì—ì§€ ì„ ëª… |

### num_scales

| ê°’ | ì¶”ì²œ ì‚¬ìš©ì²˜ |
|----|------------|
| `2` | ë¹ ë¥¸ ì‹¤í—˜ |
| `4` | **ê¸°ë³¸ê°’** |
| `5` | ê³ í’ˆì§ˆ ê²°ê³¼ |

### use_inv_depth (â­ ì„±ëŠ¥ ì˜µì…˜)

| ê°’ | ì„¤ëª… | ì¶”ì²œ |
|----|------|------|
| `false` | depthë¡œ ë³€í™˜ í›„ ê³„ì‚° (ì •í™•) | ì—°êµ¬/ë…¼ë¬¸ |
| `true` | inv_depth ì§ì ‘ ê³„ì‚° (ë¹ ë¦„) | í”„ë¡œë•ì…˜/GPU ë¶€ì¡± |

**ì–¸ì œ `true`ë¡œ ì„¤ì •?**
- GPU ë©”ëª¨ë¦¬ ë¶€ì¡±í•  ë•Œ
- í•™ìŠµ ì†ë„ê°€ ì¤‘ìš”í•  ë•Œ
- SSI ë“± ë‹¤ë¥¸ lossì™€ ì¼ê´€ì„± í•„ìš”í•  ë•Œ

---

## ğŸ”¥ ìì£¼ ì‚¬ìš©í•˜ëŠ” ëª…ë ¹ì–´

### 1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•™ìŠµ

```bash
python scripts/train.py \
    configs/train_resnet_san_kitti.yaml \
    --supervised-method sparse-scale-adaptive \
    --lambda-sg 0.5 \
    --max-epochs 5 \
    --name "quick_test"
```

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° Sweep

```bash
for lambda in 0.3 0.5 0.7; do
    python scripts/train.py \
        configs/train_scale_adaptive.yaml \
        --lambda-sg $lambda \
        --name "lambda_${lambda}" \
        --max-epochs 10
done
```

### 3. TensorBoard ëª¨ë‹ˆí„°ë§

```bash
tensorboard --logdir outputs/ --port 6006
```

---

## âœ… ì„±ê³µ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì™„ë£Œí•œ í•­ëª©ì— ì²´í¬:

- [ ] `scale_adaptive_loss.py` íŒŒì¼ ìƒì„±
- [ ] `supervised_loss.py`ì— import ì¶”ê°€
- [ ] `get_loss_func()`ì— elif ë¸”ë¡ ì¶”ê°€
- [ ] Python ì¸í„°í”„ë¦¬í„° í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] Loss ê°’ì´ ìˆ«ìë¡œ ì¶œë ¥ë¨ (NaN ì•„ë‹˜)
- [ ] Metrics ë”•ì…”ë„ˆë¦¬ í™•ì¸ ì™„ë£Œ
- [ ] (ì„ íƒ) 5 ì—í­ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ

---

## ğŸ› ë¹ ë¥¸ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: Import ì—ëŸ¬

```python
ModuleNotFoundError: No module named 'packnet_sfm.losses.scale_adaptive_loss'
```

**í•´ê²°:** íŒŒì¼ ê²½ë¡œ í™•ì¸
```bash
ls -l packnet_sfm/losses/scale_adaptive_loss.py
# íŒŒì¼ì´ ì—†ìœ¼ë©´ Step 1ë¶€í„° ë‹¤ì‹œ
```

### ë¬¸ì œ 2: Lossê°€ NaN

```python
Loss: nan
```

**í•´ê²°:** ê¹Šì´ ê°’ ë²”ìœ„ í™•ì¸
```python
# scale_adaptive_loss.pyì˜ forward()ì— ì¶”ê°€
pred_depth = torch.clamp(inv2depth(pred_inv_depth), min=0.1, max=100.0)
gt_depth = torch.clamp(inv2depth(gt_inv_depth), min=0.1, max=100.0)
```

### ë¬¸ì œ 3: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**í•´ê²°:** íŒŒë¼ë¯¸í„° ì¤„ì´ê¸°
```yaml
num_scales: 2      # 4 â†’ 2
batch_size: 2      # 4 â†’ 2
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### ì´ˆê¸° Loss ê°’

```
Epoch 1:
  loss_sa: 0.85
  loss_sg: 1.23
  total_loss: 1.47
```

### ìˆ˜ë ´ í›„ (20 epochs)

```
Epoch 20:
  loss_sa: 0.12
  loss_sg: 0.31
  total_loss: 0.28
```

### TensorBoard ê·¸ë˜í”„

ì •ìƒì ì¸ ê²½ìš°:
- âœ… Total loss: ì ì§„ì  ê°ì†Œ
- âœ… Loss_sa: ë¹ ë¥´ê²Œ ê°ì†Œ
- âœ… Loss_sg: ì²œì²œíˆ ê°ì†Œ
- âœ… Gradient scales: ëª¨ë“  ìŠ¤ì¼€ì¼ ê· í˜•

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

êµ¬í˜„ì´ ì„±ê³µí–ˆë‹¤ë©´:

1. **ì „ì²´ í•™ìŠµ ì‹¤í–‰**
   ```bash
   python scripts/train.py configs/train_scale_adaptive.yaml --max-epochs 50
   ```

2. **ì„±ëŠ¥ ë¹„êµ**
   - ê¸°ì¡´ SSI lossì™€ ë¹„êµ
   - Evaluation metrics í™•ì¸
   - ì‹œê°ì  ê²°ê³¼ ë¹„êµ

3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**
   - lambda_sg íŠœë‹
   - num_scales ì‹¤í—˜
   - ë°ì´í„°ì…‹ë³„ ìµœì ê°’ ì°¾ê¸°

---

## ğŸ“š ë” ìì„¸í•œ ì •ë³´

- **ì „ì²´ êµ¬í˜„ ê°€ì´ë“œ:** [`SCALE_ADAPTIVE_LOSS_IMPLEMENTATION.md`](./SCALE_ADAPTIVE_LOSS_IMPLEMENTATION.md)
- **ì´ë¡ ì  ë°°ê²½:** [`SCALE_ADAPTIVE_LOSS.md`](./SCALE_ADAPTIVE_LOSS.md)
- **í”„ë¡œì íŠ¸ README:** `../README.md`

---

**ì†Œìš” ì‹œê°„:** ~5ë¶„  
**ë‚œì´ë„:** â­â­ (ì¤‘ê¸‰)  
**ë²„ì „:** 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸:** 2025ë…„ 10ì›” 17ì¼
