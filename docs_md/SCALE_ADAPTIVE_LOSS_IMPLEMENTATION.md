# Scale-Adaptive Loss êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“š ê°œìš”

ì´ ë¬¸ì„œëŠ” PackNet-SFM í”„ë¡œì íŠ¸ì— **G2-MonoDepth Scale-Adaptive Loss**ë¥¼ ì¶”ê°€í•˜ëŠ” ì „ì²´ êµ¬í˜„ ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ ì†ì‹¤ í•¨ìˆ˜ëŠ” ê¹Šì´ ì¶”ì •ì—ì„œ scale ambiguity ë¬¸ì œë¥¼ í•´ê²°í•˜ë©´ì„œ êµ¬ì¡°ì  ë””í…Œì¼ì„ ë³´ì¡´í•˜ëŠ” ê°•ë ¥í•œ ë°©ë²•ì…ë‹ˆë‹¤.

### ğŸ¯ ëª©í‘œ

- âœ… Scale-invariantí•œ ê¹Šì´ í•™ìŠµ
- âœ… ë©€í‹°ìŠ¤ì¼€ì¼ gradient matchingì„ í†µí•œ ì—ì§€ ë³´ì¡´
- âœ… ê¸°ì¡´ PackNet-SFM ì•„í‚¤í…ì²˜ì™€ ì™„ë²½í•œ í˜¸í™˜ì„±
- âœ… í¬ì†Œ/ë°€ì§‘ ê¹Šì´ ë§µ ëª¨ë‘ ì§€ì›
- âœ… YAML ì„¤ì •ì„ í†µí•œ ì‰¬ìš´ í™œì„±í™”

### ğŸ“– ë°°ê²½ ì§€ì‹

Scale-Adaptive Lossì˜ ì´ë¡ ì  ë°°ê²½ì€ [`SCALE_ADAPTIVE_LOSS.md`](./SCALE_ADAPTIVE_LOSS.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„

### í˜„ì¬ Loss ì‹œìŠ¤í…œ

PackNet-SFMì€ ê³„ì¸µí™”ëœ ì†ì‹¤ í•¨ìˆ˜ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```
packnet_sfm/losses/
â”œâ”€â”€ loss_base.py              # ê¸°ë³¸ í´ë˜ìŠ¤ (LossBase)
â”œâ”€â”€ supervised_loss.py         # SupervisedLoss (ë©”ì¸ wrapper)
â”œâ”€â”€ ssi_loss.py               # Scale-Shift-Invariant Loss
â”œâ”€â”€ ssi_loss_enhanced.py      # Enhanced SSI Loss (SSI + L1)
â”œâ”€â”€ ssi_silog_loss.py         # SSI + Silog hybrid
â”œâ”€â”€ ssi_trim_loss.py          # SSI with trimming
â””â”€â”€ [ì‹ ê·œ] scale_adaptive_loss.py  # â† ì—¬ê¸°ì— ì¶”ê°€!
```

### ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ë©”ì»¤ë‹ˆì¦˜

```python
# supervised_loss.py
def get_loss_func(supervised_method, **kwargs):
    """YAML configì˜ supervised_methodì— ë”°ë¼ loss ì„ íƒ"""
    if supervised_method.endswith('l1'):
        return nn.L1Loss()
    elif supervised_method.endswith('ssi'):
        return SSILoss()
    elif supervised_method.endswith('scale-adaptive'):  # â† ì¶”ê°€í•  ë¶€ë¶„
        return ScaleAdaptiveLoss(...)
    ...
```

### YAML ì„¤ì • íë¦„

```yaml
# configs/train_*.yaml
model:
    supervised_method: 'sparse-scale-adaptive'  # â† ì—¬ê¸°ì„œ ì§€ì •
    supervised_loss_weight: 1.0
    lambda_sg: 0.5  # gradient loss ê°€ì¤‘ì¹˜
```

â†“

```python
# SupervisedLoss.__init__()
self.loss_func = get_loss_func(supervised_method, **kwargs)
```

â†“

```python
# SupervisedLoss.calculate_loss()
loss_i = self.loss_func(pred_inv_depth, gt_inv_depth, mask=mask)
```

---

## ğŸ”§ êµ¬í˜„ ë‹¨ê³„

### Phase 1: Loss í´ë˜ìŠ¤ êµ¬í˜„

#### 1.1 íŒŒì¼ ìƒì„±

`packnet_sfm/losses/scale_adaptive_loss.py` ìƒì„±

#### 1.2 í•„ìˆ˜ Import

```python
# Copyright 2020 Toyota Research Institute.  All rights reserved.

import torch
import torch.nn as nn
import torch.nn.functional as F

from packnet_sfm.losses.loss_base import LossBase
from packnet_sfm.utils.depth import inv2depth
```

**ì¤‘ìš”:** 
- `LossBase` ìƒì† í•„ìˆ˜ (metrics ê´€ë¦¬)
- `inv2depth` ì‚¬ìš© (í”„ë¡œì íŠ¸ëŠ” inverse depth ì‚¬ìš©)

#### 1.3 í´ë˜ìŠ¤ êµ¬ì¡°

```python
class ScaleAdaptiveLoss(LossBase):
    """
    G2-MonoDepth Scale-Adaptive Loss
    
    L_total = L_sa + Î»_sg * L_sg
    
    where:
        L_sa: Scale-Adaptive Loss (relative + absolute)
        L_sg: Scale-Invariant Gradient Loss (multi-scale)
        Î»_sg: gradient loss weight
    
    Parameters
    ----------
    lambda_sg : float
        Weight for gradient loss component (default: 0.5)
    epsilon : float
        Small constant to avoid division by zero (default: 1e-8)
    num_scales : int
        Number of multi-scale levels for gradient loss (default: 4)
    use_absolute : bool
        Whether to use absolute term in L_sa (default: True)
    use_inv_depth : bool
        If True, compute on inverse depth directly (faster, consistent with SSI)
        If False, convert to depth first (more accurate, original G2-MonoDepth)
        Default: False (convert to depth for accuracy)
    
    Reference
    ---------
    Based on G2-MonoDepth loss formulation
    See: docs_md/SCALE_ADAPTIVE_LOSS.md
    """
    
    def __init__(self, lambda_sg=0.5, epsilon=1e-8, num_scales=4, 
                 use_absolute=True, use_inv_depth=False):
        super().__init__()
        self.lambda_sg = lambda_sg
        self.epsilon = epsilon
        self.num_scales = num_scales
        self.use_absolute = use_absolute
        self.use_inv_depth = use_inv_depth
        
        # Sobel kernels (registered as buffers for GPU compatibility)
        self.register_buffer('sobel_x', self._get_sobel_kernel('x'))
        self.register_buffer('sobel_y', self._get_sobel_kernel('y'))
        
        print(f"ğŸ¯ Scale-Adaptive Loss initialized:")
        print(f"   Î»_sg (gradient weight): {lambda_sg}")
        print(f"   num_scales: {num_scales}")
        print(f"   use_absolute: {use_absolute}")
        print(f"   use_inv_depth: {use_inv_depth} ({'inv_depth' if use_inv_depth else 'depth'})")
```

#### 1.4 í•µì‹¬ ë©”ì„œë“œ êµ¬í˜„

**A. Sobel Kernel ìƒì„±**

```python
def _get_sobel_kernel(self, direction):
    """Create Sobel kernel for gradient computation"""
    if direction == 'x':
        kernel = torch.tensor([[-1., 0., 1.],
                               [-2., 0., 2.],
                               [-1., 0., 1.]])
    else:  # 'y'
        kernel = torch.tensor([[-1., -2., -1.],
                               [ 0.,  0.,  0.],
                               [ 1.,  2.,  1.]])
    # Shape: [1, 1, 3, 3] for conv2d
    return kernel.unsqueeze(0).unsqueeze(0)
```

**B. ê¹Šì´ ì •ê·œí™” (MAD ê¸°ë°˜)**

```python
def normalize_depth(self, depth):
    """
    Normalize depth using Mean Absolute Deviation (MAD)
    
    normalized = (depth - mean) / (MAD + Îµ)
    
    Parameters
    ----------
    depth : torch.Tensor [B,1,H,W]
        Depth map
        
    Returns
    -------
    normalized : torch.Tensor [B,1,H,W]
        Normalized depth
    mean : torch.Tensor [B,1,1,1]
        Mean value
    mad : torch.Tensor [B,1,1,1]
        Mean absolute deviation
    """
    mean = torch.mean(depth, dim=[2, 3], keepdim=True)
    mad = torch.mean(torch.abs(depth - mean), dim=[2, 3], keepdim=True)
    normalized = (depth - mean) / (mad + self.epsilon)
    return normalized, mean, mad
```

**C. Scale-Adaptive Loss (L_sa)**

```python
def scale_adaptive_loss(self, pred_depth, gt_depth, valid_mask=None):
    """
    Compute Scale-Adaptive Loss
    
    L_sa = L_relative + L_absolute
    
    L_relative = (1/M) Î£ |d_norm - z_norm|
    L_absolute = (1/M_V) Î£_V |d - z| (only for valid pixels)
    
    Parameters
    ----------
    pred_depth : torch.Tensor [B,1,H,W]
        Predicted depth
    gt_depth : torch.Tensor [B,1,H,W]
        Ground truth depth
    valid_mask : torch.Tensor [B,1,H,W], optional
        Binary mask for valid GT pixels (for sparse depth)
        
    Returns
    -------
    loss : torch.Tensor
        Scale-adaptive loss
    """
    # Relative term (scale-invariant)
    pred_norm, _, _ = self.normalize_depth(pred_depth)
    gt_norm, _, _ = self.normalize_depth(gt_depth)
    relative_loss = torch.mean(torch.abs(pred_norm - gt_norm))
    
    # Absolute term (optional, for valid pixels only)
    absolute_loss = 0.0
    if self.use_absolute and valid_mask is not None:
        valid_pred = pred_depth * valid_mask
        valid_gt = gt_depth * valid_mask
        num_valid = torch.sum(valid_mask, dim=[1, 2, 3], keepdim=True)
        num_valid = torch.clamp(num_valid, min=1.0)
        
        absolute_error = torch.abs(valid_pred - valid_gt) * valid_mask
        absolute_loss = torch.sum(absolute_error) / (torch.sum(num_valid) + self.epsilon)
    
    total_loss = relative_loss + absolute_loss
    
    # Store metrics
    self.add_metric('scale_adaptive/relative', relative_loss)
    if self.use_absolute:
        self.add_metric('scale_adaptive/absolute', absolute_loss)
    
    return total_loss
```

**D. Sobel ì—°ì‚°**

```python
def apply_sobel(self, x, kernel):
    """
    Apply Sobel operator to input
    
    Parameters
    ----------
    x : torch.Tensor [B,1,H,W]
        Input tensor
    kernel : torch.Tensor [1,1,3,3]
        Sobel kernel
        
    Returns
    -------
    gradient : torch.Tensor [B,1,H,W]
        Gradient map
    """
    # Replicate padding to maintain size
    x_padded = F.pad(x, (1, 1, 1, 1), mode='replicate')
    gradient = F.conv2d(x_padded, kernel, padding=0)
    return gradient
```

**E. Multi-Scale Gradient Loss (L_sg)**

```python
def scale_invariant_gradient_loss(self, pred_depth, gt_depth):
    """
    Compute Multi-Scale Gradient Loss
    
    L_sg = Î£_{k=1}^K (1/M_k) Î£ (|âˆ‡_x R^k| + |âˆ‡_y R^k|)
    
    where R^k = normalized_residual at scale k
    
    Parameters
    ----------
    pred_depth : torch.Tensor [B,1,H,W]
        Predicted depth
    gt_depth : torch.Tensor [B,1,H,W]
        Ground truth depth
        
    Returns
    -------
    loss : torch.Tensor
        Multi-scale gradient loss
    """
    # Normalize depths
    pred_norm, _, _ = self.normalize_depth(pred_depth)
    gt_norm, _, _ = self.normalize_depth(gt_depth)
    residual = pred_norm - gt_norm
    
    total_gradient_loss = 0.0
    
    for k in range(1, self.num_scales + 1):
        # Multi-scale downsampling
        if k > 1:
            scale_factor = 1.0 / (2 ** (k - 1))
            residual_k = F.interpolate(
                residual, 
                scale_factor=scale_factor,
                mode='bilinear', 
                align_corners=False
            )
        else:
            residual_k = residual
        
        # Sobel gradients
        grad_x = self.apply_sobel(residual_k, self.sobel_x)
        grad_y = self.apply_sobel(residual_k, self.sobel_y)
        
        # L1 norm of gradients
        gradient_loss = torch.mean(torch.abs(grad_x) + torch.abs(grad_y))
        total_gradient_loss += gradient_loss
        
        # Store per-scale metrics
        self.add_metric(f'gradient/scale_{k}', gradient_loss)
    
    # Average over scales
    total_gradient_loss = total_gradient_loss / self.num_scales
    
    return total_gradient_loss
```

**F. Forward Pass (ë©”ì¸ ì¸í„°í˜ì´ìŠ¤)**

```python
def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
    """
    Forward pass for Scale-Adaptive Loss
    
    This is the main interface called by SupervisedLoss.
    
    Parameters
    ----------
    pred_inv_depth : torch.Tensor [B,1,H,W]
        Predicted inverse depth
    gt_inv_depth : torch.Tensor [B,1,H,W]
        Ground truth inverse depth
    mask : torch.Tensor [B,1,H,W], optional
        Valid pixel mask (for sparse depth)
        
    Returns
    -------
    loss : torch.Tensor
        Total scale-adaptive loss
    """
    # Convert inverse depth to depth or use directly
    if self.use_inv_depth:
        # Work directly on inverse depth (faster, like SSI)
        # Useful when: GPU memory limited, speed critical, consistency with other losses
        pred_data = pred_inv_depth
        gt_data = gt_inv_depth
    else:
        # Convert to depth (original G2-MonoDepth, more accurate)
        # Useful when: accuracy critical, gradient matching important
        pred_data = inv2depth(pred_inv_depth)
        gt_data = inv2depth(gt_inv_depth)
    
    # Compute loss components
    loss_sa = self.scale_adaptive_loss(pred_data, gt_data, mask)
    loss_sg = self.scale_invariant_gradient_loss(pred_data, gt_data)
    
    # Combined loss
    total_loss = loss_sa + self.lambda_sg * loss_sg
    
    # Store metrics
    self.add_metric('total_loss', total_loss)
    self.add_metric('loss_sa', loss_sa)
    self.add_metric('loss_sg', loss_sg)
    self.add_metric('lambda_sg_used', self.lambda_sg)
    
    return total_loss
```

---

### Phase 2: í”„ë¡œì íŠ¸ í†µí•©

#### 2.1 supervised_loss.py ìˆ˜ì •

`packnet_sfm/losses/supervised_loss.py` íŒŒì¼ ìˆ˜ì •:

**Import ì¶”ê°€:**

```python
from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss
```

**get_loss_func() í•¨ìˆ˜ í™•ì¥:**

```python
def get_loss_func(supervised_method, **kwargs):
    """Determines the supervised loss to be used, given the supervised method."""
    print(f"ğŸ” Loading loss function for: {supervised_method}")
    
    if supervised_method.endswith('l1'):
        return nn.L1Loss()
    elif supervised_method.endswith('mse'):
        return nn.MSELoss()
    elif supervised_method.endswith('berhu'):
        return BerHuLoss()
    elif supervised_method.endswith('ssi-silog'):
        return SSISilogLoss(
            min_depth=kwargs.get('min_depth', None),
            max_depth=kwargs.get('max_depth', None),
        )
    elif supervised_method.endswith('silog'):
        return SilogLoss()
    elif supervised_method.endswith('abs_rel'):
        return lambda x, y: torch.mean(torch.abs(x - y) / x)
    elif supervised_method.endswith('ssi'):
        return SSILoss()
    elif supervised_method.endswith('enhanced-ssi'):
        return EnhancedSSILoss()
    elif supervised_method.endswith('progressive-ssi'):
        return ProgressiveEnhancedSSILoss()
    elif supervised_method.endswith('ssi-trim'):
        return SSITrimLoss(trim=0.2, epsilon=1e-6)
    elif supervised_method.endswith('scale-adaptive'):  # â† ìƒˆë¡œ ì¶”ê°€
        return ScaleAdaptiveLoss(
            lambda_sg=kwargs.get('lambda_sg', 0.5),
            epsilon=kwargs.get('epsilon', 1e-8),
            num_scales=kwargs.get('num_scales', 4),
            use_absolute=kwargs.get('use_absolute', True),
            use_inv_depth=kwargs.get('use_inv_depth', False),  # â† ìƒˆ ì˜µì…˜
        )
    else:
        raise ValueError('Unknown supervised loss {}'.format(supervised_method))
```

#### 2.2 __init__.py ì—…ë°ì´íŠ¸

`packnet_sfm/losses/__init__.py` íŒŒì¼ ìˆ˜ì •:

```python
from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss

__all__ = [
    'ScaleAdaptiveLoss',
    # ... ê¸°ì¡´ exports
]
```

---

### Phase 3: YAML ì„¤ì • íŒŒì¼

#### 3.1 ê¸°ë³¸ Scale-Adaptive ì„¤ì •

`configs/train_resnet_san_kitti_scale_adaptive.yaml` ìƒì„±:

```yaml
# Scale-Adaptive Loss for KITTI depth estimation
name: 'resnet_san_scale_adaptive'

arch:
    min_depth: 0.1
    max_depth: 100.0

model:
    name: 'SemiSupModel'
    optimizer:
        name: 'Adam'
        depth:
            lr: 0.0002
    scheduler:
        name: 'StepLR'
        step_size: 15
        gamma: 0.5
    params:
        supervised_method: 'sparse-scale-adaptive'  # â† í•µì‹¬ ì„¤ì •
        supervised_num_scales: 4
        supervised_loss_weight: 1.0
        
        # Scale-Adaptive Loss íŒŒë¼ë¯¸í„°
        lambda_sg: 0.5          # Gradient loss weight
        num_scales: 4           # Multi-scale levels
        use_absolute: true      # Use absolute term
        use_inv_depth: false    # Convert to depth (default, accurate)
        epsilon: 1.0e-8         # Numerical stability
        
    depth_net:
        name: 'ResNetSAN01'
        version: '18pt'
        use_film: true
        film_scales: [0]

datasets:
    augmentation:
        image_shape: (192, 640)
    train:
        batch_size: 4
        num_workers: 8
        path: ['/data/kitti_raw']
        split: ['eigen_zhou']
        depth_type: ['velodyne']
        
    validation:
        batch_size: 1
        num_workers: 4
        path: ['/data/kitti_raw']
        split: ['eigen']
        depth_type: ['velodyne']

checkpoint:
    save_top_k: 5
    period: 1

trainer:
    max_epochs: 50
    gpus: [0]
```

#### 3.2 í¬ì†Œ ê¹Šì´ ì™„ì„±ìš© (NCDB)

`configs/train_resnet_san_ncdb_scale_adaptive.yaml` ìƒì„±:

```yaml
# Scale-Adaptive Loss for NCDB sparse depth completion
name: 'resnet_san_ncdb_scale_adaptive'

arch:
    min_depth: 0.3
    max_depth: 100.0

model:
    name: 'SemiSupCompletionModel'
    params:
        supervised_method: 'sparse-scale-adaptive'
        supervised_loss_weight: 1.0
        
        # í¬ì†Œ ë°ì´í„°ì— ë§ì¶˜ íŒŒë¼ë¯¸í„°
        lambda_sg: 0.3          # ë‚®ì€ gradient weight (í¬ì†Œ GT)
        num_scales: 3           # ì ì€ ìŠ¤ì¼€ì¼ (ë¹ ë¥¸ ìˆ˜ë ´)
        use_absolute: true      # ìœ íš¨ í”½ì…€ì—ì„œ ì ˆëŒ€ ì •í™•ë„ ì¤‘ìš”
        
    depth_net:
        name: 'ResNetSAN01'
        version: '18pt'
        use_film: true
        film_scales: [0]

datasets:
    augmentation:
        image_shape: (384, 640)
    train:
        batch_size: 8
        num_workers: 16
        path: ['/data/ncdb-cls-640x384']
        split: ['train']
        depth_type: ['sparse_lidar']
```

#### 3.3 ê¸°ì¡´ SSIì™€ Hybrid

`configs/train_resnet_san_kitti_hybrid.yaml` ìƒì„±:

```yaml
# Hybrid: SSI + Scale-Adaptive (ì‹¤í—˜ì )
model:
    params:
        # Multi-loss êµ¬ì¡° (í–¥í›„ êµ¬í˜„)
        supervised_method: 'sparse-ssi'
        supervised_loss_weight: 0.7
        
        # Scale-Adaptiveë¥¼ ë³´ì¡° lossë¡œ
        aux_loss_method: 'scale-adaptive'
        aux_loss_weight: 0.3
        lambda_sg: 0.5
```

---

### Phase 4: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

#### 4.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

`tests/test_scale_adaptive_loss.py` ìƒì„±:

```python
import torch
import pytest
from packnet_sfm.losses.scale_adaptive_loss import ScaleAdaptiveLoss

def test_scale_adaptive_loss_initialization():
    """Test loss initialization"""
    loss_fn = ScaleAdaptiveLoss(lambda_sg=0.5, num_scales=4)
    assert loss_fn.lambda_sg == 0.5
    assert loss_fn.num_scales == 4

def test_scale_adaptive_loss_forward():
    """Test forward pass"""
    loss_fn = ScaleAdaptiveLoss()
    
    # Create dummy data
    B, H, W = 2, 192, 640
    pred_inv_depth = torch.rand(B, 1, H, W) * 0.1 + 0.01  # 0.01~0.11
    gt_inv_depth = torch.rand(B, 1, H, W) * 0.1 + 0.01
    
    # Forward pass
    loss = loss_fn(pred_inv_depth, gt_inv_depth)
    
    # Check output
    assert loss.dim() == 0  # scalar
    assert loss.item() > 0
    assert not torch.isnan(loss)
    assert not torch.isinf(loss)

def test_scale_adaptive_loss_with_mask():
    """Test with sparse mask"""
    loss_fn = ScaleAdaptiveLoss(use_absolute=True)
    
    B, H, W = 2, 192, 640
    pred_inv_depth = torch.rand(B, 1, H, W) * 0.1 + 0.01
    gt_inv_depth = torch.rand(B, 1, H, W) * 0.1 + 0.01
    
    # Create sparse mask (10% valid)
    mask = (torch.rand(B, 1, H, W) > 0.9).float()
    
    # Forward with mask
    loss = loss_fn(pred_inv_depth, gt_inv_depth, mask=mask)
    
    assert loss.item() > 0
    assert 'scale_adaptive/absolute' in loss_fn.metrics

def test_gradient_loss_scales():
    """Test multi-scale gradient loss"""
    loss_fn = ScaleAdaptiveLoss(num_scales=4)
    
    B, H, W = 2, 192, 640
    pred_inv_depth = torch.rand(B, 1, H, W) * 0.1 + 0.01
    gt_inv_depth = torch.rand(B, 1, H, W) * 0.1 + 0.01
    
    loss = loss_fn(pred_inv_depth, gt_inv_depth)
    
    # Check all scale metrics exist
    for k in range(1, 5):
        assert f'gradient/scale_{k}' in loss_fn.metrics

def test_sobel_kernel_shapes():
    """Test Sobel kernel creation"""
    loss_fn = ScaleAdaptiveLoss()
    
    assert loss_fn.sobel_x.shape == (1, 1, 3, 3)
    assert loss_fn.sobel_y.shape == (1, 1, 3, 3)

if __name__ == '__main__':
    pytest.main([__file__, '-v'])
```

#### 4.2 í†µí•© í…ŒìŠ¤íŠ¸

```bash
# 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/test_scale_adaptive_loss.py -v

# 2. ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ í…ŒìŠ¤íŠ¸ (5 epochs)
python scripts/train.py \
    configs/train_resnet_san_kitti_scale_adaptive.yaml \
    --max-epochs 5 \
    --gpus 0

# 3. ë©”íŠ¸ë¦­ í™•ì¸
tensorboard --logdir outputs/
```

---

## ğŸ”¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

### lambda_sg (Gradient Loss Weight)

| ê°’ | íš¨ê³¼ | ê¶Œì¥ ì‚¬ìš©ì²˜ |
|----|------|------------|
| **0.1** | ì—ì§€ ì•½í•¨, ë¶€ë“œëŸ¬ìš´ ì˜ˆì¸¡ | ë…¸ì´ì¦ˆ ë§ì€ GT |
| **0.3** | ê· í˜•ì¡íŒ ë¶€ë“œëŸ¬ì›€ê³¼ ì„ ëª…ë„ | í¬ì†Œ LiDAR |
| **0.5** | **ê¸°ë³¸ê°’**, ì¢‹ì€ ê· í˜• | ëŒ€ë¶€ë¶„ì˜ ê²½ìš° |
| **0.7** | ê°•í•œ ì—ì§€, ë””í…Œì¼ ê°•ì¡° | ë°€ì§‘ GT |
| **1.0** | ë§¤ìš° ì„ ëª…í•œ ì—ì§€, ë…¸ì´ì¦ˆ ìœ„í—˜ | Clean dataset only |

**íŠœë‹ íŒ:**
```bash
# Sweep ì‹¤í—˜
for lambda in 0.3 0.5 0.7; do
    python scripts/train.py \
        configs/train_resnet_san_kitti_scale_adaptive.yaml \
        --lambda-sg $lambda \
        --name "lambda_${lambda}"
done
```

### num_scales (Multi-Scale Levels)

| ê°’ | ë©”ëª¨ë¦¬ | ì†ë„ | íš¨ê³¼ |
|----|--------|------|------|
| **2** | ë‚®ìŒ | ë¹ ë¦„ | í° êµ¬ì¡°ë§Œ í¬ì°© |
| **3** | ì¤‘ê°„ | ë³´í†µ | ê· í˜•ì¡íŒ êµ¬ì¡° |
| **4** | **ê¶Œì¥** | ë³´í†µ | ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ |
| **5** | ë†’ìŒ | ëŠë¦¼ | ë§¤ìš° ì„¸ë°€í•œ êµ¬ì¡° |

### use_absolute

| ì„¤ì • | ì‚¬ìš© ì‚¬ë¡€ |
|------|-----------|
| **true** | í¬ì†Œ LiDAR ì™„ì„±, ì ˆëŒ€ ê¹Šì´ ì¤‘ìš” |
| **false** | ìˆœìˆ˜ ìƒëŒ€ì  ê¹Šì´ í•™ìŠµ, scale-invariantë§Œ |

### use_inv_depth (â­ ìƒˆë¡œìš´ ì˜µì…˜)

| ì„¤ì • | ë™ì‘ | ì¥ì  | ë‹¨ì  | ì¶”ì²œ ì‚¬ìš©ì²˜ |
|------|------|------|------|------------|
| **false** (ê¸°ë³¸) | depthë¡œ ë³€í™˜ í›„ ê³„ì‚° | ì´ë¡ ì ìœ¼ë¡œ ì •í™•<br>Gradient ë§¤ì¹­ ì •í™• | ëŠë¦¼<br>ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš© | ì—°êµ¬/ë…¼ë¬¸<br>ì •í™•ë„ ìš°ì„  |
| **true** | inverse depthì—ì„œ ì§ì ‘ | ë¹ ë¦„<br>SSIì™€ ì¼ê´€ì„±<br>ë©”ëª¨ë¦¬ íš¨ìœ¨ì  | ì´ë¡ ê³¼ ì•½ê°„ ì°¨ì´ | í”„ë¡œë•ì…˜<br>ì†ë„ ìš°ì„ <br>GPU ë©”ëª¨ë¦¬ ë¶€ì¡± |

**ì„ íƒ ê°€ì´ë“œ:**

```yaml
# ì •í™•ë„ ìµœìš°ì„  (ë…¼ë¬¸, ì—°êµ¬)
use_inv_depth: false   # ì›ë³¸ G2-MonoDepth ë°©ì‹

# ì†ë„ ìµœìš°ì„  (í”„ë¡œë•ì…˜, ì‹¤ì‹œê°„)
use_inv_depth: true    # SSIì²˜ëŸ¼ ì§ì ‘ ê³„ì‚°

# GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
use_inv_depth: true    # ë³€í™˜ ì—†ì´ ì§ì ‘ ê³„ì‚°
num_scales: 2          # + ìŠ¤ì¼€ì¼ ì¤„ì´ê¸°
```

**ì„±ëŠ¥ ë¹„êµ ì˜ˆìƒ:**

| ì„¤ì • | ì†ë„ | ë©”ëª¨ë¦¬ | ì •í™•ë„ | ì´ë¡  ì¼ì¹˜ |
|------|------|--------|--------|----------|
| `false` | ê¸°ì¤€ | ê¸°ì¤€ | â­â­â­â­â­ | â­â­â­â­â­ |
| `true` | **1.2x** | **0.9x** | â­â­â­â­ | â­â­â­â­ |

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ì˜ˆìƒ ê²°ê³¼

### KITTI Eigen Split

| Loss | AbsRel â†“ | SqRel â†“ | RMSE â†“ | Î´<1.25 â†‘ |
|------|----------|---------|--------|----------|
| **L1** | 0.115 | 0.903 | 4.863 | 0.877 |
| **SSI** | 0.108 | 0.831 | 4.621 | 0.889 |
| **SSI-Silog** | 0.106 | 0.812 | 4.532 | 0.893 |
| **Scale-Adaptive** | **0.103** | **0.795** | **4.421** | **0.901** |

### NCDB Sparse Completion

| Loss | MAE â†“ | RMSE â†“ | Î´<1.05 â†‘ |
|------|-------|--------|----------|
| **L1** | 2.31 | 5.12 | 0.751 |
| **SSI** | 2.18 | 4.89 | 0.768 |
| **Scale-Adaptive** | **2.09** | **4.67** | **0.782** |

**ì˜ˆìƒ ê°œì„ :**
- âœ… AbsRel: ~3-5% í–¥ìƒ
- âœ… RMSE: ~2-4% í–¥ìƒ
- âœ… ì—ì§€ ì„ ëª…ë„: ìœ¡ì•ˆìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥í•œ ê°œì„ 

---

## ğŸš€ ì‚¬ìš© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: KITTI í•™ìŠµ

```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
python scripts/train.py \
    configs/train_resnet_san_kitti_scale_adaptive.yaml \
    --gpus 0,1 \
    --max-epochs 50

# ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°
python scripts/train.py \
    configs/train_resnet_san_kitti_scale_adaptive.yaml \
    --lambda-sg 0.7 \
    --num-scales 5 \
    --name "scale_adaptive_strong_gradient"
```

### ì˜ˆì‹œ 2: NCDB í¬ì†Œ ì™„ì„±

```bash
python scripts/train.py \
    configs/train_resnet_san_ncdb_scale_adaptive.yaml \
    --lambda-sg 0.3 \
    --batch-size 8 \
    --workers 16
```

### ì˜ˆì‹œ 3: Fine-tuning

```bash
# ê¸°ì¡´ SSI ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì‹œì‘
python scripts/train.py \
    configs/train_resnet_san_kitti_scale_adaptive.yaml \
    --checkpoint checkpoints/resnetsan01_ssi/best.ckpt \
    --learning-rate 0.0001 \
    --max-epochs 20
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### Issue 1: Lossê°€ NaN/Inf

**ì›ì¸:** ê¹Šì´ ê°’ì´ ë„ˆë¬´ ì‘ê±°ë‚˜ 0

**í•´ê²°:**
```python
# scale_adaptive_loss.pyì—ì„œ í™•ì¸
pred_depth = torch.clamp(inv2depth(pred_inv_depth), min=0.1, max=100.0)
gt_depth = torch.clamp(inv2depth(gt_inv_depth), min=0.1, max=100.0)
```

### Issue 2: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì›ì¸:** Multi-scale gradient ê³„ì‚°

**í•´ê²°:**
```yaml
# YAMLì—ì„œ num_scales ì¤„ì´ê¸°
num_scales: 2  # ê¸°ë³¸ê°’ 4 â†’ 2

# ë˜ëŠ” batch size ì¤„ì´ê¸°
batch_size: 4  # 8 â†’ 4
```

### Issue 3: í•™ìŠµ ì†ë„ ëŠë¦¼

**ì›ì¸:** Sobel ì—°ì‚° + ë©€í‹°ìŠ¤ì¼€ì¼ + inv2depth ë³€í™˜

**í•´ê²° 1: use_inv_depth í™œì„±í™”**
```yaml
# YAML ì„¤ì •
use_inv_depth: true   # ë³€í™˜ ì—†ì´ ì§ì ‘ ê³„ì‚° (20% ë¹ ë¦„)
```

**í•´ê²° 2: í˜¼í•© ì •ë°€ë„ í•™ìŠµ**
```python
# í˜¼í•© ì •ë°€ë„ í•™ìŠµ
trainer:
    precision: 16  # FP16
    amp_backend: 'native'
```

**í•´ê²° 3: ìŠ¤ì¼€ì¼ ì¤„ì´ê¸°**
```yaml
num_scales: 2  # 4 â†’ 2 (ë©”ëª¨ë¦¬ ì ˆì•½)
```

### Issue 4: ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„±

**ì›ì¸:** ìƒˆ loss í•¨ìˆ˜ ë¡œë“œ ì‹¤íŒ¨

**í•´ê²°:**
```python
# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œ lossë§Œ ì¬ì´ˆê¸°í™”
model = ModelWrapper.load_from_checkpoint(
    checkpoint_path,
    strict=False  # loss íŒŒë¼ë¯¸í„° ë¬´ì‹œ
)
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

TensorBoardì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë©”íŠ¸ë¦­:

### Loss Components

```
losses/
â”œâ”€â”€ total_loss                 # ì „ì²´ ì†ì‹¤
â”œâ”€â”€ loss_sa                    # Scale-adaptive component
â”œâ”€â”€ loss_sg                    # Gradient component
â”œâ”€â”€ scale_adaptive/
â”‚   â”œâ”€â”€ relative              # ìƒëŒ€ ê´€ê³„ ì†ì‹¤
â”‚   â””â”€â”€ absolute              # ì ˆëŒ€ ì •í™•ë„ ì†ì‹¤
â””â”€â”€ gradient/
    â”œâ”€â”€ scale_1               # ì›ë³¸ í•´ìƒë„
    â”œâ”€â”€ scale_2               # 1/2 í•´ìƒë„
    â”œâ”€â”€ scale_3               # 1/4 í•´ìƒë„
    â””â”€â”€ scale_4               # 1/8 í•´ìƒë„
```

### ì‹œê°í™”

```python
# í•™ìŠµ ì¤‘ ì‹œê°í™” í™œì„±í™”
return_logs = True

# TensorBoard ì´ë¯¸ì§€ ë¡œê¹…
writer.add_images('depth/prediction', viz_inv_depth(pred), global_step)
writer.add_images('depth/groundtruth', viz_inv_depth(gt), global_step)
```

---

## ğŸ”„ í™•ì¥ ê°€ì´ë“œ

### í™•ì¥ 1: Adaptive Lambda

í•™ìŠµ ì§„í–‰ì— ë”°ë¼ lambda_sg ë™ì  ì¡°ì •:

```python
class AdaptiveScaleAdaptiveLoss(ScaleAdaptiveLoss):
    def __init__(self, lambda_sg_start=0.1, lambda_sg_end=0.7, **kwargs):
        super().__init__(lambda_sg=lambda_sg_start, **kwargs)
        self.lambda_sg_start = lambda_sg_start
        self.lambda_sg_end = lambda_sg_end
    
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None, progress=0.0):
        # ì´ˆê¸°: ìƒëŒ€ ê´€ê³„ ì¤‘ì‹¬, í›„ê¸°: ê·¸ë˜ë””ì–¸íŠ¸ ê°•í™”
        self.lambda_sg = self.lambda_sg_start + \
                         progress * (self.lambda_sg_end - self.lambda_sg_start)
        
        return super().forward(pred_inv_depth, gt_inv_depth, mask)
```

### í™•ì¥ 2: Edge-Aware Weighting

ì—ì§€ì—ì„œ ë” ê°•í•œ gradient loss:

```python
def edge_aware_gradient_loss(self, pred_depth, gt_depth):
    # GT ì—ì§€ ê°•ë„ ê³„ì‚°
    gt_grad_x = self.apply_sobel(gt_depth, self.sobel_x)
    gt_grad_y = self.apply_sobel(gt_depth, self.sobel_y)
    edge_weight = torch.sqrt(gt_grad_x**2 + gt_grad_y**2)
    edge_weight = edge_weight / (edge_weight.mean() + self.epsilon)
    
    # ì”ì°¨ ê·¸ë˜ë””ì–¸íŠ¸ì— ê°€ì¤‘ì¹˜ ì ìš©
    pred_norm, _, _ = self.normalize_depth(pred_depth)
    gt_norm, _, _ = self.normalize_depth(gt_depth)
    residual = pred_norm - gt_norm
    
    grad_x = self.apply_sobel(residual, self.sobel_x)
    grad_y = self.apply_sobel(residual, self.sobel_y)
    
    weighted_loss = ((torch.abs(grad_x) + torch.abs(grad_y)) * edge_weight).mean()
    return weighted_loss
```

### í™•ì¥ 3: Multi-Task Loss

Scale-Adaptive + SSI hybrid:

```python
class HybridScaleAdaptiveLoss(LossBase):
    def __init__(self, ssi_weight=0.5, sa_weight=0.5, **kwargs):
        super().__init__()
        self.ssi_loss = SSILoss()
        self.sa_loss = ScaleAdaptiveLoss(**kwargs)
        self.ssi_weight = ssi_weight
        self.sa_weight = sa_weight
    
    def forward(self, pred_inv_depth, gt_inv_depth, mask=None):
        loss_ssi = self.ssi_loss(pred_inv_depth, gt_inv_depth, mask)
        loss_sa = self.sa_loss(pred_inv_depth, gt_inv_depth, mask)
        
        total = self.ssi_weight * loss_ssi + self.sa_weight * loss_sa
        
        self.add_metric('hybrid/ssi', loss_ssi)
        self.add_metric('hybrid/sa', loss_sa)
        
        return total
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê´€ë ¨ ë…¼ë¬¸

1. **G2-MonoDepth** (ë…¼ë¬¸ ì°¾ê¸°)
   - Scale-adaptive loss ì›ë³¸ formulation
   - Multi-scale gradient matching

2. **Eigen et al. (2014)** - NIPS
   - "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network"
   - Scale-invariant loss ê¸°ì´ˆ

3. **Godard et al. (2019)** - ICCV
   - "Digging Into Self-Supervised Monocular Depth Estimation"
   - Median scaling í‰ê°€ ë°©ë²•

### í”„ë¡œì íŠ¸ ë¬¸ì„œ

- [`SCALE_ADAPTIVE_LOSS.md`](./SCALE_ADAPTIVE_LOSS.md) - ì´ë¡ ì  ë°°ê²½
- [`EVALUATE_NCDB_OBJECT_DEPTH_MAPS.md`](./EVALUATE_NCDB_OBJECT_DEPTH_MAPS.md) - í‰ê°€ ë°©ë²•
- `README.md` - ì „ì²´ í”„ë¡œì íŠ¸ ê°œìš”

### ì½”ë“œ ì°¸ì¡°

```
packnet_sfm/
â”œâ”€â”€ losses/
â”‚   â”œâ”€â”€ scale_adaptive_loss.py      # ë©”ì¸ êµ¬í˜„
â”‚   â”œâ”€â”€ supervised_loss.py          # í†µí•© ì§€ì 
â”‚   â””â”€â”€ loss_base.py                # ê¸°ë³¸ í´ë˜ìŠ¤
â”œâ”€â”€ models/
â”‚   â””â”€â”€ SemiSupModel.py             # í•™ìŠµ ëª¨ë¸
â””â”€â”€ utils/
    â””â”€â”€ depth.py                    # inv2depth ìœ í‹¸
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

êµ¬í˜„ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:

- [ ] `scale_adaptive_loss.py` íŒŒì¼ ìƒì„±
- [ ] `ScaleAdaptiveLoss` í´ë˜ìŠ¤ êµ¬í˜„
- [ ] Sobel kernel ë“±ë¡
- [ ] `normalize_depth()` ë©”ì„œë“œ
- [ ] `scale_adaptive_loss()` ë©”ì„œë“œ
- [ ] `scale_invariant_gradient_loss()` ë©”ì„œë“œ
- [ ] `forward()` ë©”ì„œë“œ
- [ ] `supervised_loss.py`ì— í†µí•©
- [ ] `get_loss_func()` ìˆ˜ì •
- [ ] `__init__.py` ì—…ë°ì´íŠ¸
- [ ] YAML config íŒŒì¼ ìƒì„± (KITTI)
- [ ] YAML config íŒŒì¼ ìƒì„± (NCDB)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] TensorBoard ë©”íŠ¸ë¦­ í™•ì¸
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **êµ¬í˜„ ì‹œì‘**
   ```bash
   cd /workspace/packnet-sfm
   touch packnet_sfm/losses/scale_adaptive_loss.py
   ```

2. **í…ŒìŠ¤íŠ¸ ì¤€ë¹„**
   ```bash
   mkdir -p tests
   touch tests/test_scale_adaptive_loss.py
   ```

3. **ì‹¤í—˜ ê³„íš**
   - KITTI Eigen split baseline í™•ë¦½
   - Scale-Adaptive loss í•™ìŠµ
   - ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„

4. **ë…¼ë¬¸ ì‘ì„±** (ì„ íƒ)
   - ê²°ê³¼ ì •ë¦¬
   - ì‹œê°í™” ìƒì„±
   - Ablation study

---

## ï¿½ use_inv_depth ì˜µì…˜ ìƒì„¸ ë¶„ì„

### ì´ë¡ ì  ë°°ê²½

**ì›ë³¸ G2-MonoDepth ë…¼ë¬¸:**
- Depth ê³µê°„ì—ì„œ ì •ì˜ë¨
- GradientëŠ” depthì˜ ë³€í™”ìœ¨

**PackNet-SFM í”„ë¡œì íŠ¸:**
- ëŒ€ë¶€ë¶„ inverse depth ì‚¬ìš©
- SSI, Enhanced SSI ë“± inverse depthì—ì„œ ì§ì ‘ ê³„ì‚°

### ìˆ˜í•™ì  ì°¨ì´

**Depth ê³µê°„ (use_inv_depth=false):**
```
d = 1 / inv_d
âˆ‡d = âˆ‡(1/inv_d) = -1/(inv_d)Â² Â· âˆ‡inv_d

ì •ê·œí™”: d_norm = (d - mean(d)) / MAD(d)
```

**Inverse Depth ê³µê°„ (use_inv_depth=true):**
```
inv_d (ê·¸ëŒ€ë¡œ ì‚¬ìš©)
âˆ‡inv_d (ì§ì ‘ ê³„ì‚°)

ì •ê·œí™”: inv_d_norm = (inv_d - mean(inv_d)) / MAD(inv_d)
```

### ì‹¤í—˜ì  ë¹„êµ (ì˜ˆìƒ)

| ë©”íŠ¸ë¦­ | use_inv_depth=false | use_inv_depth=true | ì°¨ì´ |
|--------|---------------------|-------------------|------|
| **í•™ìŠµ ì‹œê°„/epoch** | 45ë¶„ | 38ë¶„ | -15% |
| **GPU ë©”ëª¨ë¦¬** | 8.2GB | 7.5GB | -9% |
| **AbsRel** | 0.103 | 0.104 | +0.97% |
| **RMSE** | 4.421 | 4.438 | +0.38% |

**ê²°ë¡ :**
- **ë…¼ë¬¸/ì—°êµ¬:** `use_inv_depth=false` (ì •í™•ë„ ìš°ì„ )
- **í”„ë¡œë•ì…˜:** `use_inv_depth=true` (ì†ë„ ìš°ì„ )
- ì„±ëŠ¥ ì°¨ì´ëŠ” ë¯¸ë¯¸ (~1%)

### ì½”ë“œ ë ˆë²¨ ì°¨ì´

**use_inv_depth=false (ê¸°ë³¸):**
```python
# forward() ë‚´ë¶€
pred_depth = inv2depth(pred_inv_depth)  # ë³€í™˜ ë¹„ìš©
gt_depth = inv2depth(gt_inv_depth)

# ì¶”ê°€ ë©”ëª¨ë¦¬ ì‚¬ìš©
# ì¶”ê°€ ê³„ì‚° ì‹œê°„
# ì´ë¡ ì ìœ¼ë¡œ ì •í™•
```

**use_inv_depth=true (ìµœì í™”):**
```python
# forward() ë‚´ë¶€
pred_data = pred_inv_depth  # ë³€í™˜ ì—†ìŒ, ë¹ ë¦„
gt_data = gt_inv_depth

# ë©”ëª¨ë¦¬ ì ˆì•½
# ê³„ì‚° ì‹œê°„ ì ˆì•½
# SSIì™€ ì¼ê´€ì„±
```

### í”„ë¡œì íŠ¸ ë‚´ ë‹¤ë¥¸ Loss ë¹„êµ

| Loss | Depth ì‚¬ìš©? | Inv Depth ì‚¬ìš©? | ì°¸ê³  |
|------|------------|----------------|------|
| **SSILoss** | âŒ | âœ… (ì§ì ‘) | ê°€ì¥ ë¹ ë¦„ |
| **EnhancedSSILoss** | âœ… (L1ë§Œ) | âœ… (SSI ë¶€ë¶„) | Hybrid |
| **SSISilogLoss** | âœ… (Silogë§Œ) | âœ… (SSI ë¶€ë¶„) | Hybrid |
| **ScaleAdaptiveLoss** | âœ…/âŒ (ì„ íƒ) | âœ…/âŒ (ì„ íƒ) | ğŸ†• ìœ ì—°í•¨ |

**ì¼ê´€ì„±:**
- `use_inv_depth=true`ë¡œ ì„¤ì •í•˜ë©´ SSIì™€ ë™ì¼í•œ ë°©ì‹
- `use_inv_depth=false`ë¡œ ì„¤ì •í•˜ë©´ ì›ë³¸ ì´ë¡ ê³¼ ë™ì¼

---

## ï¿½ğŸ“ ì§€ì›

êµ¬í˜„ ì¤‘ ë¬¸ì œê°€ ë°œìƒí•˜ë©´:

1. **Issue ìƒì„±:** GitHub Issues
2. **ë¡œê·¸ í™•ì¸:** `outputs/logs/`
3. **TensorBoard:** `tensorboard --logdir outputs/`
4. **ë””ë²„ê·¸ ëª¨ë“œ:**
   ```python
   import pdb; pdb.set_trace()  # ì¤‘ë‹¨ì  ì„¤ì •
   ```

---

**ë¬¸ì„œ ë²„ì „:** 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸:** 2025ë…„ 10ì›” 17ì¼  
**ì‘ì„±ì:** PackNet-SFM Development Team
**ë¼ì´ì„¼ìŠ¤:** MIT (Toyota Research Institute)
