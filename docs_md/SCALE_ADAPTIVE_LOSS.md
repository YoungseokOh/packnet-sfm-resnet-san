# ê¹Šì´ ì¶”ì •ì„ ìœ„í•œ Scale-Adaptive Loss

## ğŸ“š ê°œìš”

**Scale-Adaptive Loss**ëŠ” ì£¼ë¡œ **ìê¸°ì§€ë„ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •(self-supervised monocular depth estimation)**ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¡œ, ê·¼ë³¸ì ì¸ scale ambiguity ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì ˆëŒ€ scale ì •ë³´ ì—†ì´ë„ ê¹Šì´ ì¶”ì • ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

## ğŸ¯ ë¬¸ì œ: Scale Ambiguity

ë‹¨ì•ˆ(single camera) ê¹Šì´ ì¶”ì •ì—ì„œëŠ” **ì ˆëŒ€ scaleì„ ì•Œ ìˆ˜ ì—†ëŠ” ê·¼ë³¸ì ì¸ ë¬¸ì œ**ê°€ ìˆìŠµë‹ˆë‹¤:

- ë‹¨ì¼ ì¹´ë©”ë¼ë§Œìœ¼ë¡œëŠ” ì¶”ê°€ ì •ë³´ ì—†ì´ ì‹¤ì œ ê±°ë¦¬ë¥¼ ê²°ì •í•  ìˆ˜ ì—†ìŒ
- ì˜ˆì¸¡ëœ ê¹Šì´ì™€ ground truth ê¹Šì´ê°€ scaleì—ì„œ ì°¨ì´ë‚  ìˆ˜ ìˆìŒ
- ì „í†µì ì¸ L1/L2 ì†ì‹¤ì€ ì´ëŸ¬í•œ scale ì°¨ì´ì— ë§¤ìš° ë¯¼ê°í•¨

### ìˆ˜í•™ì  ì •ì‹í™”

ë‹¨ì•ˆ ë¹„ì „ì—ì„œ íˆ¬ì˜ ë°©ì •ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```
p = K [R|t] P

where:
  p: 2D ì´ë¯¸ì§€ í¬ì¸íŠ¸ (u, v, 1)áµ€
  P: 3D ì›”ë“œ í¬ì¸íŠ¸ (X, Y, Z, 1)áµ€
  K: ë‚´ë¶€ íŒŒë¼ë¯¸í„° í–‰ë ¬ (3Ã—3)
  [R|t]: ì™¸ë¶€ íŒŒë¼ë¯¸í„° í–‰ë ¬ (3Ã—4)
```

**ëª¨í˜¸ì„±:**
```
p = K [R|t] (Î»Â·P)  ì—¬ê¸°ì„œ Î»ëŠ” ì„ì˜ì˜ scale

â†’ ê°™ì€ ì´ë¯¸ì§€ pì´ì§€ë§Œ, ê¹Šì´ ZëŠ” Î»ë°°ë§Œí¼ ì°¨ì´ë‚¨
```

---

## ğŸ’¡ í•´ê²°ì±…: Scale-Adaptive Loss

Scale-Adaptive LossëŠ” ì˜ˆì¸¡ê³¼ ground truth ì‚¬ì´ì˜ **scale ì°¨ì´ë¥¼ ìë™ìœ¼ë¡œ ë³´ì •**í•©ë‹ˆë‹¤.

### í•µì‹¬ ê³µì‹

```
L_scale_adaptive = min_s (1/N) Î£ |sÂ·d_pred - d_gt|Â²

where s* = argmin_s Î£ (sÂ·d_pred - d_gt)Â²
```

**ì˜ë¯¸:** ìµœì ì˜ scale factor `s*`ë¥¼ ì°¾ì•„ ì˜ˆì¸¡ì„ ground truthì— ì •ë ¬í•©ë‹ˆë‹¤.

---

## ğŸ”¬ ìµœì  Scale Factor ê³„ì‚°

### ë°©ë²• 1: Median Scaling (ê°€ì¥ ì¼ë°˜ì )

```
s* = median(d_gt) / median(d_pred)
```

**ì¥ì :**
- âœ… ì´ìƒì¹˜(outlier)ì— ê°•ê±´í•¨
- âœ… Scale invariance ë³´ì¥
- âœ… ê³„ì‚° íš¨ìœ¨ì 

**ì™œ í‰ê· ì´ ì•„ë‹Œ ì¤‘ì•™ê°’?**
- ê·¹ë‹¨ì ì¸ ê¹Šì´ ê°’ì— ë” ê°•ê±´
- í¬ì†Œí•˜ê±°ë‚˜ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ê¹Šì´ ë§µì„ ë” ì˜ ì²˜ë¦¬
- ê¹Šì´ ì¶”ì • ë²¤ì¹˜ë§ˆí¬(KITTI, NYU-Depth)ì—ì„œ í‘œì¤€

### ë°©ë²• 2: Least Squares Scaling

**ëª©ì  í•¨ìˆ˜:**
```
s* = argmin_s Î£ (sÂ·d_pred - d_gt)Â²
```

**ìœ ë„:**
sì— ëŒ€í•´ ë¯¸ë¶„í•˜ê³  0ìœ¼ë¡œ ì„¤ì •:
```
âˆ‚/âˆ‚s Î£ (sÂ·d_pred - d_gt)Â² = 0

2 Î£ d_pred(sÂ·d_pred - d_gt) = 0

s Î£ d_predÂ² = Î£ d_predÂ·d_gt

s* = (Î£ d_predÂ·d_gt) / (Î£ d_predÂ²)
```

í–‰ë ¬ í˜•íƒœ:
```
s* = (d_pred^T Â· d_gt) / (d_pred^T Â· d_pred)
```

**ì¥ì :**
- âœ… ìˆ˜í•™ì ìœ¼ë¡œ ìµœì  (L2 ì˜¤ì°¨ ìµœì†Œí™”)
- âœ… ë°€ì§‘ ì˜ˆì¸¡ì— ì í•©

**ë‹¨ì :**
- âš ï¸ ì´ìƒì¹˜ì— ë” ë¯¼ê°
- âš ï¸ ê³„ì‚° ë¹„ìš© ì¦ê°€

### ë°©ë²• 3: Mean Scaling

```
s* = mean(d_gt) / mean(d_pred) = (Î£ d_gt) / (Î£ d_pred)
```

**ì¥ì :**
- âœ… êµ¬í˜„ ê°„ë‹¨
- âœ… ë¹ ë¥¸ ê³„ì‚°

**ë‹¨ì :**
- âš ï¸ ì´ìƒì¹˜ì— ë§¤ìš° ë¯¼ê°
- âš ï¸ íìƒ‰ì´ ìˆëŠ” ê¹Šì´ ë§µì—ëŠ” ë¹„ì¶”ì²œ

---

## ğŸ“ Scale-Invariant Loss (Eigen et al., 2014)

ë¡œê·¸ ê³µê°„ì—ì„œ ì‘ë™í•˜ëŠ” ë” ê³ ê¸‰ ê³µì‹:

### ê³µì‹

```
L_si = (1/N) Î£ (log(d_pred) - log(d_gt) + Î±)Â²

where Î± = (1/N) Î£ (log(d_gt) - log(d_pred))
```

**ì „ê°œ í˜•íƒœ:**
```
L_si = (1/N) Î£ (log(d_pred) - log(d_gt))Â² 
     - (1/NÂ²)(Î£ (log(d_pred) - log(d_gt)))Â²
```

### í•´ì„

- **ì²« ë²ˆì§¸ í•­:** í”½ì…€ë³„ ë¡œê·¸ ì°¨ì´ì˜ ì œê³± (í”½ì…€ ë‹¨ìœ„ ì˜¤ì°¨)
- **ë‘ ë²ˆì§¸ í•­:** ì „ì—­ scale ì°¨ì´ ì œê±° (scale-invariant í•­)

**ë¡œê·¸ ê³µê°„ì—ì„œ:**
```
Î´áµ¢ = log(d_pred_i) - log(d_gt_i)

Î´_mean = (1/N) Î£ Î´áµ¢

L_si = (1/N) Î£ Î´áµ¢Â² - Î´_meanÂ²
     = Var(log(d_pred/d_gt))
```

**ì˜ë¯¸:** ë¡œê·¸ ê¹Šì´ ë¹„ìœ¨ì˜ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ì—¬ ì†ì‹¤ì„ ì§„ì •ìœ¼ë¡œ scale-invariantí•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

---

## ğŸ”§ PyTorch êµ¬í˜„

### ê¸°ë³¸ Scale-Adaptive Loss

```python
import torch
import torch.nn as nn

def scale_adaptive_loss(pred, gt, loss_type='l1', scaling='median'):
    """
    Scale-Adaptive Loss êµ¬í˜„
    
    Args:
        pred: [B, 1, H, W] ì˜ˆì¸¡ ê¹Šì´
        gt: [B, 1, H, W] ground truth ê¹Šì´
        loss_type: 'l1', 'l2', 'berhu', 'si' (scale-invariant)
        scaling: 'median', 'mean', 'least_squares'
    
    Returns:
        loss: ìŠ¤ì¹¼ë¼
        scale: ìµœì  scale factor
    """
    # ìœ íš¨ ë§ˆìŠ¤í¬ (depth > 0)
    valid = (gt > 0) & (pred > 0)
    
    if scaling == 'median':
        # s* = median(d_gt) / median(d_pred)
        scale = torch.median(gt[valid]) / torch.median(pred[valid])
        
    elif scaling == 'mean':
        # s* = mean(d_gt) / mean(d_pred)
        scale = gt[valid].mean() / pred[valid].mean()
        
    elif scaling == 'least_squares':
        # s* = (pred^T Â· gt) / (pred^T Â· pred)
        pred_v = pred[valid]
        gt_v = gt[valid]
        scale = (pred_v * gt_v).sum() / (pred_v ** 2).sum()
    
    # Scale ë³´ì •
    pred_scaled = pred * scale
    
    # ì†ì‹¤ ê³„ì‚°
    if loss_type == 'l1':
        loss = torch.abs(pred_scaled[valid] - gt[valid]).mean()
        
    elif loss_type == 'l2':
        loss = ((pred_scaled[valid] - gt[valid]) ** 2).mean()
        
    elif loss_type == 'berhu':
        diff = torch.abs(pred_scaled[valid] - gt[valid])
        c = 0.2 * diff.max()
        loss = torch.where(
            diff <= c,
            diff,  # L1
            (diff ** 2 + c ** 2) / (2 * c)  # L2
        ).mean()
        
    elif loss_type == 'si':
        # Scale-Invariant Loss
        log_diff = torch.log(pred_scaled[valid]) - torch.log(gt[valid])
        loss = (log_diff ** 2).mean() - (log_diff.mean() ** 2)
    
    return loss, scale
```

### Scale-Invariant Gradient Loss

```python
def scale_invariant_gradient_loss(pred, gt):
    """
    Scale-Invariant Gradient Matching Loss
    
    L = Î£ |âˆ‡log(d_pred) - âˆ‡log(d_gt)|
    """
    log_pred = torch.log(pred.clamp(min=1e-3))
    log_gt = torch.log(gt.clamp(min=1e-3))
    
    # x ë°©í–¥ ê·¸ë˜ë””ì–¸íŠ¸
    grad_pred_x = log_pred[:, :, :, 1:] - log_pred[:, :, :, :-1]
    grad_gt_x = log_gt[:, :, :, 1:] - log_gt[:, :, :, :-1]
    
    # y ë°©í–¥ ê·¸ë˜ë””ì–¸íŠ¸
    grad_pred_y = log_pred[:, :, 1:, :] - log_pred[:, :, :-1, :]
    grad_gt_y = log_gt[:, :, 1:, :] - log_gt[:, :, :-1, :]
    
    loss_x = torch.abs(grad_pred_x - grad_gt_x).mean()
    loss_y = torch.abs(grad_pred_y - grad_gt_y).mean()
    
    return loss_x + loss_y
```

---

## ğŸ“ G2-MonoDepth Loss: ê³ ê¸‰ êµ¬í˜„

G2-MonoDepthëŠ” ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ scale-adaptiveì™€ gradient ì†ì‹¤ì„ ê²°í•©í•©ë‹ˆë‹¤.

### ì „ì²´ êµ¬ì¡°

```
L_G2 = L_sa + Î» Â· L_sg

where:
    L_sa: Scale-Adaptive Loss (ìƒëŒ€ + ì ˆëŒ€)
    L_sg: Scale-Invariant Gradient Loss (ë©€í‹°ìŠ¤ì¼€ì¼)
    Î»: gradient ì†ì‹¤ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.5)
```

### 1. Scale-Adaptive Loss (L_sa)

```
L_sa = L_relative + L_absolute

L_relative = (1/M) Î£ |d_norm - z_norm|

L_absolute = (1/M_V) Î£_V |d_i - z_i|
```

#### A. ìƒëŒ€ ê´€ê³„ í•­

**ëª©ì :** Scale-invariantí•œ ìƒëŒ€ì  ê¹Šì´ ê´€ê³„ í•™ìŠµ

**ì •ê·œí™”:**
```
mean(x) = (1/M) Î£ x_i

Ïƒ(x) = (1/M) Î£ |x_i - mean(x)|  (í‰ê·  ì ˆëŒ€ í¸ì°¨, MAD)

x_norm = (x - mean(x)) / (Ïƒ(x) + Îµ)
```

**ì†ì‹¤:**
```
d_norm = normalize(pred_depth)
z_norm = normalize(gt_depth)

L_relative = (1/M) Î£ |d_norm - z_norm|
```

**íŠ¹ì§•:**
- âœ… Scale ambiguity í•´ê²°
- âœ… ì „ì—­ ê¹Šì´ ë¶„í¬ ì •ë ¬
- âœ… ëª¨ë“  í”½ì…€ ì‚¬ìš©

#### B. ì ˆëŒ€ ê´€ê³„ í•­

**ëª©ì :** ìœ íš¨í•œ GTê°€ ìˆëŠ” ì˜ì—­ì—ì„œ ì ˆëŒ€ ê¹Šì´ ì •í™•ë„ í–¥ìƒ

```
L_absolute = (1/M_V) Î£_V |d_i - z_i| Â· mask_i

where:
    M_V: ìœ íš¨ í”½ì…€ ìˆ˜
    mask_i: ìœ íš¨ í”½ì…€ ë§ˆìŠ¤í¬ (0 ë˜ëŠ” 1)
```

**ì‚¬ìš© ì‚¬ë¡€:**
- í¬ì†Œ LiDAR ê¹Šì´ ì™„ì„±
- ë¶€ë¶„ ground truth ì‹œë‚˜ë¦¬ì˜¤
- ìœ íš¨ ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ì´ í•­ì€ 0

### 2. Scale-Invariant Gradient Loss (L_sg)

```
L_sg = (1/M) Î£_{k=1}^{K} Î£_i (|âˆ‡_h R_i^k| + |âˆ‡_w R_i^k|)

where:
    R_i^k = Ï_k(d_norm - z_norm)  (ìŠ¤ì¼€ì¼ kì—ì„œì˜ ì •ê·œí™” ì”ì°¨)
    âˆ‡_h: ìˆ˜í‰ ê·¸ë˜ë””ì–¸íŠ¸ (Sobel-x)
    âˆ‡_w: ìˆ˜ì§ ê·¸ë˜ë””ì–¸íŠ¸ (Sobel-y)
    K: ë©€í‹°ìŠ¤ì¼€ì¼ ë ˆë²¨ ìˆ˜ (ê¸°ë³¸ê°’: 4)
```

#### ê³¼ì •

**1ë‹¨ê³„: ì”ì°¨ ê³„ì‚°**
```
R = d_norm - z_norm
```

**2ë‹¨ê³„: ë©€í‹°ìŠ¤ì¼€ì¼ ë‹¤ìš´ìƒ˜í”Œë§**
```
RÂ¹ = R                    (ì›ë³¸)
RÂ² = downsample(R, 1/2)   (1/2 í•´ìƒë„)
RÂ³ = downsample(R, 1/4)   (1/4 í•´ìƒë„)
Râ´ = downsample(R, 1/8)   (1/8 í•´ìƒë„)
```

**3ë‹¨ê³„: Sobel ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°**

Sobel-x (ìˆ˜í‰ ì—ì§€):
```
       [-1  0  1]
K_x =  [-2  0  2]
       [-1  0  1]
```

Sobel-y (ìˆ˜ì§ ì—ì§€):
```
       [-1 -2 -1]
K_y =  [ 0  0  0]
       [ 1  2  1]
```

ê·¸ë˜ë””ì–¸íŠ¸:
```
âˆ‡_h R^k = K_x * R^k
âˆ‡_w R^k = K_y * R^k
```

**4ë‹¨ê³„: ë©€í‹°ìŠ¤ì¼€ì¼ ì†ì‹¤ ì§‘ê³„**
```
L_sg = Î£_{k=1}^{4} mean(|âˆ‡_h R^k| + |âˆ‡_w R^k|)
```

**ì´ì :**
- ë©€í‹°ìŠ¤ì¼€ì¼ì´ ë‹¤ì–‘í•œ êµ¬ì¡° í¬ê¸°ë¥¼ í¬ì°©
- ì—ì§€ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ë¶€ë“œëŸ¬ì›€ ìœ ë„
- Scale-invariantí•œ êµ¬ì¡°ì  ìœ ì‚¬ì„±

### ì™„ì „í•œ G2-MonoDepth êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class G2MonoDepthLoss(nn.Module):
    """
    G2-MonoDepth Loss Function
    L_G2 = L_sa + Î» * L_sg
    """
    
    def __init__(self, lambda_sg=0.5, epsilon=1e-8, num_scales=4):
        super(G2MonoDepthLoss, self).__init__()
        self.lambda_sg = lambda_sg
        self.epsilon = epsilon
        self.num_scales = num_scales
        
        # Sobel ì»¤ë„
        self.register_buffer('sobel_x', self._get_sobel_kernel('x'))
        self.register_buffer('sobel_y', self._get_sobel_kernel('y'))
    
    def _get_sobel_kernel(self, direction):
        if direction == 'x':
            kernel = torch.tensor([[-1., 0., 1.],
                                   [-2., 0., 2.],
                                   [-1., 0., 1.]])
        else:  # 'y'
            kernel = torch.tensor([[-1., -2., -1.],
                                   [ 0.,  0.,  0.],
                                   [ 1.,  2.,  1.]])
        return kernel.unsqueeze(0).unsqueeze(0)
    
    def normalize_depth(self, depth):
        """MADë¥¼ ì‚¬ìš©í•œ ê¹Šì´ ë§µ ì •ê·œí™”"""
        mean = torch.mean(depth, dim=[1, 2, 3], keepdim=True)
        std = torch.mean(torch.abs(depth - mean), dim=[1, 2, 3], keepdim=True)
        normalized = (depth - mean) / (std + self.epsilon)
        return normalized, mean, std
    
    def scale_adaptive_loss(self, pred_depth, gt_depth, valid_mask=None):
        """Scale-Adaptive Loss"""
        # ìƒëŒ€ í•­
        pred_norm, _, _ = self.normalize_depth(pred_depth)
        gt_norm, _, _ = self.normalize_depth(gt_depth)
        relative_loss = torch.mean(torch.abs(pred_norm - gt_norm))
        
        # ì ˆëŒ€ í•­ (ì„ íƒì )
        if valid_mask is not None:
            valid_pred = pred_depth * valid_mask
            valid_gt = gt_depth * valid_mask
            num_valid = torch.sum(valid_mask, dim=[1, 2, 3], keepdim=True)
            num_valid = torch.clamp(num_valid, min=1.0)
            
            absolute_error = torch.abs(valid_pred - valid_gt) * valid_mask
            absolute_loss = torch.sum(absolute_error) / (torch.sum(num_valid) + self.epsilon)
        else:
            absolute_loss = 0.0
        
        return relative_loss + absolute_loss
    
    def apply_sobel(self, x, kernel):
        """Sobel ì—°ì‚°ì ì ìš©"""
        x_padded = F.pad(x, (1, 1, 1, 1), mode='replicate')
        gradient = F.conv2d(x_padded, kernel, padding=0)
        return gradient
    
    def scale_invariant_gradient_loss(self, pred_depth, gt_depth):
        """ë©€í‹°ìŠ¤ì¼€ì¼ ê·¸ë˜ë””ì–¸íŠ¸ ì†ì‹¤"""
        pred_norm, _, _ = self.normalize_depth(pred_depth)
        gt_norm, _, _ = self.normalize_depth(gt_depth)
        residual = pred_norm - gt_norm
        
        total_gradient_loss = 0.0
        for k in range(1, self.num_scales + 1):
            if k > 1:
                scale_factor = 1.0 / (2 ** (k - 1))
                residual_k = F.interpolate(residual, scale_factor=scale_factor,
                                          mode='bilinear', align_corners=False)
            else:
                residual_k = residual
            
            grad_x = self.apply_sobel(residual_k, self.sobel_x)
            grad_y = self.apply_sobel(residual_k, self.sobel_y)
            gradient_loss = torch.mean(torch.abs(grad_x) + torch.abs(grad_y))
            total_gradient_loss += gradient_loss
        
        return total_gradient_loss
    
    def forward(self, pred_depth, gt_depth, valid_mask=None):
        """Forward pass"""
        loss_sa = self.scale_adaptive_loss(pred_depth, gt_depth, valid_mask)
        loss_sg = self.scale_invariant_gradient_loss(pred_depth, gt_depth)
        total_loss = loss_sa + self.lambda_sg * loss_sg
        
        loss_dict = {
            'total': total_loss.item(),
            'scale_adaptive': loss_sa.item() if isinstance(loss_sa, torch.Tensor) else loss_sa,
            'gradient': loss_sg.item(),
        }
        
        return total_loss, loss_dict
```

---

## ğŸ“Š ë°©ë²• ë¹„êµ

| ë°©ë²• | ê³µì‹ | ì¥ì  | ë‹¨ì  |
|--------|---------|------|------|
| **Median** | `s* = median(gt)/median(pred)` | ê°•ê±´í•¨, ë¹ ë¦„ | ë¶„í¬ ë¬´ì‹œ |
| **Mean** | `s* = mean(gt)/mean(pred)` | ê°„ë‹¨í•¨ | ì´ìƒì¹˜ì— ë¯¼ê° |
| **Least Squares** | `s* = (predÂ·gt)/(predÂ·pred)` | ìµœì  | ê³„ì‚°ëŸ‰ ì¦ê°€ |
| **Scale-Invariant** | `L = Î£Î´Â² - (Î£Î´)Â²/N` | ì§„ì •í•œ ë¶ˆë³€ì„± | ë” ë³µì¡ |

---

## ğŸ“ˆ Scalingì„ ì ìš©í•œ í‰ê°€ ë©”íŠ¸ë¦­

### ì„ê³„ê°’ ì •í™•ë„

```
Î´_t = % of pixels where max(d_pred/d_gt, d_gt/d_pred) < 1.25^t

for t âˆˆ {1, 2, 3}
```

**Scale ì ìš© í›„:**
```
d_pred_scaled = s* Â· d_pred

Î´_t = % of pixels where max(d_pred_scaled/d_gt, d_gt/d_pred_scaled) < 1.25^t
```

### ì˜¤ì°¨ ë©”íŠ¸ë¦­

```
AbsRel = (1/N) Î£ |d_pred - d_gt| / d_gt

SqRel = (1/N) Î£ (d_pred - d_gt)Â² / d_gt

RMSE = sqrt((1/N) Î£ (d_pred - d_gt)Â²)

RMSE_log = sqrt((1/N) Î£ (log(d_pred) - log(d_gt))Â²)
```

**Scaling ì ìš© ì‹œ:**
ëª¨ë“  ë©”íŠ¸ë¦­ì€ `d_pred â†’ s* Â· d_pred` í›„ ê³„ì‚°

---

## ğŸ¯ ì‚¬ìš© ì‚¬ë¡€

### ì‚¬ë¡€ 1: ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •
```python
# ìœ íš¨ ë§ˆìŠ¤í¬ ì—†ìŒ
loss, loss_dict = criterion(pred_depth, gt_depth, valid_mask=None)
```
- ëª¨ë“  í”½ì…€ ì‚¬ìš©
- Scale-invariant í•™ìŠµ
- êµ¬ì¡° ë³´ì¡´ ê°•ì¡°

### ì‚¬ë¡€ 2: í¬ì†Œ ê¹Šì´ ì™„ì„±
```python
# 10% ìœ íš¨ í”½ì…€
valid_mask = (torch.rand(B, 1, H, W) > 0.9).float()
loss, loss_dict = criterion(pred_depth, gt_depth, valid_mask=valid_mask)
```
- í¬ì†Œ GT í™œìš©
- ìœ íš¨ í”½ì…€ì—ì„œ ì ˆëŒ€ ì •í™•ë„
- ë‚˜ë¨¸ì§€ ì˜ì—­ì€ ìƒëŒ€ ê´€ê³„

### ì‚¬ë¡€ 3: ë°€ì§‘ ê¹Šì´ í–¥ìƒ
```python
# 100% ìœ íš¨ í”½ì…€
valid_mask = torch.ones(B, 1, H, W)
loss, loss_dict = criterion(pred_depth, gt_depth, valid_mask=valid_mask)
```
- ë°€ì§‘ ì§€ë„
- ì ˆëŒ€ ê¹Šì´ ì •í™•ë„ ìµœëŒ€í™”
- Scaleê³¼ ì ˆëŒ€ê°’ ëª¨ë‘ ìµœì í™”

---

## ğŸ’¡ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì˜ë¯¸ | ê¶Œì¥ ë²”ìœ„ |
|-----------|---------|---------|-------------------|
| `lambda_sg` | 0.5 | ê·¸ë˜ë””ì–¸íŠ¸ ì†ì‹¤ ê°€ì¤‘ì¹˜ | 0.1 ~ 1.0 |
| `epsilon` | 1e-8 | 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ | 1e-10 ~ 1e-6 |
| `num_scales` | 4 | ë©€í‹°ìŠ¤ì¼€ì¼ ë ˆë²¨ | 3 ~ 5 |

**íŠœë‹ ê°€ì´ë“œ:**
- `lambda_sg` â†‘ â†’ ì—ì§€ ì„ ëª…ë„ â†‘, í‰íƒ„ ì˜ì—­ ë…¸ì´ì¦ˆ â†‘
- `lambda_sg` â†“ â†’ ë¶€ë“œëŸ¬ìš´ ì˜ˆì¸¡, ì—ì§€ íë¦¼
- `num_scales` â†‘ â†’ ë‹¤ì–‘í•œ êµ¬ì¡° í•™ìŠµ, ê³„ì‚°ëŸ‰ â†‘

---

## ğŸ”¬ ìˆ˜í•™ì  íŠ¹ì„±

### ì™œ í‰ê·  ì ˆëŒ€ í¸ì°¨(MAD)?

```
Ïƒ_MAD(x) = (1/M) Î£ |x_i - mean(x)|
```

**í‘œì¤€í¸ì°¨ì™€ ë¹„êµ:**
```
Ïƒ_std(x) = sqrt((1/M) Î£ (x_i - mean(x))Â²)
```

**ì¥ì :**
- âœ… ì´ìƒì¹˜ì— ë” ê°•ê±´ (ì œê³± ì—†ìŒ)
- âœ… ê³„ì‚°ì´ ì•ˆì •ì  (ì œê³±ê·¼ ë¶ˆí•„ìš”)
- âœ… ë¶ˆê·œì¹™í•œ ê¹Šì´ ë¶„í¬ì— ì í•©
- âœ… í¬ì†Œ ê¹Šì´ ë§µì— ì í•©

### ì •ê·œí™” íš¨ê³¼

```
x_norm = (x - Î¼) / Ïƒ

â†’ E[x_norm] = 0
â†’ Var[x_norm] â‰ˆ 1
```

**ê¹Šì´ì— ì ìš©:**
```
d_norm ~ N(0, 1)
z_norm ~ N(0, 1)

â†’ Scale-free ë¹„êµ
```

### ë¡œê·¸ ê³µê°„ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë§¤ì¹­

```
âˆ‡log(d) = âˆ‡d / d

â†’ ìƒëŒ€ì  ë³€í™”ìœ¨
```

**ì´ì :**
- ê¹Šì´ ë¶ˆì—°ì†(ì—ì§€) ë³´ì¡´
- í‰íƒ„í•œ ì˜ì—­ì˜ ë¶€ë“œëŸ¬ì›€ ìœ ì§€
- ë©€í‹°ìŠ¤ì¼€ì¼ë¡œ ë‹¤ì–‘í•œ êµ¬ì¡° í¬ê¸° ì»¤ë²„

---

## ğŸ“š ê´€ë ¨ ì—°êµ¬

### ì£¼ìš” ë…¼ë¬¸

1. **Eigen et al. (2014)** - "Depth Map Prediction from a Single Image using a Multi-Scale Deep Network"
   - Scale-invariant loss ë„ì…
   - ë¡œê·¸ ê³µê°„ ê³µì‹í™” ì œì•ˆ

2. **Godard et al. (2019)** - "Digging Into Self-Supervised Monocular Depth Estimation" (Monodepth2)
   - Scale-adaptive í•™ìŠµ
   - Median scaling í‰ê°€

3. **Guizilini et al. (2020)** - "3D Packing for Self-Supervised Monocular Depth Estimation" (PackNet-SFM)
   - Scale ì¼ê´€ì„±ì„ ìœ„í•œ 3D packing
   - í‰ê°€ì—ì„œ GT scale ì˜µì…˜ ì œê³µ

4. **Bhat et al. (2021)** - "AdaBins: Depth Estimation using Adaptive Bins"
   - Scale-invariant lossì™€ í•¨ê»˜ ì ì‘í˜• binning

---

## âœ… ì¥ì 

1. **Scale-Invariant**: ì ˆëŒ€ scale ì—†ì´ í•™ìŠµ ê°€ëŠ¥
2. **í†µí•© í”„ë ˆì„ì›Œí¬**: ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤(í¬ì†Œ/ë°€ì§‘)ì— ë‹¨ì¼ ì†ì‹¤
3. **êµ¬ì¡° ë³´ì¡´**: ê·¸ë˜ë””ì–¸íŠ¸ ë§¤ì¹­ìœ¼ë¡œ ì—ì§€ ì„ ëª…ë„ ìœ ì§€
4. **ê°•ê±´í•¨**: MAD ì •ê·œí™”ë¡œ ì´ìƒì¹˜ ì²˜ë¦¬
5. **ë©€í‹°ìŠ¤ì¼€ì¼**: ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ êµ¬ì¡° í¬ì°©

## âš ï¸ í•œê³„

1. **ê³„ì‚°ëŸ‰**: ë©€í‹°ìŠ¤ì¼€ì¼ + Sobel ì—°ì‚°ìœ¼ë¡œ ëŠë¦¼
2. **ë©”ëª¨ë¦¬**: ì¤‘ê°„ ê²°ê³¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
3. **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: `lambda_sg` íŠœë‹ í•„ìš”
4. **ì ˆëŒ€ í•­**: ìœ íš¨ ë§ˆìŠ¤í¬ í•„ìš” (ì„ íƒì  í•œê³„)

---

## ğŸš€ ê³ ê¸‰ í™•ì¥

### 1. ì ì‘í˜• Lambda (í•™ìŠµ ì¤‘ ë™ì  ì¡°ì •)

```python
class AdaptiveG2Loss(G2MonoDepthLoss):
    def forward(self, pred_depth, gt_depth, valid_mask=None, epoch=0):
        # ì´ˆê¸°: ìƒëŒ€ ê´€ê³„ ì¤‘ì‹¬, í›„ê¸°: ê·¸ë˜ë””ì–¸íŠ¸ ì¤‘ì‹¬
        lambda_adaptive = self.lambda_sg * min(epoch / 50, 1.0)
        
        loss_sa = self.scale_adaptive_loss(pred_depth, gt_depth, valid_mask)
        loss_sg = self.scale_invariant_gradient_loss(pred_depth, gt_depth)
        
        return loss_sa + lambda_adaptive * loss_sg
```

### 2. ê°€ì¤‘ ë©€í‹°ìŠ¤ì¼€ì¼ (íŠ¹ì • ìŠ¤ì¼€ì¼ì— ìš°ì„ ìˆœìœ„)

```python
def scale_invariant_gradient_loss_weighted(self, pred_depth, gt_depth):
    weights = [1.0, 0.5, 0.25, 0.125]  # ì›ë³¸ í•´ìƒë„ì— ë” í° ê°€ì¤‘ì¹˜
    
    total_loss = 0.0
    for k, w in enumerate(weights, start=1):
        # ... ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ...
        total_loss += w * gradient_loss
    
    return total_loss / sum(weights)
```

### 3. ì—ì§€ ì¸ì‹ ê°€ì¤‘ì¹˜ (ì—ì§€ì—ì„œ ë” í° ê°€ì¤‘ì¹˜)

```python
def edge_aware_gradient_loss(self, pred_depth, gt_depth):
    # GT ì—ì§€ ê°•ë„ ê³„ì‚°
    gt_grad_x = self.apply_sobel(gt_depth, self.sobel_x)
    gt_grad_y = self.apply_sobel(gt_depth, self.sobel_y)
    gt_edge_weight = torch.sqrt(gt_grad_x**2 + gt_grad_y**2)
    gt_edge_weight = gt_edge_weight / (gt_edge_weight.mean() + 1e-8)
    
    # ... ì”ì°¨ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ...
    
    # ì—ì§€ ê°€ì¤‘ ì†ì‹¤
    weighted_loss = (torch.abs(grad_x) + torch.abs(grad_y)) * gt_edge_weight
    return weighted_loss.mean()
```

---

## ğŸ“ ìš”ì•½

Scale-Adaptive LossëŠ” **scale-invariance**ì™€ **êµ¬ì¡° ë³´ì¡´**ì„ ë™ì‹œì— ë‹¬ì„±í•˜ëŠ” ìš°ì•„í•œ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. íŠ¹íˆ ë‹¤ìŒì— íš¨ê³¼ì ì…ë‹ˆë‹¤:

- ìê¸°ì§€ë„ ë‹¨ì•ˆ ê¹Šì´ ì¶”ì •
- í¬ì†Œ ê¹Šì´ ì™„ì„± (LiDAR + ì¹´ë©”ë¼ ìœµí•©)
- ë°€ì§‘ ê¹Šì´ ê°œì„ 
- Scale ambiguityê°€ ìˆëŠ” ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤

G2-MonoDepth ê³µì‹ì€ ë©€í‹°ìŠ¤ì¼€ì¼ ê·¸ë˜ë””ì–¸íŠ¸ ë§¤ì¹­ìœ¼ë¡œ ì´ë¥¼ í™•ì¥í•˜ì—¬ ë‹¤ì–‘í•œ ê¹Šì´ ì¶”ì • ì‘ì—…ì—ì„œ ìµœê³  ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

---

## ğŸ“– ì°¸ê³ ë¬¸í—Œ

1. Eigen, D., & Fergus, R. (2014). "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture"
2. Godard, C., et al. (2019). "Digging Into Self-Supervised Monocular Depth Estimation"
3. Guizilini, V., et al. (2020). "3D Packing for Self-Supervised Monocular Depth Estimation"
4. Bhat, S. F., et al. (2021). "AdaBins: Depth Estimation using Adaptive Bins"

---

**ë¬¸ì„œ ë²„ì „:** 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸:** 2025ë…„ 10ì›” 17ì¼  
**ì‘ì„±ì:** PackNet-SFM ResNet-SAN íŒ€
